{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why have I created this repo? I like to read a lot; papers, blog posts, twitter threads, notebooks, you name it. When I read, i've been quite well at documenting this through my blog. The blog let's me sement my thoughts, improve retention, and log what i've read. it can be quite tedious work, and sometimes I simply don't have the time, or rather capacity to do it, but i've been quite good at it, at least up until a couple of months ago. now, even though I sometimes write quite extensive summaries or thoughts about papers, i've been pretty poor at looking back at my notes when i need to remember something in a paper i read, and i attribute this mainly to a lack of **indexing**. the process often looks like this: \"ooh, i know i've read something about X in a paper\" *scrolls through all [blog entries](https://leonericsson.github.io/indexer), not finding what i'm looking for immediately and just give up*. yes, i could just become more patient and keep looking, why not instead take this opportunity and build something! this is the motivation for this repo. \n",
    "\n",
    "my initial plan. before doing any valuable research, here's what i **think** is the way i want to solve this problem, including some questions that i yet don't know the answer to:\n",
    "\n",
    "1. save the link to every piece of research content i've read, that i can recall and find (i've already got a decent chunk saved).\n",
    "2. download the content.\n",
    "    - should everything be standardized into a format?\n",
    "    - do i need to be careful to exclude irrelevant information from the source?\n",
    "    - how do i deal with twitter threads?\n",
    "3. decide on an embedding model\n",
    "    - what is a good context size? relates to how we plan to chunk a 20 page paper.\n",
    "4. embed content\n",
    "    - how do we deal with long format texts e.g. papers?\n",
    "    - does the search engine need to rely on more than just a single embedding model?\n",
    "5. quantize embeddings to int8\n",
    "    - does this process impact the selected embedding model?\n",
    "5. index the embeddings\n",
    "6. re-ranker\n",
    "7. search engine\n",
    "     - how do we link the source content + source url to the search results? especially\n",
    "     considering that we will surely have multiple embedding points from the same source content.\n",
    "    \n",
    "okay, this seems like a good place to start don't you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comparing embedding models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n",
    "\n",
    "# For retrieval you need to pass this prompt.\n",
    "query = 'Represent this sentence for searching relevant passages: are all layers in a transformer equally important?'\n",
    "\n",
    "docs = [\n",
    "    query,\n",
    "    \"\"\"Within the field of vector search, an intriguing development has arisen: binary vector search. This approach shows promise in tackling the long-standing issue of memory consumption by achieving a remarkable 30x reduction. However, a critical aspect that sparks debate is its effect on accuracy.\n",
    "\n",
    "We believe that using binary vector search, along with specific optimization techniques, can maintain similar accuracy. To provide clarity on this subject, we showcase a series of experiments that will demonstrate the effects and implications of this approach.\"\"\",\n",
    "    \"\"\"We empirically study a simple layer-pruning strategy for popular families of openweight pretrained LLMs, finding minimal degradation of performance on different\n",
    "question-answering benchmarks until after a large fraction (up to half) of the layers\n",
    "are removed. To prune these models, we identify the optimal block of layers to\n",
    "prune by considering similarity across layers; then, to “heal” the damage, we\n",
    "perform a small amount of finetuning. In particular, we use parameter-efficient\n",
    "finetuning (PEFT) methods, specifically quantization and Low Rank Adapters\n",
    "(QLoRA), such that each of our experiments can be performed on a single A100\n",
    "GPU. From a practical perspective, these results suggest that layer pruning methods\n",
    "can complement other PEFT strategies to further reduce computational resources of\n",
    "finetuning on the one hand, and can improve the memory and latency of inference\n",
    "on the other hand. From a scientific perspective, the robustness of these LLMs\n",
    "to the deletion of layers implies either that current pretraining methods are not\n",
    "properly leveraging the parameters in the deeper layers of the network or that the\n",
    "shallow layers play a critical role in storing knowledge.\"\"\",\n",
    "    \"\"\"In this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\n",
    "is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\n",
    "BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\n",
    "1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\n",
    "operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\n",
    "consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\n",
    "throughput and latency compared to FP16 LLM baselines.\"\"\",\n",
    "]\n",
    "\n",
    "# 2. Encode\n",
    "embeddings = model.encode(docs)\n",
    "\n",
    "# Optional: Quantize the embeddings\n",
    "binary_embeddings = quantize_embeddings(embeddings, precision=\"ubinary\")\n",
    "\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print('similarities:', similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "matryoshka_dim = 512\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "sentences = [\n",
    "    \"\"\"search_document: Within the field of vector search, an intriguing development has arisen: binary vector search. This approach shows promise in tackling the long-standing issue of memory consumption by achieving a remarkable 30x reduction. However, a critical aspect that sparks debate is its effect on accuracy.\n",
    "\n",
    "We believe that using binary vector search, along with specific optimization techniques, can maintain similar accuracy. To provide clarity on this subject, we showcase a series of experiments that will demonstrate the effects and implications of this approach.\"\"\",\n",
    "    \"\"\"search_document: We empirically study a simple layer-pruning strategy for popular families of openweight pretrained LLMs, finding minimal degradation of performance on different\n",
    "question-answering benchmarks until after a large fraction (up to half) of the layers\n",
    "are removed. To prune these models, we identify the optimal block of layers to\n",
    "prune by considering similarity across layers; then, to “heal” the damage, we\n",
    "perform a small amount of finetuning. In particular, we use parameter-efficient\n",
    "finetuning (PEFT) methods, specifically quantization and Low Rank Adapters\n",
    "(QLoRA), such that each of our experiments can be performed on a single A100\n",
    "GPU. From a practical perspective, these results suggest that layer pruning methods\n",
    "can complement other PEFT strategies to further reduce computational resources of\n",
    "finetuning on the one hand, and can improve the memory and latency of inference\n",
    "on the other hand. From a scientific perspective, the robustness of these LLMs\n",
    "to the deletion of layers implies either that current pretraining methods are not\n",
    "properly leveraging the parameters in the deeper layers of the network or that the\n",
    "shallow layers play a critical role in storing knowledge.\"\"\",\n",
    "    \"\"\"search_document: In this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\n",
    "is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\n",
    "BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\n",
    "1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\n",
    "operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\n",
    "consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\n",
    "throughput and latency compared to FP16 LLM baselines.\"\"\",\n",
    "]\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n",
    "#embeddings = embeddings[:, :matryoshka_dim]\n",
    "#embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"search_query: are all layers in a transformer equally important?\"\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"jinaai/jina-embeddings-v2-base-en\", # switch to en/zh for English or Chinese\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# control your input sequence length up to 8192\n",
    "model.max_seq_length = 1024\n",
    "\n",
    "embeddings = model.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I spent the last 30 minutes comparing embedding models before realizing that this doesn't matter right now. we'll come back to this decision when we have a framework to populate the embedding space, at this stage any decent model will do. i'll leave the links to the prospects for future reference\n",
    "\n",
    "1. https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n",
    "2. https://huggingface.co/jinaai/jina-embeddings-v2-base-en\n",
    "3. https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\n",
    "\n",
    "for no specific reason let's move forward with mixedbread's embedder for now..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we've settled on an embedding model, the next major consideration is how to handle the varying content lengths of our documents. i want to be very specific in my search queries, far more so than what embedding only the abstract of a paper allows. for instance, i know that the llama 2 paper contains detailed ablations comparing mqa, gqa, and mha; and i should easily be able to query for that. so what granularity of document chunking does this specificity require? i don't know. but we'll find out together! starting of with the most naive approach; just embed the entire document!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's download and store our content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Tuple\n",
    "\n",
    "def download(url: str) -> Tuple[str, str]:\n",
    "    \"\"\"Download content from the given URL, determine its type, and extract the title and text.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    content_type = response.headers['Content-Type']\n",
    "\n",
    "    def _parse_html(content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Extract the title and text content from HTML data.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        title = soup.find('title').text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return title, text\n",
    "    \n",
    "    def _parse_pdf(content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Extract the title and text content from a PDF file.\n",
    "        \"\"\"\n",
    "        filename = 'document.pdf'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        doc = fitz.open(filename)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Extract title from metadata or the first block of text\n",
    "        metadata = doc.metadata\n",
    "        title = metadata.get('title', '')\n",
    "        if not title:\n",
    "            first_page = doc[0]\n",
    "            blocks = first_page.get_text(\"blocks\")\n",
    "            \n",
    "            # Assuming the title is in the first block\n",
    "            if blocks:\n",
    "                title = blocks[0][4].strip()\n",
    "            else:\n",
    "                title = 'unknown'\n",
    "\n",
    "        return title, text\n",
    "\n",
    "    if 'text/html' in content_type:\n",
    "        title, text = _parse_html(response.content)\n",
    "    elif 'application/pdf' in content_type:\n",
    "        title, text = _parse_pdf(response.content)\n",
    "    else:\n",
    "        raise Exception('Unsupported content type')\n",
    "    \n",
    "    # Save content to file\n",
    "    with open(f'data/{title}.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above is straightforward. most of our content will either be html parsable by beautifulsoup or a pdf, which we can handle using this [pymupdf](https://pymupdf.readthedocs.io/en/latest/the-basics.html) package i found. i’ve future-proofed by implementing download(url), which takes a url and downloads the content to a .txt file. we’ll revisit the save format in the future; perhaps some kind of json structure, given that we’ll want to link content chunks to urls. however, that is for future me to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://arxiv.org/pdf/2401.01325\")\n",
    "download(\"https://arxiv.org/pdf/2310.02207\")\n",
    "download(\"https://huggingface.co/blog/moe#:~:text=Mixture%20of%20Experts%20enable%20models,budget%20as%20a%20dense%20model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i've downloaded three papers that i'm going to embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n",
    "\n",
    "query_prefix = 'Represent this sentence for searching relevant passages: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load all documents in the data folder, returning a list of text content\n",
    "def load(dir: str) -> list[str]:\n",
    "    data = []\n",
    "    for filename in os.listdir(dir):\n",
    "        with open(f'data/{filename}', 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load('data')\n",
    "db_embedding = model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str) -> list[str]:\n",
    "    query = query_prefix + query\n",
    "    query_embedding = model.encode(query)\n",
    "    similarities = cos_sim(db_embedding, query_embedding)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've got three papers embedded in our database. this is a list of specific concepts from one of these papers:\n",
    "\n",
    "1. positional out-of-distribution\n",
    "2. passkey retrieval\n",
    "3. context extension without fine-tuning\n",
    "4. RoPE\n",
    "5. selfextend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = [\"positional out-of-distribution\", \"passkey retrieval\",\n",
    "                  \"context extension without fine-tuning\", \"RoPE\",\n",
    "                  \"selfextend\"]\n",
    "\n",
    "for sq in search_queries:\n",
    "    print(f\"Query: {sq}\")\n",
    "    results = search(sq)\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"Document {i+1}: {r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concepts are all extracted from Document 3. Note the wavering stability in similarity between the query and doc 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we split each document into N chunks, linking each chunk back to its source document. This should drastically increase similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(documents: list[str], N: int) -> Tuple[list[str], list[int]]:\n",
    "    \"\"\" Split documents into 512-word segments. \"\"\"\n",
    "    prev_chunk_index = 0\n",
    "    chunk_index = []\n",
    "    document_chunks = []\n",
    "    for _, doc in enumerate(documents):\n",
    "        # chunk document into 512-word segments\n",
    "        chunks = [doc[i:i+N] for i in range(0, len(doc), N)]\n",
    "        if len(chunks[-1]) != N:\n",
    "            chunks.pop()\n",
    "        \n",
    "        chunk_index.append(prev_chunk_index + len(chunks))\n",
    "        document_chunks.extend(chunks)\n",
    "        prev_chunk_index = chunk_index[-1]\n",
    "\n",
    "    return document_chunks, chunk_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the documents into fixed-size segments. For each document, create segments of a specified length, ensuring uniformity by discarding smaller, incomplete segments. Maintain a cumulative count of segments for each document and compile all segments into a single list. The function returns both the list of segments and the cumulative segment indices for tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import bisect\n",
    "\n",
    "def search_top_k(query: str, top_k: int, n_chunks: list[int]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given a query, return the top-k most relevant documents, by index in n_chunks.\n",
    "    \"\"\"\n",
    "    query = query_prefix + query\n",
    "    query_embedding = model.encode(query)\n",
    "    similarities = cos_sim(db_embedding, query_embedding).numpy()\n",
    "    top_k_indices = numpy.argsort(similarities)[:top_k]\n",
    "    \n",
    "    source_documents = []\n",
    "    for ind in top_k_indices:\n",
    "        doc = bisect.bisect_right(n_chunks, ind)\n",
    "        source_documents.append(doc)\n",
    "\n",
    "    return source_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i'm still uncertain on the implementation of search, it depends on what we want to display as results of a query. In it's current state, search will return the documents that match the top_k chunks in the embedding search, without filtering out multiple pointers to the same source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static per db\n",
    "documents = load('data')\n",
    "chunks, n_chunks = chunk(documents, 512) \n",
    "db_embedding = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = [\"positional out-of-distribution\", \"passkey retrieval\",\n",
    "                  \"context extension without fine-tuning\", \"RoPE\",\n",
    "                  \"selfextend\"]\n",
    "\n",
    "for sq in search_queries:\n",
    "    print(f\"Query: {sq}\")\n",
    "    result = search_top_k(sq, 3, n_chunks)\n",
    "    print(f\"Top-3 documents: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't surprising, but encouraging nonetheless. We need to remember that we're going to introduce a re-ranking algorithm around here as well. We'll re-rank on the chunked embeddings, then comes the question whether we want to completely disregard chunks that pertain to the same source document or not, this depends on what we want to display in the results of the search; just the source document or the best matching chunk(s) as well. \n",
    "\n",
    "you know what, on second thought, after re-ranking the embedding search results we could actually: (1) **sum** the similarity scores per source document or (2) count the number of pointers to the source document! This would improve the search, prioritizing documents that contain multiple matches to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's gather everything that we're using down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Tuple\n",
    "\n",
    "def download(url: str) -> Tuple[str, str]:\n",
    "    \"\"\"Download content from the given URL, determine its type, and extract the title and text.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    content_type = response.headers['Content-Type']\n",
    "\n",
    "    def _parse_html(content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Extract the title and text content from HTML data.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        title = soup.find('title').text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return title, text\n",
    "    \n",
    "    def _parse_pdf(content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Extract the title and text content from a PDF file.\n",
    "        \"\"\"\n",
    "        filename = 'document.pdf'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        doc = fitz.open(filename)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Extract title from metadata or the first block of text\n",
    "        metadata = doc.metadata\n",
    "        title = metadata.get('title', '')\n",
    "        if not title:\n",
    "            first_page = doc[0]\n",
    "            blocks = first_page.get_text(\"blocks\")\n",
    "            \n",
    "            # Assuming the title is in the first block\n",
    "            if blocks:\n",
    "                title = blocks[0][4].strip()\n",
    "            else:\n",
    "                title = 'unknown'\n",
    "\n",
    "        return title, text\n",
    "\n",
    "    if 'text/html' in content_type:\n",
    "        title, text = _parse_html(response.content)\n",
    "    elif 'application/pdf' in content_type:\n",
    "        title, text = _parse_pdf(response.content)\n",
    "    else:\n",
    "        raise Exception('Unsupported content type')\n",
    "    \n",
    "    # Save content to file\n",
    "    with open(f'data/{title}.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    return title, text\n",
    "\n",
    "def load(dir: str) -> list[str]:\n",
    "    \"\"\" Load all documents in the data folder, returning a list of text content.\"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(dir):\n",
    "        with open(f'data/{filename}', 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def chunk(documents: list[str], N: int) -> Tuple[list[str], list[int]]:\n",
    "    \"\"\" Split documents into 512-word segments. \"\"\"\n",
    "    prev_chunk_index = 0\n",
    "    chunk_index = []\n",
    "    document_chunks = []\n",
    "    for _, doc in enumerate(documents):\n",
    "        # chunk document into 512-word segments\n",
    "        chunks = [doc[i:i+N] for i in range(0, len(doc), N)]\n",
    "        if len(chunks[-1]) != N:\n",
    "            chunks.pop()\n",
    "        \n",
    "        chunk_index.append(prev_chunk_index + len(chunks))\n",
    "        document_chunks.extend(chunks)\n",
    "        prev_chunk_index = chunk_index[-1]\n",
    "\n",
    "    return document_chunks, chunk_index\n",
    "\n",
    "def search_top_k(query: str, top_k: int) -> Tuple[list[int], list[int]]:\n",
    "    \"\"\"\n",
    "    Given a query, find top-k document chunk matches by cosine similarity.\n",
    "    \"\"\"\n",
    "    query = query_prefix + query\n",
    "    query_embedding = model.encode(query)\n",
    "    similarities = cos_sim(db_embedding, query_embedding).numpy()\n",
    "    top_k_indices = np.argsort(similarities, axis=0, )[-top_k:][::-1]\n",
    "    \n",
    "    return top_k_indices.flatten(), similarities[top_k_indices].flatten()\n",
    "\n",
    "def aggregate_and_sort(documents, scores):\n",
    "    \"\"\" Aggregate scores by source document and sort the documents by new scores.\"\"\"\n",
    "\n",
    "    unique_docs, inverse_indices = np.unique(documents, return_inverse=True)\n",
    "    \n",
    "    # aggregate scores \n",
    "    aggregated_scores = np.bincount(inverse_indices, weights=scores)\n",
    "    \n",
    "    # sort descending order\n",
    "    sorted_indices = np.argsort(-aggregated_scores)\n",
    "    \n",
    "    sorted_documents = unique_docs[sorted_indices]\n",
    "    sorted_scores = aggregated_scores[sorted_indices]\n",
    "    \n",
    "    return sorted_documents, sorted_scores\n",
    "\n",
    "def search(query: str, top_k: int, n_chunks: list[int]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Search the embedding database for the most relevant documents to the query. \n",
    "    \"\"\"\n",
    "    \n",
    "    top_k_indices, scores = search_top_k(query, top_k)\n",
    "    # re-rank the top-k document chunks\n",
    "    top_k_documents = numpy.searchsorted(n_chunks, top_k_indices, side='right')\n",
    "    top_k_documents = aggregate_and_sort(top_k_documents, scores)\n",
    "\n",
    "    return top_k_documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n",
    "\n",
    "query_prefix = 'Represent this sentence for searching relevant passages: '\n",
    "\n",
    "# load data and initialize embedded db\n",
    "documents = load('data')\n",
    "chunks, n_chunks = chunk(documents, 512) \n",
    "db_embedding = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([2.03844404]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"positional out-of-distribution\", 3, n_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is everything we've got so far. i've cleaned up the search code, it now aggregates similarity scores pertaining to the same\n",
    "source document. we also have easy access to the chunk matches if we need them down the line. \n",
    "\n",
    "we're still missing a re-ranking algorithm. \n",
    "\n",
    "i'm keen on changing the whole n_chunks / document setup we've got right now. i feel like this is a band-aid solution that isn't going to hold up long term. we need to think of a better way to connect documents in /data to chunks. there should also be a link to the source url. this needs to be saved to disk along side the embedding database. it needs to be extendable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
