{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why have I created this repo? I like to read a lot; papers, blog posts, twitter threads, notebooks, you name it. When I read, i've been quite well at documenting this through my blog. The blog let's me sement my thoughts, improve retention, and log what i've read. it can be quite tedious work, and sometimes I simply don't have the time, or rather capacity to do it, but i've been quite good at it, at least up until a couple of months ago. now, even though I sometimes write quite extensive summaries or thoughts about papers, i've been pretty poor at looking back at my notes when i need to remember something in a paper i read, and i attribute this mainly to a lack of **indexing**. the process often looks like this: \"ooh, i know i've read something about X in a paper\" *scrolls through all [blog entries](https://leonericsson.github.io/indexer), not finding what i'm looking for immediately and just give up*. yes, i could just become more patient and keep looking, why not instead take this opportunity and build something! this is the motivation for this repo. \n",
    "\n",
    "my initial plan. before doing any valuable research, here's what i **think** is the way i want to solve this problem, including some questions that i yet don't know the answer to:\n",
    "\n",
    "1. save the link to every piece of research content i've read, that i can recall and find (i've already got a decent chunk saved).\n",
    "2. download the content.\n",
    "    - should everything be standardized into a format?\n",
    "    - do i need to be careful to exclude irrelevant information from the source?\n",
    "    - how do i deal with twitter threads?\n",
    "3. decide on an embedding model\n",
    "    - what is a good context size? relates to how we plan to chunk a 20 page paper.\n",
    "4. embed content\n",
    "    - how do we deal with long format texts e.g. papers?\n",
    "    - does the search engine need to rely on more than just a single embedding model?\n",
    "5. quantize embeddings to int8\n",
    "    - does this process impact the selected embedding model?\n",
    "5. index the embeddings\n",
    "6. re-ranker\n",
    "7. search engine\n",
    "     - how do we link the source content + source url to the search results? especially\n",
    "     considering that we will surely have multiple embedding points from the same source content.\n",
    "    \n",
    "okay, this seems like a good place to start don't you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comparing embedding models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n",
    "\n",
    "# For retrieval you need to pass this prompt.\n",
    "query = 'Represent this sentence for searching relevant passages: are all layers in a transformer equally important?'\n",
    "\n",
    "docs = [\n",
    "    query,\n",
    "    \"\"\"Within the field of vector search, an intriguing development has arisen: binary vector search. This approach shows promise in tackling the long-standing issue of memory consumption by achieving a remarkable 30x reduction. However, a critical aspect that sparks debate is its effect on accuracy.\n",
    "\n",
    "We believe that using binary vector search, along with specific optimization techniques, can maintain similar accuracy. To provide clarity on this subject, we showcase a series of experiments that will demonstrate the effects and implications of this approach.\"\"\",\n",
    "    \"\"\"We empirically study a simple layer-pruning strategy for popular families of openweight pretrained LLMs, finding minimal degradation of performance on different\n",
    "question-answering benchmarks until after a large fraction (up to half) of the layers\n",
    "are removed. To prune these models, we identify the optimal block of layers to\n",
    "prune by considering similarity across layers; then, to “heal” the damage, we\n",
    "perform a small amount of finetuning. In particular, we use parameter-efficient\n",
    "finetuning (PEFT) methods, specifically quantization and Low Rank Adapters\n",
    "(QLoRA), such that each of our experiments can be performed on a single A100\n",
    "GPU. From a practical perspective, these results suggest that layer pruning methods\n",
    "can complement other PEFT strategies to further reduce computational resources of\n",
    "finetuning on the one hand, and can improve the memory and latency of inference\n",
    "on the other hand. From a scientific perspective, the robustness of these LLMs\n",
    "to the deletion of layers implies either that current pretraining methods are not\n",
    "properly leveraging the parameters in the deeper layers of the network or that the\n",
    "shallow layers play a critical role in storing knowledge.\"\"\",\n",
    "    \"\"\"In this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\n",
    "is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\n",
    "BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\n",
    "1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\n",
    "operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\n",
    "consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\n",
    "throughput and latency compared to FP16 LLM baselines.\"\"\",\n",
    "]\n",
    "\n",
    "# 2. Encode\n",
    "embeddings = model.encode(docs)\n",
    "\n",
    "# Optional: Quantize the embeddings\n",
    "binary_embeddings = quantize_embeddings(embeddings, precision=\"ubinary\")\n",
    "\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print('similarities:', similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "matryoshka_dim = 512\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "sentences = [\n",
    "    \"\"\"search_document: Within the field of vector search, an intriguing development has arisen: binary vector search. This approach shows promise in tackling the long-standing issue of memory consumption by achieving a remarkable 30x reduction. However, a critical aspect that sparks debate is its effect on accuracy.\n",
    "\n",
    "We believe that using binary vector search, along with specific optimization techniques, can maintain similar accuracy. To provide clarity on this subject, we showcase a series of experiments that will demonstrate the effects and implications of this approach.\"\"\",\n",
    "    \"\"\"search_document: We empirically study a simple layer-pruning strategy for popular families of openweight pretrained LLMs, finding minimal degradation of performance on different\n",
    "question-answering benchmarks until after a large fraction (up to half) of the layers\n",
    "are removed. To prune these models, we identify the optimal block of layers to\n",
    "prune by considering similarity across layers; then, to “heal” the damage, we\n",
    "perform a small amount of finetuning. In particular, we use parameter-efficient\n",
    "finetuning (PEFT) methods, specifically quantization and Low Rank Adapters\n",
    "(QLoRA), such that each of our experiments can be performed on a single A100\n",
    "GPU. From a practical perspective, these results suggest that layer pruning methods\n",
    "can complement other PEFT strategies to further reduce computational resources of\n",
    "finetuning on the one hand, and can improve the memory and latency of inference\n",
    "on the other hand. From a scientific perspective, the robustness of these LLMs\n",
    "to the deletion of layers implies either that current pretraining methods are not\n",
    "properly leveraging the parameters in the deeper layers of the network or that the\n",
    "shallow layers play a critical role in storing knowledge.\"\"\",\n",
    "    \"\"\"search_document: In this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\n",
    "is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\n",
    "BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\n",
    "1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\n",
    "operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\n",
    "consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\n",
    "throughput and latency compared to FP16 LLM baselines.\"\"\",\n",
    "]\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n",
    "#embeddings = embeddings[:, :matryoshka_dim]\n",
    "#embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"search_query: are all layers in a transformer equally important?\"\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"jinaai/jina-embeddings-v2-base-en\", # switch to en/zh for English or Chinese\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# control your input sequence length up to 8192\n",
    "model.max_seq_length = 1024\n",
    "\n",
    "embeddings = model.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I spent the last 30 minutes comparing embedding models before realizing that this doesn't matter right now. we'll come back to this decision when we have a framework to populate the embedding space, at this stage any decent model will do. i'll leave the links to the prospects for future reference\n",
    "\n",
    "1. https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n",
    "2. https://huggingface.co/jinaai/jina-embeddings-v2-base-en\n",
    "3. https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\n",
    "\n",
    "for no specific reason let's move forward with mixedbread's embedder for now..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we've settled on an embedding model, the next major consideration is how to handle the varying content lengths of our documents. i want to be very specific in my search queries, far more so than embedding only the abstract of a paper allows. for instance, i know that the llama 2 paper contains detailed ablations comparing mqa, gqa, and mha, so i definitely want to be able to query for those specific ablations. this raises the question of what granularity of document chunking is required. to which i currently don't know the answer, but we'll investigate this together! starting of with the most naive approach; just embed the entire document!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
