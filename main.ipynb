{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why have I created this repo? I like to read a lot; papers, blog posts, twitter threads, notebooks, you name it. When I read, i've been quite well at documenting this through my blog. The blog let's me sement my thoughts, improve retention, and log what i've read. it can be quite tedious work, and sometimes I simply don't have the time, or rather capacity to do it, but i've been quite good at it, at least up until a couple of months ago. now, even though I sometimes write quite extensive summaries or thoughts about papers, i've been pretty poor at looking back at my notes when i need to remember something in a paper i read, and i attribute this mainly to a lack of **indexing**. the process often looks like this: \"ooh, i know i've read something about X in a paper\" *scrolls through all [blog entries](https://leonericsson.github.io/indexer), not finding what i'm looking for immediately and just give up*. yes, i could just become more patient and keep looking, why not instead take this opportunity and build something! this is the motivation for this repo. \n",
    "\n",
    "my initial plan. before doing any valuable research, here's what i **think** is the way i want to solve this problem, including some questions that i yet don't know the answer to:\n",
    "\n",
    "1. save the link to every piece of research content i've read, that i can recall and find (i've already got a decent chunk saved).\n",
    "2. download the content.\n",
    "    - should everything be standardized into a format?\n",
    "    - do i need to be careful to exclude irrelevant information from the source?\n",
    "    - how do i deal with twitter threads?\n",
    "3. decide on an embedding model\n",
    "    - what is a good context size? relates to how we plan to chunk a 20 page paper.\n",
    "4. embed content\n",
    "    - how do we deal with long format texts e.g. papers?\n",
    "    - does the search engine need to rely on more than just a single embedding model?\n",
    "5. quantize embeddings to int8\n",
    "    - does this process impact the selected embedding model?\n",
    "5. index the embeddings\n",
    "6. re-ranker\n",
    "7. search engine\n",
    "     - how do we link the source content + source url to the search results? especially\n",
    "     considering that we will surely have multiple embedding points from the same source content.\n",
    "    \n",
    "okay, this seems like a good place to start don't you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comparing embedding models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "model_id = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n",
    "\n",
    "# For retrieval you need to pass this prompt.\n",
    "query = 'Represent this sentence for searching relevant passages: are all layers in a transformer equally important?'\n",
    "\n",
    "docs = [\n",
    "    query,\n",
    "    \"\"\"Within the field of vector search, an intriguing development has arisen: binary vector search. This approach shows promise in tackling the long-standing issue of memory consumption by achieving a remarkable 30x reduction. However, a critical aspect that sparks debate is its effect on accuracy.\n",
    "\n",
    "We believe that using binary vector search, along with specific optimization techniques, can maintain similar accuracy. To provide clarity on this subject, we showcase a series of experiments that will demonstrate the effects and implications of this approach.\"\"\",\n",
    "    \"\"\"We empirically study a simple layer-pruning strategy for popular families of openweight pretrained LLMs, finding minimal degradation of performance on different\n",
    "question-answering benchmarks until after a large fraction (up to half) of the layers\n",
    "are removed. To prune these models, we identify the optimal block of layers to\n",
    "prune by considering similarity across layers; then, to “heal” the damage, we\n",
    "perform a small amount of finetuning. In particular, we use parameter-efficient\n",
    "finetuning (PEFT) methods, specifically quantization and Low Rank Adapters\n",
    "(QLoRA), such that each of our experiments can be performed on a single A100\n",
    "GPU. From a practical perspective, these results suggest that layer pruning methods\n",
    "can complement other PEFT strategies to further reduce computational resources of\n",
    "finetuning on the one hand, and can improve the memory and latency of inference\n",
    "on the other hand. From a scientific perspective, the robustness of these LLMs\n",
    "to the deletion of layers implies either that current pretraining methods are not\n",
    "properly leveraging the parameters in the deeper layers of the network or that the\n",
    "shallow layers play a critical role in storing knowledge.\"\"\",\n",
    "    \"\"\"In this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\n",
    "is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\n",
    "BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\n",
    "1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\n",
    "operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\n",
    "consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\n",
    "throughput and latency compared to FP16 LLM baselines.\"\"\",\n",
    "]\n",
    "\n",
    "# 2. Encode\n",
    "embeddings = model_id.encode(docs)\n",
    "\n",
    "# Optional: Quantize the embeddings\n",
    "binary_embeddings = quantize_embeddings(embeddings, precision=\"ubinary\")\n",
    "\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print('similarities:', similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "matryoshka_dim = 512\n",
    "\n",
    "model_id = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "\n",
    "sentences = [\n",
    "    \"\"\"search_document: Within the field of vector search, an intriguing development has arisen: binary vector search. This approach shows promise in tackling the long-standing issue of memory consumption by achieving a remarkable 30x reduction. However, a critical aspect that sparks debate is its effect on accuracy.\n",
    "\n",
    "We believe that using binary vector search, along with specific optimization techniques, can maintain similar accuracy. To provide clarity on this subject, we showcase a series of experiments that will demonstrate the effects and implications of this approach.\"\"\",\n",
    "    \"\"\"search_document: We empirically study a simple layer-pruning strategy for popular families of openweight pretrained LLMs, finding minimal degradation of performance on different\n",
    "question-answering benchmarks until after a large fraction (up to half) of the layers\n",
    "are removed. To prune these models, we identify the optimal block of layers to\n",
    "prune by considering similarity across layers; then, to “heal” the damage, we\n",
    "perform a small amount of finetuning. In particular, we use parameter-efficient\n",
    "finetuning (PEFT) methods, specifically quantization and Low Rank Adapters\n",
    "(QLoRA), such that each of our experiments can be performed on a single A100\n",
    "GPU. From a practical perspective, these results suggest that layer pruning methods\n",
    "can complement other PEFT strategies to further reduce computational resources of\n",
    "finetuning on the one hand, and can improve the memory and latency of inference\n",
    "on the other hand. From a scientific perspective, the robustness of these LLMs\n",
    "to the deletion of layers implies either that current pretraining methods are not\n",
    "properly leveraging the parameters in the deeper layers of the network or that the\n",
    "shallow layers play a critical role in storing knowledge.\"\"\",\n",
    "    \"\"\"search_document: In this work, we introduce a significant 1-bit LLM variant called BitNet b1.58, where every parameter\n",
    "is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit\n",
    "BitNet, resulting in 1.58 bits in the binary system. BitNet b1.58 retains all the benefits of the original\n",
    "1-bit BitNet, including its new computation paradigm, which requires almost no multiplication\n",
    "operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy\n",
    "consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption,\n",
    "throughput and latency compared to FP16 LLM baselines.\"\"\",\n",
    "]\n",
    "embeddings = model_id.encode(sentences, convert_to_tensor=True)\n",
    "embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n",
    "#embeddings = embeddings[:, :matryoshka_dim]\n",
    "#embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"search_query: are all layers in a transformer equally important?\"\n",
    "query_embedding = model_id.encode(query, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "model_id = SentenceTransformer(\n",
    "    \"jinaai/jina-embeddings-v2-base-en\", # switch to en/zh for English or Chinese\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# control your input sequence length up to 8192\n",
    "model_id.max_seq_length = 1024\n",
    "\n",
    "embeddings = model_id.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I spent the last 30 minutes comparing embedding models before realizing that this doesn't matter right now. we'll come back to this decision when we have a framework to populate the embedding space, at this stage any decent model will do. i'll leave the links to the prospects for future reference\n",
    "\n",
    "1. https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n",
    "2. https://huggingface.co/jinaai/jina-embeddings-v2-base-en\n",
    "3. https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\n",
    "\n",
    "for no specific reason let's move forward with mixedbread's embedder for now..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we've settled on an embedding model, the next major consideration is how to handle the varying content lengths of our documents. i want to be very specific in my search queries, far more so than what embedding only the abstract of a paper allows. for instance, i know that the llama 2 paper contains detailed ablations comparing mqa, gqa, and mha; and i should easily be able to query for that. so what granularity of document chunking does this specificity require? i don't know. but we'll find out together! starting of with the most naive approach; just embed the entire document!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's download and store our content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Tuple\n",
    "\n",
    "def download(url: str) -> Tuple[str, str]:\n",
    "    \"\"\"Download content from the given URL, determine its type, and extract the title and text.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    content_type = response.headers['Content-Type']\n",
    "\n",
    "    def _parse_html(content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Extract the title and text content from HTML data.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        title = soup.find('title').text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return title, text\n",
    "    \n",
    "    def _parse_pdf(content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Extract the title and text content from a PDF file.\n",
    "        \"\"\"\n",
    "        filename = 'document.pdf'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        doc = fitz.open(filename)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        \n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Extract title from metadata or the first block of text\n",
    "        metadata = doc.metadata\n",
    "        title = metadata.get('title', '')\n",
    "        if not title:\n",
    "            first_page = doc[0]\n",
    "            blocks = first_page.get_text(\"blocks\")\n",
    "            \n",
    "            # Assuming the title is in the first block\n",
    "            if blocks:\n",
    "                title = blocks[0][4].strip()\n",
    "            else:\n",
    "                title = 'unknown'\n",
    "\n",
    "        return title, text\n",
    "\n",
    "    if 'text/html' in content_type:\n",
    "        title, text = _parse_html(response.content)\n",
    "    elif 'application/pdf' in content_type:\n",
    "        title, text = _parse_pdf(response.content)\n",
    "    else:\n",
    "        raise Exception('Unsupported content type')\n",
    "    \n",
    "    # Save content to file\n",
    "    with open(f'data/{title}.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above is straightforward. most of our content will either be html parsable by beautifulsoup or a pdf, which we can handle using this [pymupdf](https://pymupdf.readthedocs.io/en/latest/the-basics.html) package i found. i’ve future-proofed by implementing download(url), which takes a url and downloads the content to a .txt file. we’ll revisit the save format in the future; perhaps some kind of json structure, given that we’ll want to link content chunks to urls. however, that is for future me to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(\"https://arxiv.org/pdf/2401.01325\")\n",
    "download(\"https://arxiv.org/pdf/2310.02207\")\n",
    "download(\"https://huggingface.co/blog/moe#:~:text=Mixture%20of%20Experts%20enable%20models,budget%20as%20a%20dense%20model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i've downloaded three papers that i'm going to embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "model_id = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n",
    "\n",
    "query_prefix = 'Represent this sentence for searching relevant passages: '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load all documents in the data folder, returning a list of text content\n",
    "def load(dir: str) -> list[str]:\n",
    "    data = []\n",
    "    for filename in os.listdir(dir):\n",
    "        with open(f'data/{filename}', 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load('data')\n",
    "index = model_id.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str) -> list[str]:\n",
    "    query = query_prefix + query\n",
    "    query_embedding = model_id.encode(query)\n",
    "    similarities = cos_sim(index, query_embedding)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've got three papers embedded in our database. this is a list of specific concepts from one of these papers:\n",
    "\n",
    "1. positional out-of-distribution\n",
    "2. passkey retrieval\n",
    "3. context extension without fine-tuning\n",
    "4. RoPE\n",
    "5. selfextend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = [\"positional out-of-distribution\", \"passkey retrieval\",\n",
    "                  \"context extension without fine-tuning\", \"RoPE\",\n",
    "                  \"selfextend\"]\n",
    "\n",
    "for sq in search_queries:\n",
    "    print(f\"Query: {sq}\")\n",
    "    results = search(sq)\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"Document {i+1}: {r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concepts are all extracted from Document 3. Note the wavering stability in similarity between the query and doc 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we split each document into N chunks, linking each chunk back to its source document. This should drastically increase similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(documents: list[str], N: int) -> Tuple[list[str], list[int]]:\n",
    "    \"\"\" Split documents into 512-word segments. \"\"\"\n",
    "    prev_chunk_index = 0\n",
    "    chunk_index = []\n",
    "    document_chunks = []\n",
    "    for _, doc in enumerate(documents):\n",
    "        # chunk document into 512-word segments\n",
    "        chunks = [doc[i:i+N] for i in range(0, len(doc), N)]\n",
    "        if len(chunks[-1]) != N:\n",
    "            chunks.pop()\n",
    "        \n",
    "        chunk_index.append(prev_chunk_index + len(chunks))\n",
    "        document_chunks.extend(chunks)\n",
    "        prev_chunk_index = chunk_index[-1]\n",
    "\n",
    "    return document_chunks, chunk_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the documents into fixed-size segments. For each document, create segments of a specified length, ensuring uniformity by discarding smaller, incomplete segments. Maintain a cumulative count of segments for each document and compile all segments into a single list. The function returns both the list of segments and the cumulative segment indices for tracking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import bisect\n",
    "\n",
    "def search_top_k(query: str, top_k: int, n_chunks: list[int]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given a query, return the top-k most relevant documents, by index in n_chunks.\n",
    "    \"\"\"\n",
    "    query = query_prefix + query\n",
    "    query_embedding = model_id.encode(query)\n",
    "    similarities = cos_sim(index, query_embedding).numpy()\n",
    "    top_k_indices = numpy.argsort(similarities)[:top_k]\n",
    "    \n",
    "    source_documents = []\n",
    "    for ind in top_k_indices:\n",
    "        doc = bisect.bisect_right(n_chunks, ind)\n",
    "        source_documents.append(doc)\n",
    "\n",
    "    return source_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i'm still uncertain on the implementation of search, it depends on what we want to display as results of a query. In it's current state, search will return the documents that match the top_k chunks in the embedding search, without filtering out multiple pointers to the same source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static per db\n",
    "documents = load('data')\n",
    "new_chunks, n_chunks = chunk(documents, 512) \n",
    "index = model_id.encode(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = [\"positional out-of-distribution\", \"passkey retrieval\",\n",
    "                  \"context extension without fine-tuning\", \"RoPE\",\n",
    "                  \"selfextend\"]\n",
    "\n",
    "for sq in search_queries:\n",
    "    print(f\"Query: {sq}\")\n",
    "    result = search_top_k(sq, 3, n_chunks)\n",
    "    print(f\"Top-3 documents: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't surprising, but encouraging nonetheless. We need to remember that we're going to introduce a re-ranking algorithm around here as well. We'll re-rank on the chunked embeddings, then comes the question whether we want to completely disregard chunks that pertain to the same source document or not, this depends on what we want to display in the results of the search; just the source document or the best matching chunk(s) as well. \n",
    "\n",
    "you know what, on second thought, after re-ranking the embedding search results we could actually: (1) **sum** the similarity scores per source document or (2) count the number of pointers to the source document! This would improve the search, prioritizing documents that contain multiple matches to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's gather everything that we're using down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Tuple, List\n",
    "\n",
    "Array = np.ndarray\n",
    "\n",
    "# Constants\n",
    "CHUNK_SIZE = 512\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "def download(url: str) -> Tuple[str, str]:\n",
    "    \"\"\"Download content from the given URL, determine its type, and extract the title and text.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    content_type = response.headers['Content-Type']\n",
    "\n",
    "    if 'text/html' in content_type:\n",
    "        title, text = _parse_html(response.content)\n",
    "    elif 'application/pdf' in content_type:\n",
    "        title, text = _parse_pdf(response.content)\n",
    "    else:\n",
    "        raise Exception('Unsupported content type')\n",
    "    \n",
    "    # Save content to file\n",
    "    with open(f'{DATA_DIR}/{title}.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    return title, text\n",
    "\n",
    "def _parse_html(content: bytes) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract the title and text content from HTML data.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    title = soup.find('title').text\n",
    "    text = _clean_text(soup.get_text())\n",
    "    \n",
    "    return title, text\n",
    "    \n",
    "def _parse_pdf(content: bytes) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract the title and text content from a PDF file.\n",
    "    \"\"\"\n",
    "    filename = 'temp.pdf'\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    doc = fitz.open(filename)\n",
    "    text = ' '.join([page.get_text() for page in doc])\n",
    "    text = _clean_text(text)\n",
    "\n",
    "    # Extract title from metadata or the first block of text\n",
    "    title = doc.metadata.get('title', _extract_first_block_text(doc[0]))\n",
    "\n",
    "    os.remove(filename)\n",
    "    return title, text\n",
    "\n",
    "def _clean_text(text: str) -> str:\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def _extract_first_block_text(page) -> str:\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    return blocks[0][4].strip() if blocks else 'unknown'\n",
    "\n",
    "def load(dir: str) -> list[str]:\n",
    "    \"\"\" Load all documents in the data folder, returning a list of text content.\"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(dir):\n",
    "        with open(f'{DATA_DIR}/{filename}', 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def chunk(documents: list[str], N: int = CHUNK_SIZE) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\" Split documents into 512-word segments. \"\"\"\n",
    "    prev_chunk_index = 0\n",
    "    chunk_index = []\n",
    "    document_chunks = []\n",
    "    for _, doc in enumerate(documents):\n",
    "        # chunk document into 512-word segments\n",
    "        chunks = [doc[i:i+N] for i in range(0, len(doc), N)]\n",
    "        if len(chunks[-1]) != N:\n",
    "            chunks.pop()\n",
    "        \n",
    "        chunk_index.append(prev_chunk_index + len(chunks))\n",
    "        document_chunks.extend(chunks)\n",
    "        prev_chunk_index = chunk_index[-1]\n",
    "\n",
    "    return document_chunks, chunk_index\n",
    "\n",
    "def search_top_k(db_embedding: Array, query: List[str], top_k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given queries, find top-k document chunk matches by cosine similarity. Exact NN search.\n",
    "    \"\"\"\n",
    "    query_embedding = model_id.encode(query_prefix + query)\n",
    "    similarities = cos_sim(db_embedding, query_embedding).numpy()\n",
    "    top_k_indices = np.argsort(similarities, axis=0, )[-top_k:][::-1]\n",
    "    \n",
    "    return top_k_indices, similarities[top_k_indices]\n",
    "\n",
    "def aggregate_and_sort(documents: Array, scores: Array) -> Tuple[Array, Array]:\n",
    "    \"\"\" Aggregate chunk similarity scores by source document and sort the documents by new scores.\"\"\"\n",
    "\n",
    "    unique_docs, inverse_indices = np.unique(documents, return_inverse=True)    \n",
    "    aggregated_scores = np.bincount(inverse_indices, weights=scores)\n",
    "    sorted_indices = np.argsort(-aggregated_scores)\n",
    "    \n",
    "    sorted_documents = unique_docs[sorted_indices]\n",
    "    sorted_scores = aggregated_scores[sorted_indices]\n",
    "    return sorted_documents, sorted_scores\n",
    "\n",
    "def search(db_embedding: Array, queries: List[str], top_k: int, n_chunks: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Search the embedding database for the most relevant documents to the query. \n",
    "    \"\"\"\n",
    "    \n",
    "    top_k_chunks, scores = search_top_k(db_embedding, queries, top_k)\n",
    "    # re-rank the top-k document chunks\n",
    "\n",
    "    flat_indices = top_k_chunks.flatten()\n",
    "    flat_scores = scores.flatten()\n",
    "    top_k_documents = np.searchsorted(n_chunks, flat_indices, side='right')\n",
    "    top_k_documents = aggregate_and_sort(top_k_documents, flat_scores)\n",
    "\n",
    "    return top_k_documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "\n",
    "# 1. Specify preffered dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. load model\n",
    "model_id = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n",
    "\n",
    "query_prefix = 'Represent this sentence for searching relevant passages: '\n",
    "\n",
    "# load data and initialize embedded db\n",
    "documents = load('data')\n",
    "new_chunks, n_chunks = chunk(documents, 512) \n",
    "index = model_id.encode(new_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is everything we've got so far. i've cleaned up the search code, it now aggregates similarity scores pertaining to the same\n",
    "source document. we also have easy access to the chunk matches if we need them down the line. it's using brute-force search atm, we should consider\n",
    "swapping this to something more effective.\n",
    "\n",
    "we're still missing a [re-ranking algorithm](https://huggingface.co/BAAI/bge-reranker-base). i'm yet unsure if this is actually necessary given the small embedding space we're working with. i'll leave this as an ablation\n",
    "for much later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def faiss(index, db_embedding: Array, query: str, top_k: int) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Given a query, find top-k document chunk matches by cosine similarity. Exact NN search.\n",
    "    \"\"\"\n",
    "    query = query_prefix + query\n",
    "    query_embedding = model_id.encode(query)\n",
    "\n",
    "    # Perform ANN search\n",
    "    _, I = index.search(query_embedding[np.newaxis, :], top_k)\n",
    "    I = I.flatten() # remove batch dimension\n",
    "    top_k_vectors = db_embedding[I]\n",
    "    similarities = cos_sim(query_embedding, top_k_vectors).numpy().flatten()\n",
    "\n",
    "    return I, similarities\n",
    "\n",
    "def search_faiss(index, db_embedding: Array, query: str, top_k: int, n_chunks: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Search the embedding database for the most relevant documents to the query. \n",
    "    \"\"\"\n",
    "    \n",
    "    top_k_indices, scores = faiss(index, db_embedding, query, top_k)\n",
    "    # re-rank the top-k document chunks\n",
    "    top_k_documents = np.searchsorted(n_chunks, top_k_indices, side='right')\n",
    "    top_k_documents = aggregate_and_sort(top_k_documents, scores)\n",
    "\n",
    "    return top_k_documents\n",
    "\n",
    "index = faiss.IndexFlatL2(index.shape[1])\n",
    "index.add(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented a FAISS version of the search and benchmarked it against our previous solution. Note the difference in `faiss()` vs `search_top_k()`; faiss uses a flat index and performs a optimized KNN search in the embedding space, while search_top_k calculates the cosine similarity for every vector in the embedding database and then sorts it using quicksort (`np.argsort`). Benchmarking the two against each other:\n",
    "\n",
    "`%timeit search(db_embedding, \"positional out-of-distribution\", 50, n_chunks)`\n",
    "\n",
    "50.86 ms ± 318 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "\n",
    "`%timeit search_faiss(index, db_embedding, \"positional out-of-distribution\", 50, n_chunks)`\n",
    "\n",
    "93.2 ms ± 6.72 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "naive brute force is quicker. remember that these libraries are implemented to **scale**. Right now, we don't have scale. but we can come back to this comparison later down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "let's take stock of what we've got so far. We can input a bunch of source urls, download the content, create a vector storage from chunking these documents, query this chunk space (top_k) and ultimately return the source document index of the best matching chunk.\n",
    "\n",
    "Note that there are a couple of key data holders in this setup:\n",
    "\n",
    "1. The embedding database `db_embedding`\n",
    "2. The chunk -> document index `n_chunks`\n",
    "3. The original documents `documents`\n",
    "4. The chunks `chunks`\n",
    "\n",
    "and because we need someone to own this data now is a good time to move our implementation to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import tempfile\n",
    "from typing import Union, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from vector_store import VectorStorage, SimilarityMetric\n",
    "\n",
    "Array = np.ndarray\n",
    "\n",
    "class Mindex:\n",
    "    def __init__(self, name: str, model_id: str = \"mixedbread-ai/mxbai-embed-large-v1\", EMBEDDING_DIM: int = 512, CHUNK_SIZE: int = 200, QUERY_PREFIX = '') -> None:\n",
    "        self.NAME = name\n",
    "        self.CHUNK_SIZE = CHUNK_SIZE\n",
    "        self.EMBEDDING_DIM = EMBEDDING_DIM\n",
    "        self.model_id = model_id\n",
    "\n",
    "        self.storage = VectorStorage(\n",
    "            embedder=SentenceTransformer(model_id, truncate_dim=EMBEDDING_DIM),\n",
    "            similarity=SimilarityMetric.COSINE,\n",
    "            query_prefix=QUERY_PREFIX,\n",
    "            save_embedder=False\n",
    "        )\n",
    "\n",
    "\n",
    "        self.documents: List[Tuple[str, str]] = []\n",
    "        self.chunks: List[str] = []\n",
    "        self.chunk_index: List[int] = [0]\n",
    "\n",
    "    def add(self, urls: Union[Tuple[str, ...], List[str]] = [], filename: str = None):\n",
    "        \"\"\"Add document(s) to Mindex.\n",
    "\n",
    "        Args:\n",
    "            urls (Union[Tuple[str, ...], List[str]]): List of URLs to add.\n",
    "            filename (str, optional): Path to a file containing URLs to add.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        assert isinstance(urls, (tuple, list))\n",
    "        assert urls != [] or filename is not None\n",
    "        \n",
    "        if filename:\n",
    "            with open(filename, 'r') as f:\n",
    "                urls.extend([line.strip() for line in f.readlines()])\n",
    "\n",
    "        new_chunks = []\n",
    "        for url in urls:\n",
    "            if url in [doc[1] for doc in self.documents]:\n",
    "                print(f\"Skipped {url} as it already exists in the index.\")\n",
    "                continue\n",
    "\n",
    "            title, text = self._download(url)\n",
    "            self.documents.append((title, url))\n",
    "            chunks, n_chunks = self._chunk(text)\n",
    "            new_chunks.extend(chunks)\n",
    "            self.chunk_index.append(self.chunk_index[-1] + n_chunks)\n",
    "\n",
    "        assert new_chunks != []\n",
    "        \n",
    "        self.storage.index(new_chunks)\n",
    "        self.chunks.extend(new_chunks)\n",
    "\n",
    "        self.save(f\"{self.NAME}.pkl\")\n",
    "\n",
    "    \n",
    "    def save(self, filename: str):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename: str):\n",
    "        with open(filename, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "            \n",
    "            # reload embedding model\n",
    "            obj.storage.set_embedder(\n",
    "                SentenceTransformer(\n",
    "                    obj.model_id, \n",
    "                    truncate_dim=obj.EMBEDDING_DIM\n",
    "                    )\n",
    "                )\n",
    "            return obj\n",
    "\n",
    "    def search(self, query: str, top_k: int) -> Tuple[Array, Array, Array]:\n",
    "        \"\"\"\n",
    "        Search the embedding database for the most relevant documents to the query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query.\n",
    "            top_k (int): The number of top results to return.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray]: Top documents, doc scores, top chunks, chunk scores.\n",
    "        \"\"\"\n",
    "        assert top_k > 0 and top_k <= len(self.chunks)\n",
    "        \n",
    "        top_k_chunks, chunk_scores = self.storage.search_top_k([query], top_k)\n",
    "        top_k_chunks = top_k_chunks.squeeze()\n",
    "        chunk_scores = chunk_scores.squeeze()\n",
    "\n",
    "        # connect chunks to documents\n",
    "        top_k_documents = np.searchsorted(self.chunk_index, top_k_chunks, side='right') \n",
    "        top_m_documents, document_scores = self._aggregate_and_sort(top_k_documents, chunk_scores) # m <= k\n",
    "\n",
    "        return top_m_documents.squeeze(), document_scores.squeeze(), top_k_chunks, chunk_scores\n",
    "\n",
    "\n",
    "    def _aggregate_and_sort(self, documents: Array, scores: Array) -> Tuple[Array, Array]:\n",
    "        \"\"\" Aggregate chunk similarity scores by source document and sort the documents by new scores.\"\"\"\n",
    "\n",
    "        unique_docs, inverse_indices = np.unique(documents, return_inverse=True)    \n",
    "        aggregated_scores = np.bincount(inverse_indices, weights=scores)\n",
    "        sorted_indices = np.argsort(-aggregated_scores)\n",
    "        \n",
    "        sorted_documents = unique_docs[sorted_indices]\n",
    "        sorted_scores = aggregated_scores[sorted_indices]\n",
    "        return sorted_documents, sorted_scores\n",
    "    \n",
    "\n",
    "    def _chunk(self, text: str) -> Tuple[List[str], int]:\n",
    "        \"\"\"Split documents into 50% overlapping segments.\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = [' '.join(words[i:i+self.CHUNK_SIZE]) for i in range(0, len(words), self.CHUNK_SIZE // 2)]\n",
    "        return chunks, len(chunks)\n",
    "\n",
    "    \n",
    "    def _download(self, url: str) -> Tuple[str, str]:\n",
    "        \"\"\"Download content from the given URL, determine its type, and extract the title and text.\"\"\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() \n",
    "\n",
    "        content_type = response.headers['Content-Type']\n",
    "\n",
    "        if 'text/html' in content_type:\n",
    "            return self._parse_html(response.content)\n",
    "        elif 'application/pdf' in content_type:\n",
    "            return self._parse_pdf(response.content)\n",
    "        else:\n",
    "            raise Exception('Unsupported content type')\n",
    "\n",
    "    def _parse_html(self, content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"Extract the title and text content from HTML data.\"\"\"\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        title = soup.find('title').text if soup.find('title') else 'No Title'\n",
    "        text = self._clean_text(soup.get_text())\n",
    "        return title, text\n",
    "\n",
    "    def _parse_pdf(self, content: bytes) -> Tuple[str, str]:\n",
    "        \"\"\"Extract the title and text content from a PDF file.\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as temp_file:\n",
    "            temp_file.write(content)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        doc = fitz.open(temp_file_path)\n",
    "        text = ' '.join([page.get_text() for page in doc])\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "        # Extract title from metadata or the first block of text\n",
    "        title = doc.metadata.get('title', self._extract_first_block_text(doc[0]))\n",
    "\n",
    "        os.remove(temp_file_path)\n",
    "        return title, text\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace('- ', '')\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "\n",
    "    def _extract_first_block_text(self, page) -> str:\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        return blocks[0][4].strip() if blocks else 'unknown'    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://arxiv.org/pdf/2401.01325\",\n",
    "    \"https://arxiv.org/pdf/2310.02207\",\n",
    "    \"https://huggingface.co/blog/moe#:~:text=Mixture%20of%20Experts%20enable%20models,budget%20as%20a%20dense%20model.\"\n",
    "]\n",
    "\n",
    "mindex = Mindex('test')\n",
    "mindex.add(urls)\n",
    "\n",
    "top_docs, scores_doc, top_chunks, chunk_scores = mindex.search(\"positional out-of-distribution\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that i've refactored the vector embedding database into it's own class. it encapsulates everything related to the embedding space, allowing Mindex to interact purely through textual via `index` and `search_top_k` functions. Mindex carries the logic for chunk <-> document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "# Profile and save to a file\n",
    "cProfile.run('mindex.add(urls)', 'profile_output.prof')\n",
    "\n",
    "# Load and print stats\n",
    "with open('profile_stats.txt', 'w') as f:\n",
    "    p = pstats.Stats('profile_output.prof', stream=f)\n",
    "    p.sort_stats('cumulative').print_stats()\n",
    "\n",
    "# To read the stats file\n",
    "with open('profile_stats.txt', 'r') as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cool. we've got much of the basics in place. we can create a Mindex; add documents through urls; query; get top matching documents, chunks and their source url. \n",
    "\n",
    "the core of Mindex is of course `search`, and being most important, we'd like to improve its performance, right? we could just vibe it out - implement things we *think* will better retrieval accuracy and then empirically evaluate the improvement. but it would be fun if we could be more quantitative about it. i'm thinking we decide on some metric(s), develop a synthetic benchmark dataset, evaluate our baseline and try to improve from there. one metric for retrieval accuracy and one for retrieval speed should be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Where to look for improvements --\n",
    "\n",
    "- Chunking strategy\n",
    "    - We now split into 50% overlapping chunks of size 200 words.\n",
    "    - Shift Overlap\n",
    "    - Split by paragraph or section\n",
    "\n",
    "- Embedding strategy\n",
    "    - Simply embedding entire chunks + query might not be sufficient, especially when our query is short, or very specific in a term we are looking for. May struggle when term matching is crucial.\n",
    "    - Ensemble semantic search with keyword search. Semantic + BM25?\n",
    "\n",
    "- Embedding model\n",
    "    - mixedbread-ai/mxbai-embed-large-v1\n",
    "\n",
    "- Chunk -> Document\n",
    "    - We aggregate all the chunks scores per document into a document score.\n",
    "    - Are there other ways?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let's build an evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from nltk import ngrams\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, file_path: str):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.dataset_name = data.get('dataset', '')\n",
    "        self.validation_set = data.get('validation', [])\n",
    "        self.test_set = data.get('test', [])\n",
    "\n",
    "\n",
    "    def get_validation_set(self) -> List[str]:\n",
    "        return [(item['query'], item['answer']['text']) for item in self.validation_set]\n",
    "\n",
    "\n",
    "    def get_validation_queries(self) -> List[str]:\n",
    "        return [item['query'] for item in self.validation_set]\n",
    "\n",
    "\n",
    "    def get_test_queries(self) -> List[str]:\n",
    "        return [item['query'] for item in self.test_set]\n",
    "\n",
    "\n",
    "    def get_validation_item_by_id(self, item_id: str) -> Dict[str, Any]:\n",
    "        return next((item for item in self.validation_set if item['id'] == item_id), None)\n",
    "\n",
    "\n",
    "    def get_test_item_by_id(self, item_id: str) -> Dict[str, Any]:\n",
    "        return next((item for item in self.test_set if item['id'] == item_id), None)\n",
    "\n",
    "\n",
    "    def get_answer_for_query(self, query: str, dataset: str = 'validation') -> Dict[str, Any]:\n",
    "        target_set = self.validation_set if dataset == 'validation' else self.test_set\n",
    "        item = next((item for item in target_set if item['query'] == query), None)\n",
    "        return item['answer'] if item else None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ngram_overlap_score(retrieved_chunk: str, true_answer: str, n: int = 5) -> float:\n",
    "        def get_ngrams(t: str, n: int):\n",
    "            return set(ngrams(t.lower().split(), n))\n",
    "\n",
    "        retrieved_ngrams = get_ngrams(retrieved_chunk, n)\n",
    "        answer_ngrams = get_ngrams(true_answer, n)\n",
    "        \n",
    "        overlap = len(retrieved_ngrams.intersection(answer_ngrams))\n",
    "        return overlap / len(answer_ngrams)\n",
    "\n",
    "    def evaluate_retrieval(self, answer: str, retrieved_chunks: List[str], threshold: float = 0.7) -> Dict[str, Any]:\n",
    "        scores = [self.calculate_ngram_overlap_score(chunk, answer) for chunk in retrieved_chunks]\n",
    "        \n",
    "        max_score = max(scores)\n",
    "        best_chunk_index = scores.index(max_score)\n",
    "        is_answer_found = max_score >= threshold\n",
    "        \n",
    "        return {\n",
    "            'scores': scores,\n",
    "            'max_score': max_score,\n",
    "            'best_chunk_index': best_chunk_index,\n",
    "            'is_answer_found': is_answer_found\n",
    "        }\n",
    "\n",
    "dataset = Evaluator(\"../data/eval.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mindex = Mindex('selfextend_eval')\n",
    "\n",
    "# mindex.add(urls=[\"https://arxiv.org/pdf/2401.01325\"])\n",
    "mindex = Mindex.load('../selfextend_eval.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 0\n",
    "num_correct = 0\n",
    "sum_overlap = 0.0\n",
    "for query, answer in dataset.get_validation_set():\n",
    "    #print(f\"Query: {query}\")\n",
    "    #print(f\"Answer: {answer}\")\n",
    "    _, _, chunk_idxs, _ = mindex.search(query, top_k=5)\n",
    "    chunks = [mindex.chunks[i] for i in chunk_idxs]\n",
    "    result = dataset.evaluate_retrieval(answer, chunks)\n",
    "\n",
    "    num_samples += 1\n",
    "    num_correct += int(result['is_answer_found'])\n",
    "    sum_overlap += result['max_score']\n",
    "    #print(result['is_answer_found'], result['scores'], chunks[result['best_chunk_index']])\n",
    "    #print(\"------\")\n",
    "    #print(\"\")\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples:.4f} %\")\n",
    "print(f\"Mean n-gram overlap: {sum_overlap / num_samples:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the evaluation module. Initialized with an evaluation dataset `data/eval.json`, it retrieves validation/test samples and assesses Mindex search results. Two key aspects are noteworthy: (1) the method for determining if a retrieved chunk matches the answer, and (2) the metrics used to evaluate the entire top-k retrieval.\n",
    "\n",
    "(1) Initially, exact string matching yielded poor results, with no retrieved chunks matching the expected answers. Debugging revealed slight formatting differences between chunks and answers, causing the evaluation system to incorrectly mark correct retrievals as failures. This discrepancy arose because the evaluation dataset was synthetically generated using Claude, rather than downloaded and parsed like the documents added to Mindex. To address this issue, we now use n-gram overlap (n=3) with a threshold of 0.7 to determine if a retrieved chunk contains the answer.\n",
    "\n",
    "\n",
    "(2) We employ two metrics for evaluation:\n",
    "\n",
    "a. Top-k accuracy: If any of the retrieved chunks contains the answer (as defined by the method in (1) with a 0.7 threshold), we mark it as an accurate retrieval with a score of 1; otherwise, it's 0. This metric provides a clear percentage of successful retrievals but depends on a threshold and lacks granularity.\n",
    "\n",
    "b. Mean n-gram overlap score: We calculate the average of the best chunk's n-gram overlap score per retrieval. This metric addresses the limitations of the top-k accuracy metric by providing more nuanced results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got all of the code needed to iteratively refine Mindex and actually measure improvement quantitatively. I've moved Mindex and the Evaluator into their own files (I'm getting tired of scrolling through this notebook), and now I just need to curate the evaluation dataset.\n",
    "\n",
    "... ... ...\n",
    "\n",
    "couple of hours later, I think we've got our dataset (sonnet 3.5 is goated)! \n",
    "\n",
    "you can see the final version of the prompt i used to generate the dataset in `data/claude_prompt.txt`. it took me some time to get things working properly. I wanted to ensure that queries were general. Take \"What's the benefit of using structured input and output with LLMs?\" as an example. There should be no reference to a specific paper, author, or method. This type of query is representative of how I want to interact with Mindex, making it a good evaluation sample. Unfortunately, Sonnet was adamant about not following such queries, often referencing \"this paper\" or asking about specific methods like \"in SelfExtend how...\". I allowed some of these queries to remain, but did my best to steer Sonnet away from them whenever possible. In the end we end up with 400+ samples, split into validation & test sets. \n",
    "\n",
    "with both the benchmark dataset and the code done, we're finally ready to benchmark the baseline and improve from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new mindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindex import Mindex\n",
    "from evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(\"../data/eval.json\")\n",
    "urls = evaluator.get_document_urls()\n",
    "\n",
    "mindex = Mindex('eval')\n",
    "mindex.add(urls=urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load mindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindex import Mindex\n",
    "from evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(\"../data/eval.json\")\n",
    "mindex = Mindex.load('eval.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLI\n",
    "\n",
    "Short side quests, I need a command line interface. This is far from my favorite part of this project, but I realize that you have to be able to use Mindex somehow, and a CLI seems like the most straight-forward way to do so. I was picturing something like this:\n",
    "\n",
    "```\n",
    "$ mindex create [name]\n",
    "$ mindex search [query]\n",
    "$ mindex add [url]\n",
    "$ mindex remove [url]\n",
    "```\n",
    "\n",
    "so I got to work and created `cli.py`. I haven't created a CLI for a long time, and it's actually my first time working with [click](https://click.palletsprojects.com/en/8.1.x/) (i was using [typer](https://typer.tiangolo.com/) first but then I read that click is more lighweight) but the process was pretty seamless. Took me some time to set up the project structure correctly but we got there. Thing is, the shit is really slow... i mean look at this\n",
    "\n",
    "![](assets/search_timings.png)\n",
    "\n",
    "it takes 13 seconds, the culprit being the embedding model initialization. (not sure if i mentioned this) i'm not pickling the SentenceTransformer embedding model as part of the serialization (at least by default) because it took about a 1GB of space which felt like a lot. so instead the mindex load function initializes the embedding model from the model name\n",
    "\n",
    "````\n",
    "@classmethod\n",
    "    def load2(cls, filename: str):\n",
    "        with open(filename, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "            \n",
    "            # reload embedding model\n",
    "            obj.storage.set_embedder(\n",
    "                SentenceTransformer(\n",
    "                    obj.model_id, \n",
    "                    truncate_dim=obj.EMBEDDING_DIM\n",
    "                    )\n",
    "                )\n",
    "            return obj\n",
    "````\n",
    "\n",
    "which apparently is fucking slow. i hadn't considered at the time that I would have to load on every search. if we do actually pickle the model things get a bit more reasonable\n",
    "\n",
    "![](assets/search_timings2.png)\n",
    "\n",
    "unfortunately increasing search time, in CLI world, is quite difficult. there are massive discrepancies in search times between run 1 and subsequent runs\n",
    "\n",
    "````\n",
    "Time taken for search 1: 0.7984 seconds\n",
    "Time taken for search 2: 0.0358 seconds\n",
    "Time taken for search 3: 0.0339 seconds\n",
    "Time taken for search 4: 0.0343 seconds\n",
    "Time taken for search 5: 0.0346 seconds\n",
    "````\n",
    "\n",
    "which can't be solved because every cli call is a new process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = False\n",
    "\n",
    "num_samples = 0\n",
    "num_correct = 0\n",
    "sum_overlap = 0.0\n",
    "search_times = []\n",
    "eval_times = []\n",
    "\n",
    "eval_set = evaluator.get_validation_set() \n",
    "\n",
    "for query, answer in tqdm(eval_set, desc=\"Processing\"):\n",
    "    search_start = time.time()\n",
    "    _, _, chunk_idxs, _ = mindex.search(query, top_k=5)\n",
    "    search_end = time.time()\n",
    "    search_times.append(search_end - search_start)\n",
    "\n",
    "    chunks = [mindex.chunks[i] for i in chunk_idxs]\n",
    "\n",
    "    eval_start = time.time()\n",
    "    result = evaluator.evaluate_retrieval(answer, chunks)\n",
    "    eval_end = time.time()\n",
    "    eval_times.append(eval_end - eval_start)\n",
    "\n",
    "    num_samples += 1\n",
    "    num_correct += int(result['is_answer_found'])\n",
    "    sum_overlap += result['max_score']\n",
    "\n",
    "    if log:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(result['is_answer_found'], result['max_score'], chunks[result['best_chunk_index']])\n",
    "        print(\"------\")\n",
    "        print(\"\")\n",
    "\n",
    "print(f\"Accuracy: {num_correct / num_samples:.4f}\")\n",
    "print(f\"Mean n-gram overlap: {sum_overlap / num_samples:.4f}\")\n",
    "print(f\"Mean search time: {sum(search_times) / len(search_times):.4f} seconds\")\n",
    "print(f\"Mean evaluation time: {sum(eval_times) / len(eval_times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make this thing better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Where to look for improvements --\n",
    "\n",
    "- Chunking strategy\n",
    "    - We now split into 50% overlapping chunks of size 200 words.\n",
    "    - Shift Overlap\n",
    "    - Split by paragraph or section\n",
    "\n",
    "- Embedding strategy\n",
    "    - Simply embedding entire chunks + query might not be sufficient, especially when our query is short, or very specific in a term we are looking for. May struggle when term matching is crucial.\n",
    "    - Ensemble semantic search with keyword search. Semantic + BM25?\n",
    "\n",
    "- Embedding model\n",
    "    - mixedbread-ai/mxbai-embed-large-v1\n",
    "\n",
    "- Chunk -> Document\n",
    "    - We aggregate all the chunks scores per document into a document score.\n",
    "    - Are there other ways?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunk & Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is where I think we can find our first improvement. To remind the reader, `_chunk()` currently splits documents into 50% overlapping segments containing 200 words.\n",
    "\n",
    "Because Mindex is meant to support a wide array of sources, defining a universal cleaning (or even chunking for that matter) strategy is difficult. Even if we focus on just arXiv papers, PDFs are notoriously unstandardized. I don't want to pour an immense amount of time into this, so I'd love for something simple, like splitting chunks on semantic boundaries (think paragraphs). Now you'd think this is as simple as finding a \\n\\n, but no. Not for pdfs :). I'm going to see if the pdf parser library i'm using (pymupdf) can help me out, otherwise we might have to call this improvement attempt a failure.\n",
    "\n",
    "... (some time later) yeah, unfortunately there is no *simple* way to reliably extract sections or paragraphs. I'm keen to look into Vik Paruchuri's work @ https://github.com/VikParuchuri, I know he's created some amazing PDF analyzers, but I don't want to sink time into this right now, so we'll leave this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindex import Mindex\n",
    "\n",
    "mindex = Mindex('test')\n",
    "mindex.add(urls=[\"https://arxiv.org/pdf/2401.01325\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonericsson/miniconda3/envs/mindex/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mindex import Mindex\n",
    "\n",
    "mindex = Mindex.load('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindex import Mindex\n",
    "\n",
    "mindex = Mindex.load('test.pkl')\n",
    "\n",
    "#mindex.add(urls=[\"https://arxiv.org/pdf/2401.01325\"])\n",
    "#r = mindex.search(\"positional out-of-distribution\", top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but there's still some things I want to experiment with, chunk_size and chunk_overlap. A simple grid search over the two should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from mindex import Mindex\n",
    "from mindex.evaluator import Evaluator\n",
    "\n",
    "chunk_sizes = np.array([500, 600])\n",
    "chunk_overlaps = np.array([0.4, 0.5, 0.6])\n",
    "\n",
    "evaluator = Evaluator(\"data/eval.json\")\n",
    "urls = evaluator.get_document_urls()\n",
    "\n",
    "chunk_size_grid, overlap_ratio_grid = np.meshgrid(chunk_sizes, chunk_overlaps)\n",
    "chunk_overlap_grid = (chunk_size_grid * overlap_ratio_grid).astype(int)\n",
    "\n",
    "chunk_sizes_flat = chunk_size_grid.flatten()\n",
    "chunk_overlaps_flat = chunk_overlap_grid.flatten()\n",
    "\n",
    "results = np.zeros((len(chunk_sizes_flat), 3))\n",
    "\n",
    "# Perform grid search\n",
    "for i, (chunk_size, chunk_overlap) in enumerate(zip(chunk_sizes_flat, chunk_overlaps_flat)):\n",
    "    print(f\"Evaluating: CHUNK_SIZE={chunk_size}, CHUNK_OVERLAP={chunk_overlap}\")\n",
    "    \n",
    "    mindex = Mindex('eval', CHUNK_SIZE=int(chunk_size), CHUNK_OVERLAP=int(chunk_overlap))\n",
    "    mindex.add(urls=urls)\n",
    "    \n",
    "    score = evaluator.evaluate_mindex(mindex, validation_set=True)\n",
    "    results[i] = [chunk_size, chunk_overlap, score]\n",
    "    \n",
    "    os.remove('eval.pkl')\n",
    "    del mindex\n",
    "\n",
    "sorted_indices = np.argsort(results[:, 2])[::-1]\n",
    "sorted_results = results[sorted_indices]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(\"CHUNK_SIZE | CHUNK_OVERLAP | Score\")\n",
    "print(\"-\" * 35)\n",
    "for chunk_size, chunk_overlap, score in sorted_results:\n",
    "    print(f\"{chunk_size:^10.0f} | {chunk_overlap:^13.0f} | {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first experiment was run with \n",
    "\n",
    "```\n",
    "chunk_sizes = np.array([100, 200, 400, 800])\n",
    "chunk_overlaps = np.array([0.2, 0.4, 0.6, 0.8])\n",
    "```\n",
    "\n",
    "and you can find the results in `data/benchmark_results.md`. larger chunk sizes generally perform better, with a sweet spot around 400 to 800 words. 40% to 60% overlap percentage typically yields the best results. The standout performer is the 800-word chunk size with 40% overlap, hitting nearly 64% accuracy. However, because our evaluation is determined by whether a retrieved chunk contains the answer *somewhere* in the chunk, there is a bias towards systems with larger chunk size. I'm going to run a focus grid search in the 400 - 800, 40-60% span.\n",
    "\n",
    "\n",
    "```\n",
    "chunk_sizes = np.array([500, 600])\n",
    "chunk_overlaps = np.array([0.4, 0.5, 0.6])\n",
    "```\n",
    "\n",
    "again we see that larger chunk sizes and larger overlap improves performance. I'm thinking 600 words with a 360 word overlap is a good tradeoff. Thats a 16 point improvement over the baseline(!?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindex import Mindex\n",
    "from mindex.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(\"data/eval.json\")\n",
    "urls = evaluator.get_document_urls()\n",
    "\n",
    "mindex = Mindex('eval', CHUNK_SIZE=600, CHUNK_OVERLAP=360)\n",
    "mindex.add(urls=urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load mindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindex import Mindex\n",
    "from mindex.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(\"data/eval.json\")\n",
    "mindex = Mindex.load('eval.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, text = mindex._download(\"https://huggingface.co/blog/mlabonne/abliteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final notes (for future self):\n",
    "\n",
    "- PDF Analyzers [surya](https://github.com/VikParuchuri/surya) could be very interesting. See more from [Vik](https://github.com/VikParuchuri?tab=repositories)\n",
    "- Parser + chunker for TeX. TeX is a lot more structured than PDFs, and all arXiv papers have a TeX source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://x.com/JinaAI_/status/1823756993108304135"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
