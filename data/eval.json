{
  "dataset": "Mindex Evaluation",
  "validation": [
    {
      "id": "q001",
      "query": "What is the main goal of SelfExtend?",
      "answer": "We propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q002",
      "query": "How does SelfExtend address the positional O.O.D. issue?",
      "answer": "SelfExtend addresses the issue of O.O.D. positional information by using a simple floor division operation to map unseen large relative positions to those encountered during pretraining.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q005",
      "query": "What are the two types of attention mechanisms incorporated in SelfExtend?",
      "answer": "SelfExtend incorporates two distinct types of attention mechanisms: 1) Grouped attention, specifically designed for tokens that are far apart. This approach applies a floor operation to the positions to manage long-distance relationships between tokens; 2) Standard attention, which employs the conventional attention mechanism for adjacent tokens within a specified range.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q007",
      "query": "What are the two trade-offs observed in SelfExtend's performance?",
      "answer": "1) There is a trade-off with respect to group size in SelfExtend. Generally, both too small and too large group sizes can result in inferior performance compared to an optimal level. With a large group size, position information becomes more coarse, potentially causing performance drops. Conversely, small group sizes require SelfExtend to utilize larger position embeddings to extend the context window. These larger position embeddings are less trained compared to smaller ones.\n\n2) There is also another trade-off w.r.t. neighbor window size. With larger neighbor window sizes, there is more precise information about neighbor tokens, which is the most important. But a larger neighbor window size means SelfExtend has to use a larger group size for a long sequence, compared to using a smaller neighbor window size & smaller group size, the information about the whole sequence becomes coarse.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q008",
      "query": "How does SelfExtend perform on the passkey retrieval task with varying passkey lengths?",
      "answer": "Notably, with the exception of Yarn, many tuning-based methods are unable to accurately reproduce passkeys beyond 64 digits, and some of them even experience a marked decline in performance when the passkey length exceeds 16 digits. Remarkably, although without tuning, SelfExtend maintains its superiority.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q009",
      "query": "What is the empirical rule for selecting hyperparameters in SelfExtend?",
      "answer": "We conclude those results as an empirical rule. Denoting the pretraining context window as L, the target extension length as N , the neighbor window as W , and the group size as G, the empirical rule for selecting hyperparameters is to ensure that the following inequality holds:\n1\n2 \n\u00d7 L > W + \nN \u2212 W\nG",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q010",
      "query": "What are the main limitations of SelfExtend?",
      "answer": "Limitations: SelfExtend increases computation cost with naive implementations, since it performs extra attention across all query-key pairs. However, with optimizations like blocked kernels (e.g. Flash Attention (Dao et al., 2022)), this becomes linear rather than quadratic, and the marginal cost is small enough to be ignored for long input sequences. Also, the performance degrades with large group size, preventing indefinitely long contexts. Besides, SelfExtend still processes the entire sequence to ensure information integrity, while some methods such as prompt compression (Chuang et al., 2024; Jiang et al., 2023b) can shorten the input to reduce computation. Additionally, evaluation methodologies for assessing long context abilities remain open research questions. Standard practices have yet to emerge, complicating experimental results.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q011",
      "query": "Why might fine-tuning large language models on long texts be problematic?",
      "answer": "Additionally, high-quality long text data is scarce, hindering such fine-tuning approaches. Most real-world data is short, and much long text lacks meaningful long-range dependencies. With limited appropriate data, finetuning risks degrading existing strong performance on shorter sequences from pretraining or overfitting models to the tuning set. LLMs' generalizability to broad tasks may be reduced.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q012",
      "query": "How does the importance of precise word positions change in long texts?",
      "answer": "The core idea hinges on the observation that, in long texts, exacting word positions becomes less crucial. The overall meaning and the relative order of information hold greater significance. Just like when answering questions about lengthy texts, we rely on the general location and order, not the specific word-by-word placement.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q014",
      "query": "How does the training of language models on short texts relate to human learning?",
      "answer": "Our belief stems from the fact that when we, as human beings, are children, we are taught how to read and write using relatively short texts, such as articles spanning several pages. We rarely use extremely long texts like entire books or complete documents as learning materials. Yet, we are still able to understand long texts effectively.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q015",
      "query": "What is the relationship between perplexity and a model's ability to handle long contexts?",
      "answer": "PPL is not an effective metric for measuring the ability of LLMs to handle long contexts. In Figure 7, we introduce a seeming plausible context window extension method named 'Infinite'. When evaluated on PG19 using the same protocol, Llama-2-7b-chat with 'Infinite' achieves PPL scores that are comparable to, or even lower than, those achieved by SelfExtend, as demonstrated in Table 6. However, 'Infinite' essentially mimics the process of dividing a long sequence into short sub-sequences before processing them with LLMs, indicating that it does not genuinely address long context handling.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q017",
      "query": "What are the fundamental capabilities tested by the passkey retrieval task?",
      "answer": "Although this task is easy and far from real-world scenarios, it tests two fundamental capabilities of LLMs: 1. The model should be able to recognize and locate the useful information across all positions of the input sequence (the most fundamental understanding capability); 2. The model should be able to use the perceived information to finish tasks (the most fundamental generation capability).",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q019",
      "query": "What is the significance of neighboring tokens in language modeling?",
      "answer": "Natural language exhibits a characteristic where meaning stays relatively consistent within short ranges like paragraphs. Therefore, using close or even identical position encodings effectively captures the necessary relative ordering of important information.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q020",
      "query": "How does the computation cost of attention mechanisms typically scale with sequence length?",
      "answer": "SelfExtend increases computation cost with naive implementations, since it performs extra attention across all query-key pairs. However, with optimizations like blocked kernels (e.g. Flash Attention (Dao et al., 2022)), this becomes linear rather than quadratic, and the marginal cost is small enough to be ignored for long input sequences.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q021",
      "query": "Why is perplexity (PPL) not an effective metric for measuring LLMs' ability to handle long contexts?",
      "answer": "The discrepancy between Perplexity (PPL) and long context ability primarily stems from how PPL is calculated by averaging over numerous tokens. As long as the majority of tokens are modeled accurately, PPL will remain low. This is closely related to the influence of neighboring tokens. Information from neighboring tokens\u2014such as those within the local attention window of 'Infinite'\u2014can suffice for predicting most tokens, thus leading to a low PPL. However, a few critical tokens, which are crucial for understanding long contexts and answering questions, may not be predicted accurately.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q022",
      "query": "How do large language models represent space and time?",
      "answer": "We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q024",
      "query": "What evidence suggests that language models form coherent world representations?",
      "answer": "We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q025",
      "query": "How do researchers test if language models have learned spatial and temporal representations?",
      "answer": "Using the Llama-2 (Touvron et al., 2023) and Pythia Biderman et al. (2023) family of models, we train linear regression probes (Alain & Bengio, 2016; Belinkov, 2022) on the internal activations of the names of these places and events at each layer to predict their real-world location (i.e., latitude/longitude) or time (numeric timestamp).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q026",
      "query": "What is the relationship between model size and the quality of spatial and temporal representations in language models?",
      "answer": "These probing experiments reveal evidence that models build spatial and temporal representations throughout the early layers before plateauing at around the model halfway point with larger models consistently outperforming smaller ones (\u00a7 3.1).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q027",
      "query": "How do researchers verify that spatial and temporal representations in language models are actually used by the model?",
      "answer": "We also perform a series of neuron ablation and intervention experiments in Appendix B to verify the importance of these neurons in spatial and temporal modeling.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q028",
      "query": "What are some limitations of using English Wikipedia as a data source for studying spatial representations in language models?",
      "answer": "Several limitation are worth highlighting. First, our data only comes from English Wikipedia, and hence is skewed towards the Anglosphere. Additionally, the distribution of entity types is not uniform, e.g. we noticed the United Kingdom has many more railway stations than any other country, which could introduce unwanted correlations in the data that may affect the probes.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q029",
      "query": "How do researchers ensure that their probes are not just memorizing country or decade-level information?",
      "answer": "To better distinguish these cases, we analyze how the probes generalize when holding out specific blocks of data. In particular, we train a series of probes, where for each one, we hold out one country, state, borough, century, decade, or year for the world, USA, NYC, historical figure, entertainment, and headlines dataset respectively. We then evaluate the probes on the held out block of data.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q030",
      "query": "What evidence suggests that language models represent space and time in a unified manner across different entity types?",
      "answer": "We find that when we project the activation datasets on to the weights of the most similar neurons, these neurons are indeed highly sensitive to the true location of entities in space or time (see Figure 5). In other words, there exist individual neurons within the model that are themselves fairly predictive feature probes. Moreover, these neurons are sensitive to all of the entity types within our datasets, providing stronger evidence for the claim these representations are unified.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q031",
      "query": "How do researchers create datasets for testing spatial and temporal representations in language models?",
      "answer": "To enable our investigation, we construct six datasets of names of entities (people, places, events, etc.) with their respective location or occurrence in time, each at a different order of magnitude of scale. For each dataset, we included multiple types of entities, e.g., both populated places like cities and natural landmarks like lakes, to study how unified representations are across different object types. Furthermore, we maintain or enrich relevant metadata to enable analyzing the data with more detailed breakdowns, identify sources of train-test leakage, and support future work on factual recall within LLMs. We also attempt to deduplicate and filter out obscure or otherwise noisy data.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q034",
      "query": "How do researchers test the robustness of spatial and temporal representations to different prompts?",
      "answer": "To study this, we create new activation datasets where we prepend different prompts to each of the entity tokens, following a few basic themes. In all cases, we include an \"empty\" prompt containing nothing other than the entity tokens (and a beginning of sequence token). We then include a prompt which asks the model to recall the relevant fact, e.g., \"What is the latitude and longitude of <place>\" or \"What was the release date of <author>'s <book>.\" For the United States and NYC datasets we also include versions of these prompts asking where in the US or NYC this location is, in an attempt to disambiguate common names of places (e.g. City Hall). As a baseline we include a prompt of 10 random tokens (sampled for each entity). To determine if we can obfuscate the subject, for some datasets we fully capitalize the names of all entities. Lastly, for the headlines dataset, we try probing on both the last token and on a period token appended to the headline.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q036",
      "query": "What is the relationship between the performance of individual neurons and the overall model's representation of spatial and temporal information?",
      "answer": "If probes trained with explicit supervision are an approximate upper bound on the extent to which a model represents these spatial and temporal features, then the performance of individual neurons is a lower bound. In particular, we generally expect features to be distributed in superposition (Elhage et al., 2022b), making individual neurons the wrong level of analysis. Nevertheless, the existence of these individual neurons, which received no supervision other than from next-token prediction, is very strong evidence that the model has learned and makes use of spatial and temporal features.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q038",
      "query": "What is the relationship between the size of language models and their ability to form detailed spatial models of individual cities?",
      "answer": "The dataset with the worst performance is the New York City dataset. This was expected given the relative obscurity of most of the entities compared with other datasets. However, this is also the dataset where the largest model has the best relative performance, suggesting that sufficiently large LLMs could eventually form detailed spatial models of individual cities.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q039",
      "query": "How do researchers create datasets for testing temporal representations in language models?",
      "answer": "Our three temporal datasets consist of (1) the names and occupations of historical figures who died between 1000BC and 2000AD adapted from (Annamoradnejad & Annamoradnejad, 2022); (2) the titles and creators of songs, movies, and books from 1950 to 2020 constructed from DBpedia with the Wikipedia page views filtering technique; and (3) New York Times news headlines from 2010-2020 from news desks that write about current events, adapted from (Bandy, 2021).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q041",
      "query": "How do researchers test if language models have a unified representation of space and time across different entity types?",
      "answer": "Similar to the above, we distinguish these hypotheses by training a series of probes where the train-test split is performed to hold out all points of a particular entity class. Table 4 reports the proximity error for the entities in the default test split compared to when heldout, averaged over all such splits as before. The results suggest that the probes largely generalize across entity types, with the main exception of the entertainment dataset.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q043",
      "query": "How does BitNet b1.58 constrain the weights of the model?",
      "answer": "To constrain the weights to -1, 0, or +1, we adopt an absmean quantization function. It first scales the weight matrix by its average absolute value, and then round each value to the nearest integer among {-1, 0, +1}:",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q044",
      "query": "What are the main components adopted from LLaMA in the BitNet b1.58 architecture?",
      "answer": "Specifically, it uses RMSNorm, SwiGLU, rotary embedding, and removes all biases. In this way, BitNet b1.58 can be integrated into the popular open-source software (e.g., Huggingface, vLLM, and llama.cpp) with minimal efforts.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q045",
      "query": "At what model size does BitNet b1.58 start to match full precision LLaMA LLM in terms of perplexity?",
      "answer": "It shows that BitNet b1.58 starts to match full precision LLaMA LLM at 3B model size in terms of perplexity, while being 2.71 times faster and using 3.55 times less GPU memory.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q046",
      "query": "How does the speed-up of BitNet b1.58 change as the model size increases?",
      "answer": "Figure 2 illustrates the trends of latency and memory, showing that the speed-up increases as the model size scales. In particular, BitNet b1.58 70B is 4.1 times faster than the LLaMA LLM baseline.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q047",
      "query": "What is the main difference in energy consumption between BitNet b1.58 and LLaMA LLM?",
      "answer": "The majority of BitNet b1.58 is INT8 addition calculation, while LLaMA LLM consists of both FP16 addition and FP16 multiplication.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q048",
      "query": "How much more energy efficient is BitNet b1.58 compared to LLaMA LLM for matrix multiplication?",
      "answer": "According to the energy model in [Hor14, ZZL22], BitNet b1.58 saves 71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q050",
      "query": "How does BitNet b1.58 compare to StableLM-3B in terms of performance on various tasks?",
      "answer": "Our findings shows that BitNet b1.58 achieves a superior performance on all end tasks, indicating that 1.58-bit LLMs also have strong generalization capabilities.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q051",
      "query": "What potential advantage does BitNet b1.58 offer for Mixture-of-Experts (MoE) LLMs?",
      "answer": "Firstly, the reduced memory footprint reduces the number of devices required to deploy MoE models. Moreover, it significantly reduces the overhead of transferring activations across networks. Ultimately, there would be no overhead if the entire models could be placed on a single chip.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q052",
      "query": "How can BitNet b1.58 potentially improve handling of long sequences in LLMs?",
      "answer": "BitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the activations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources. This can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave as future work.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q053",
      "query": "What are the potential benefits of using 1.58-bit LLMs on edge and mobile devices?",
      "answer": "The reduced memory and energy consumption of 1.58-bit LLMs allows them to be deployed on these devices, enabling a wide range of applications that were previously not possible. This can greatly enhance the capabilities of edge and mobile devices and enable new and exciting applications of LLMs.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q054",
      "query": "What is the main difference between BitNet b1.58 and the original 1-bit BitNet?",
      "answer": "We have added an additional value of 0 to the original 1-bit BitNet, resulting in 1.58 bits in the binary system.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q056",
      "query": "What software was used to measure the runtime GPU memory and latency of the models?",
      "answer": "The results were measured using the FasterTransformer codebase, which is well-optimized for LLM inference latency on GPU devices. The 2-bit kernel from Ladder [WMC+23] is also integrated for BitNet b1.58.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q057",
      "query": "How does BitNet b1.58 compare to LLaMA LLM in terms of memory usage for larger models?",
      "answer": "The memory consumption follows a similar trend, as the embedding remains full precision and its memory proportion is smaller for larger models.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q059",
      "query": "How does BitNet b1.58 potentially benefit CPU devices in edge and mobile applications?",
      "answer": "Moreover, 1.58-bit LLMs are more friendly to CPU devices, which are the main processors used in edge and mobile devices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving their performance and capabilities.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q060",
      "query": "What is the call to action mentioned in the paper regarding hardware for 1-bit LLMs?",
      "answer": "Going one step further, we envision and call for actions to design new hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm enabled in BitNet [WMD+23].",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q061",
      "query": "How does BitNet b1.58 handle activations compared to full-precision models?",
      "answer": "BitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the activations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q062",
      "query": "What are some effective prompting techniques for LLMs?",
      "answer": "A few prompting techniques have consistently helped with improving performance across a variety of models and tasks: n-shot prompts + in-context learning, chain-of-thought, and providing relevant resources.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q064",
      "query": "What's the benefit of using structured input and output with LLMs?",
      "answer": "Structured input and output help models better understand the input as well as return output that can reliably integrate with downstream systems. Adding serialization formatting to your inputs can help provide more clues to the model as to the relationships between tokens in the context, additional metadata to specific tokens (like types), or relate the request to similar examples in the model's training data.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q066",
      "query": "What factors should be considered when evaluating the quality of RAG's retrieved documents?",
      "answer": "The quality of your RAG's output is dependent on the quality of retrieved documents, which in turn can be considered along a few factors. The first and most obvious metric is relevance. This is typically quantified via ranking metrics such as Mean Reciprocal Rank (MRR) or Normalized Discounted Cumulative Gain (NDCG). MRR evaluates how well a system places the first relevant result in a ranked list while NDCG considers the relevance of all the results and their positions. They measure how good the system is at ranking relevant documents higher and irrelevant documents lower.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q067",
      "query": "What are the advantages of keyword search in information retrieval?",
      "answer": "Keyword-based search, such as BM25, is explicitly designed for this. Finally, after years of keyword-based search, users have likely taken it for granted and may get frustrated if the document they expect to retrieve isn't being returned.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q069",
      "query": "How can multi-turn workflows improve LLM performance?",
      "answer": "It's common knowledge that decomposing a single big prompt into multiple smaller prompts can achieve better results. For example, AlphaCodium: By switching from a single prompt to a multi-step workflow, they increased GPT-4 accuracy (pass@5) on CodeContests from 19% to 44%. The workflow includes: - Reflecting on the problem - Reasoning on the public tests - Generating possible solutions - Ranking possible solutions - Generating synthetic tests - Iterating on the solutions on public and synthetic tests.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q070",
      "query": "What are the benefits of deterministic workflows for AI agents?",
      "answer": "A potential approach is to have agent systems produce deterministic plans which are then executed in a structured, reproducible way. First, given a high-level goal or prompt, the agent generates a plan. Then, the plan is executed deterministically. This allows each step to be more predictable and reliable. Benefits include: - Generated plans can serve as few-shot samples to prompt or finetune an agent. - Deterministic execution makes the system more reliable, and thus easier to test and debug. In addition, failures can be traced to the specific steps in the plan. - Generated plans can be represented as directed acyclic graphs (DAGs) which are easier, relative to a static prompt, to understand and adapt to new situations.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q071",
      "query": "How can you increase output diversity in LLMs beyond adjusting temperature?",
      "answer": "The simplest way is to adjust elements within the prompt. For example, if the prompt template includes a list of items, such as historical purchases, shuffling the order of these items each time they're inserted into the prompt can make a significant difference. Additionally, keeping a short list of recent outputs can help prevent redundancy. In our recommended products example, by instructing the LLM to avoid suggesting items from this recent list, or by rejecting and resampling outputs that are similar to recent suggestions, we can further diversify the responses.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q072",
      "query": "What are the benefits of caching in LLM applications?",
      "answer": "Caching saves cost and eliminates generation latency by removing the need to recompute responses for the same input. Furthermore, if a response has previously been guardrailed, we can serve these vetted responses and reduce the risk of serving harmful or inappropriate content.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q073",
      "query": "When should you consider finetuning an LLM?",
      "answer": "We may have some tasks where even the most cleverly designed prompts fall short. For example, even after significant prompt engineering, our system may still be a ways from returning reliable, high-quality output. If so, then it may be necessary to finetune a model for your specific task.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q077",
      "query": "How can you simplify annotation tasks for LLM evaluations?",
      "answer": "A more effective approach is to simplify the task and reduce the cognitive burden on annotators. Two tasks that work well are binary classifications and pairwise comparisons. In binary classifications, annotators are asked to make a simple yes-or-no judgment on the model's output. They might be asked whether the generated summary is factually consistent with the source document, or whether the proposed response is relevant, or if it contains toxicity. Compared to the Likert scale, binary decisions are more precise, have higher consistency among raters, and lead to higher throughput.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q079",
      "query": "What are the benefits of generating structured output from LLMs?",
      "answer": "For most real-world use cases, the output of an LLM will be consumed by a downstream application via some machine-readable format. For example, Rechat, a real-estate CRM, required structured responses for the front end to render widgets. Similarly, Boba, a tool for generating product strategy ideas, needed structured output with fields for title, summary, plausibility score, and time horizon. Finally, LinkedIn shared about constraining the LLM to generate YAML, which is then used to decide which skill to use, as well as provide the parameters to invoke the skill.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q080",
      "query": "Why is it important to version and pin LLM models?",
      "answer": "Pinning model versions in production can help avoid unexpected changes in model behavior, which could lead to customer complaints about issues that may crop up when a model is swapped, such as overly verbose outputs or other unforeseen failure modes. Additionally, consider maintaining a shadow pipeline that mirrors your production setup but uses the latest model versions. This enables safe experimentation and testing with new releases. Once you've validated the stability and quality of the outputs from these newer models, you can confidently update the model versions in your production environment.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q081",
      "query": "How can you design your UX for Human-In-The-Loop in LLM applications?",
      "answer": "By having the LLM suggest categories upfront, we reduce cognitive load on the user and they don't have to learn our taxonomy to categorize their product! At the same time, by allowing the user to review and edit the suggestion, they have the final say in how their product is classified, putting control firmly in their hands. As a bonus, the third approach creates a natural feedback loop for model improvement. Suggestions that are good are accepted (positive labels) and those that are bad are updated (negative followed by positive labels).",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q084",
      "query": "What is the key idea behind mixed precision decomposition for quantization?",
      "answer": "As such, we can separate these emergent features into a separate, high precision matrix multiplication, quantize the other 99.9% of values to Int8, can combine the output of both matrix multiplications. This avoids the information squishing to zero effect, and we can recover full transformer performance.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q085",
      "query": "At what model size does the phase shift for emergent features occur in transformers?",
      "answer": "The phase shift happens around 6.7B, where 100% of layers use the same dimension for outliers.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q086",
      "query": "How do emergent features affect attention layers in large transformers?",
      "answer": "Attention layers become very sparse. The attention is very concentrated so that just a few sequence dimensions determine the top probability and the overall probability mass. Almost all sequence dimensions have zero probability. However, this is still context-dependent, and the transformer seems to be \"unsure\" what to attend to for some sequences.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q088",
      "query": "How does the LLM.int8() method achieve full performance recovery in quantization?",
      "answer": "We can recover full performance by using the LLM.int8() quantization procedure. You can clearly see that there is a big dip in performance for the 8-bit baseline, which is vector-wise quantization. We need both vector-wise quantization and mixed precision decomposition, that is, the full LLM.int8() method to recover full performance. Either of these methods alone is not sufficient.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q089",
      "query": "What is the intuitive explanation for feature outliers in transformers?",
      "answer": "The most intuitive explanation of feature outliers is that transformers have two processing streams. One stream learns features that explain the inputs, and the other stream learns features that remove other features. Removing noisy, context-irrelevant features is the key to making accurate predictions. The more noisy, context-irrelevant features you remove in early layers, the less conflicting high-level features you have in later layers.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q090",
      "query": "How do emergent features in transformers relate to discretization?",
      "answer": "If you take this mechanism to an extreme, you can get discretization, which goes hand-in-hand with context-dependent memory and \"reasoning\" over elements. Discretization means, you have, say, 100 features, but you decide to remove 99% of them by setting them to zero, and you amplify the rest. The result is a single feature that is now a discrete entity. Once discretized, this entity can be stored and reused later.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q091",
      "query": "How do transformers coordinate the removal of features across layers?",
      "answer": "To coordinate these streams throughout the transformer, it is useful to dedicate certain hidden dimensions to the functionality of removing other features. That way, if the transformer needs to remove features, it knows beforehand which feature dimension to access to perform that functionality.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q094",
      "query": "What happens to outlier features in transformers after the phase shift?",
      "answer": "Outliers become very large quickly. They grow from about 15 for a 6B model to about 60 for a 13B model. OPT-66B has outliers of size around 95, which indicates this growth phase is temporary.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q095",
      "query": "How do emergent features affect the stability of transformer models?",
      "answer": "Transformers become more stable. If you treat the outlier features separately, I believe you can probably run and even train transformers in less than 8-bit precision without degradation in performance.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q096",
      "query": "What is the relationship between emergent features and model perplexity?",
      "answer": "Emergence is not sudden but gradual and grows according to an exponential function related to perplexity and not model size.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q098",
      "query": "What is the significance of the 6.7B parameter threshold in transformer models?",
      "answer": "From these findings it is clear that transformer after the phase shift at 6.7B parameters behave very different to transformers before the phase shift. As such, one should not try to generalize from <6.7B transformers to beyond 6.7B parameters.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q099",
      "query": "How can studying 'scaling laws of emergence' lead to new discoveries in transformer models?",
      "answer": "If we can correlate statistics of a property with increasing capabilities and if this property follows a function that will eventually, \"threshold\", we might have discovered a new emergent property that leads to new capabilities.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q100",
      "query": "What is the main advantage of vector-wise quantization over tensor-wise quantization?",
      "answer": "We can see a matrix multiplication as a sequence of independent inner products between row vectors of A and column vectors of B. We can have a separate constant for each of these vectors. Denormalization happens by multiplying these two constants together for a particular element. No other computation is needed. This is vector-wise quantization. More details in the paper.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q101",
      "query": "How does the coordination of outlier features change as transformer models scale up?",
      "answer": "At the 2.7B to 6B scale, things become much more coordinated. Now 60% of layers agree on which outlier dimension to use.\n\nThe phase shift happens around 6.7B, where 100% of layers use the same dimension for outliers.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q103",
      "query": "How does the performance on the ARC-AGI train set typically compare to the test set?",
      "answer": "On a held-out subset of the train set, where humans get 85% accuracy, my solution gets 72% accuracy. (The train set is significantly easier than the test set as noted here.)",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q104",
      "query": "What is a successful approach for achieving high accuracy on ARC-AGI using GPT-4?",
      "answer": "The main idea behind my solution is very simple: get GPT-4o to generate around 8,000 python programs which attempt to implement the transformation, select a program which is right on all the examples (usually there are 3 examples), and then submit the output this function produces when applied to the additional test input(s). I show GPT-4o the problem as images and in various ascii representations.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q106",
      "query": "What are the key steps in a method that achieved high performance on ARC-AGI using GPT-4?",
      "answer": "At a high level, the method I use is:\n\n- Provide the ARC-AGI problem to GPT-4o, with both an image representation and with various text representations for each grid in the problem. The text representations include showing which cells are occupied by different connected components of colors and showing diffs between the input and output (in cases where the grid shapes are the same).\n\n- Instruct GPT-4o to reason about what the transformation is, reason how to implement the transformation as code, and then finally actually implement the transformation in code.\n\n- Use a few-shot prompt with several carefully handwritten examples of step-by-step reasoning to actually get GPT-4o to do this reasoning somewhat effectively. The resulting prompt is usually around 30k tokens long including images.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q108",
      "query": "What technique can be used to improve the most promising completions in ARC-AGI solutions?",
      "answer": "Take the most promising 12 completions for each problem, and then try to fix each by showing GPT-4o what this program actually outputs on the examples, and then asking GPT-4o to revise the code to make it correct. We sample ~3,000 completions that attempt to fix per problem in total across these 12 starting implementations.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q109",
      "query": "How are the final submissions for each ARC-AGI problem typically selected in high-performing solutions?",
      "answer": "Then, we select 3 submissions to make based on a majority vote over programs which get the examples correct. (In the case where we don't have 3 distinct submissions from programs which get the examples right, we apply some heuristics to pick a submission, but this doesn't matter much.)",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q110",
      "query": "What is the relationship between the number of samples and the accuracy on ARC-AGI?",
      "answer": "There appears to be a relatively clean scaling law. Of course, a linear fit from log(k) to accuracy can't go on forever as it would imply you eventually go above 100% accuracy!",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q111",
      "query": "How much does accuracy typically improve with each doubling of samples in ARC-AGI solutions?",
      "answer": "The fit is in terms of log base 2. So, it indicates an additional 3% correct per doubling of k.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q112",
      "query": "What are the main limitations of GPT-4 in solving ARC-AGI problems?",
      "answer": "GPT-4o is limited by failures other than reasoning, such as:\n\n1. GPT-4o's vision is terrible on grids. When asked to describe what is in a somewhat large grid, it often fails to \"see\" the input correctly, and states wrong facts about what colors are in some location or what shapes are present.\n\n2. GPT-4o isn't that good at coding (especially not for these sort of geometric manipulation problems), and makes simple mistakes like off-by-one errors extremely often.\n\n3. GPT-4o is worse at using long contexts than other models",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q113",
      "query": "How does GPT-4's performance on ARC-AGI compare to human performance?",
      "answer": "To be clear, GPT-4o is also limited by being very dumb. A smart human with only access to the text representation could substantially outperform GPT-4o by spending a bunch of time on each problem.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q115",
      "query": "What predictions have been made about future LLM performance on ARC-AGI?",
      "answer": "60% probability: If a next generation frontier LLM (e.g. GPT-5) was much better at basic visual understanding (e.g. above 85% accuracy on Vibe-Eval hard), using my exact method (with minor adaptation tweaks as needed) on that LLM would surpass typical naive MTurk performance.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q116",
      "query": "Why might ARC-AGI not be an ideal benchmark for evaluating progress towards Transformative AI?",
      "answer": "ARC-AGI probably isn't a good benchmark for evaluating progress towards TAI: substantial \"elicitation\" effort could massively improve performance on ARC-AGI in a way that might not transfer to more important and realistic tasks. I am more excited about benchmarks that directly test the ability of AIs to take the role of research scientists and engineers, for example those that METR is developing.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q119",
      "query": "How does the compute usage in recent high-performing ARC-AGI solutions compare to prior work?",
      "answer": "I used over 1000x more runtime compute per problem than prior work on this benchmark. Maybe prior work on this benchmark scales well with compute and would have gotten higher accuracy with higher resources.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q120",
      "query": "What are some potential improvements for ARC-AGI solutions using GPT-4?",
      "answer": "Some tricks I don't use that might improve performance:\n\n- I currently spend the same number of samples on each problem. You could terminate early on problems which are consistently solved. I think this might reduce cost by perhaps 25-35%.\n\n- In the OpenAI API, I use n < 128 (typically 32 or 16) because n=128 typically errors from what I've seen. Currently it seems like about half of my cost is input tokens, so going to n=128 would roughly halve the cost.\n\n- It would probably help to divide the problems into substantially more categories and then build specialized prompts and tools for each category. This somewhat defeats the point of ARC-AGI though and I'm not sure what these categories would be.\n\n- Doing a second or third revision round could help substantially. (Relative to spending these samples elsewhere.)\n\n- Further extending the debugging/revision process could help substantially.\n\n- Fine-tuning of GPT-4o to better understand the representations I use (and be able to see) would surely help a bunch (though it would be expensive).",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q122",
      "query": "What is the fundamental action of attention heads in transformer models?",
      "answer": "The fundamental action of attention heads is moving information. They read information from the residual stream of one token, and write it to the residual stream of another token.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q123",
      "query": "How can attention heads in transformers be represented mathematically using tensor products?",
      "answer": "Using tensor products, we can describe the process of applying attention as:\n\nh(x) = (Id \u2297 W ) \u22c5\nO\n(A \u2297 Id) \u22c5\n(Id \u2297 W ) \u22c5\nV\nx",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q127",
      "query": "How does the 'freezing attention patterns trick' help in understanding transformer models?",
      "answer": "One thought experiment which might be helpful is to imagine running the model twice. The first time you collect the attention patterns of each head. This only depends on the QK circuit. The second time, you replace the attention patterns with the \"frozen\" attention patterns you collected the first time. This gives you a function where the logits are a linear function of the tokens! We find this a very powerful way to think about transformers.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q129",
      "query": "How do 'virtual attention heads' form in transformer models?",
      "answer": "Virtual attention heads were the terms of the form (A A ) \u2297 (\u2026 W W \u2026) in the path expansion of the logit equation, corresponding to the V-composition of two heads.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q130",
      "query": "What is the 'residual stream' in transformer models and why is it important?",
      "answer": "We generally think of the residual stream as a communication channel, since it doesn't do any processing itself and all layers communicate through it.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q131",
      "query": "How do 'skip-trigrams' function in one-layer attention-only transformers?",
      "answer": "One-layer attention-only transformers can be understood as an ensemble of a bigram model and several \"skip-trigram\" models (affecting the probabilities of sequences \"A\u2026 BC\"). Intuitively, this is because each attention head can selectively attend from the present token (\"B\") to a previous token (\"A\") and copy information to adjust the probability of possible next tokens (\"C\").",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q134",
      "query": "What is the 'OV circuit' in transformer attention heads and what does it represent?",
      "answer": "W W W \u2014 We call this matrix the \"Output-Value (OV) circuit.\" It describes how a given token will affect the output logits if attended to.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q137",
      "query": "How can the eigenvalue summary statistic be used to detect copying behavior in transformer attention heads?",
      "answer": "One natural approach might be to use eigenvectors and eigenvalues. Recall that v is an eigenvector of the matrix M with an eigenvalue \u03bb if Mv = \u03bbv. Let's consider what that means for an OV circuit M = W W W if \u03bb is a positive real number. Then we're saying that there's a linear combination of tokens which increases the linear combination of logits of those same tokens. Very roughly you could think of this as a set of tokens (perhaps all tokens representing plural words for a very broad one, or all tokens starting with a given first letter, or all tokens representing different capitalizations and inclusions of space for a single word for a narrow one) which mutually increase their own probability.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q138",
      "query": "What is the 'QK circuit' in transformer attention heads and how does it function?",
      "answer": "W W W \u2014 We call this matrix the \"query-key (QK) circuit.\" It provides the attention score for every query and key token. That is, each entry describes how much a given query token \"wants\" to attend to a given key token.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q139",
      "query": "How do transformer models handle layer normalization and what are its implications for interpretability?",
      "answer": "Layer normalization is a little more subtle. For each residual stream vector, we subtract off the average activation, normalize by variance, and then multiply by a learned set of diagonal weights and add a learned bias vector. It turns out that subtracting off the average activation is a fixed linear transformation, since it just zeros out a single dimension in the vector space. This means that everything except for normalizing by variance, layer normalization applies a fixed affine transformation. Normalizing by variance multiplies the vector by a scalar, and multiplying by a scalar commutes with all the other operations in a path. As a result, we can fold everything but normalization into adjacent parameters, and then think of the normalization scaling as a variable reweighting of the set of path terms going through that layer normalization.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q141",
      "query": "How does the 'privileged basis' concept apply to different activations in transformer models?",
      "answer": "Privileged Basis vs Basis Free: A privileged basis occurs when some aspect of a model's architecture encourages neural network features to align with basis dimensions, for example because of a sparse activation function such as ReLU. In a transformer, the only vectors with privileged bases are tokens, attention patterns and MLP activations.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q143",
      "query": "How does Matryoshka Representation Learning (MRL) help with embedding efficiency?",
      "answer": "With MRL, only the first n embedding dimensions are used. This approach has already been adopted by some open models like nomic-ai/nomic-embed-text-v1.5 and mixedbread-ai/mxbai-embed-2d-large-v1 (For OpenAIs text-embedding-3-large, we see a performance retention of 93.1% at 12x compression. For nomic's model, we retain 95.8% of performance at 3x compression and 90% at 6x compression.).",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q144",
      "query": "What is embedding quantization and how does it differ from dimensionality reduction?",
      "answer": "However, there is another new approach to achieve progress on this challenge; it does not entail dimensionality reduction, but rather a reduction in the size of each of the individual values in the embedding: Quantization. Our experiments on quantization will show that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q145",
      "query": "How does binary quantization work for embeddings?",
      "answer": "To quantize float32 embeddings to binary, we simply threshold normalized embeddings at 0:\n\n[[[$$f(x) = \\left\\{ \begin{matrix}\n0 & {\text{if~}x \\leq 0} \\\n1 & {\text{if~}x > 0}\n\\end{matrix} \right.$$]{.katex-mathml}",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q147",
      "query": "How does rescoring improve the performance of binary embeddings?",
      "answer": "Yamada et al. (2021) introduced a rescore step, which they called rerank, to boost the performance. They proposed that the float32 query embedding could be compared with the binary document embeddings using dot-product. In practice, we first retrieve rescore_multiplier * top_k results with the binary query embedding and the binary document embeddings -- i.e., the list of the first k results of the double-binary retrieval -- and then rescore that list of binary document embeddings with the float32 query embedding.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q149",
      "query": "How does scalar quantization differ from binary quantization for embeddings?",
      "answer": "We use a scalar quantization process to convert the float32 embeddings into int8. This involves mapping the continuous range of float32 values to the discrete set of int8 values, which can represent 256 distinct levels (from -128 to 127), as shown in the image below. This is done by using a large calibration dataset of embeddings. We compute the range of these embeddings, i.e., the min and max of each embedding dimension. From there, we calculate the steps (buckets) to categorize each value.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q150",
      "query": "How does the size reduction of scalar quantization compare to binary quantization?",
      "answer": "With scalar quantization to int8, we reduce the original float32 embeddings' precision so that each value is represented with an 8-bit integer (4x smaller). Note that this differs from the binary quantization case, where each value is represented by a single bit (32x smaller).",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q151",
      "query": "What is the importance of the calibration dataset in scalar quantization?",
      "answer": "To further boost the retrieval performance, you can optionally apply the same rescoring step as for the binary embeddings. It is important to note that the calibration dataset greatly influences performance since it defines the quantization buckets.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q152",
      "query": "How can you combine binary and scalar quantization for efficient retrieval?",
      "answer": "Combining binary and scalar quantization is possible to get the best of both worlds: the extreme speed from binary embeddings and the great performance preservation of scalar embeddings with rescoring. See the demo below for a real-life implementation of this approach involving 41 million texts from Wikipedia. The pipeline for that setup is as follows:\n\n1.  The query is embedded using the mixedbread-ai/mxbai-embed-large-v1 SentenceTransformer model.\n2.  The query is quantized to binary using the quantize_embeddings function from the sentence-transformers library.\n3.  A binary index (41M binary embeddings; 5.2GB of memory/disk space) is searched using the quantized query for the top 40 documents.\n4.  The top 40 documents are loaded on the fly from an int8 index on disk (41M int8 embeddings; 0 bytes of memory, 47.5GB of disk space).\n5.  The top 40 documents are rescored using the float32 query and the int8 embeddings to get the top 10 documents.\n6.  The top 10 documents are sorted by score and displayed.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q156",
      "query": "What are the average speedups achieved by int8 and binary quantization in retrieval?",
      "answer": "As shown in the table, applying int8 scalar quantization results in an average speedup of 3.66x compared to full-size float32 embeddings. Additionally, binary quantization achieves a speedup of 24.76x on average. For both scalar and binary quantization, even the worst case scenario resulted in very notable speedups.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q157",
      "query": "What potential improvements are suggested for future work in embedding quantization?",
      "answer": "We are looking forward to further advancements of binary quantization. To name a few potential improvements, we suspect that there may be room for scalar quantization smaller than int8, i.e. with 128 or 64 buckets instead of 256.\n\nAdditionally, we are excited that embedding quantization is fully perpendicular to Matryoshka Representation Learning (MRL). In other words, it is possible to shrink MRL embeddings from e.g. 1024 to 128 (which usually corresponds with a 2% reduction in performance) and then apply binary or scalar quantization. We suspect this could speed up retrieval up to 32x for a 3% reduction in quality, or up to 256x for a 10% reduction in quality.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q158",
      "query": "How can embedding quantization be combined with reranker models for improved retrieval performance?",
      "answer": "Lastly, we recognize that retrieval using embedding quantization can also be combined with a separate reranker model. We imagine that a 3-step pipeline of binary search, scalar (int8) rescoring, and cross-encoder reranking allows for state-of-the-art retrieval performance at low latencies, memory usage, disk space, and costs.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q161",
      "query": "What is the purpose of using micro-batching during inference with pipeline parallelism?",
      "answer": "We use micro-batching to improve inference throughput with pipeline parallelism.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q162",
      "query": "How does Llama 3 handle long context pre-training?",
      "answer": "In the final stages of pre-training, we train on long sequences to support context windows of up to 128K tokens. We do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length. We increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. We assess successful adaptation by measuring whether (1) model performance on short-context evaluations has recovered completely and (2) the model perfectly solves \"needle in a haystack\" tasks up to that length.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q163",
      "query": "What is the purpose of the annealing phase in Llama 3's training?",
      "answer": "During pre-training on the final 40M tokens, we linearly annealed the learning rate to 0, maintaining a context length of 128K tokens. During this annealing phase, we also adjusted the data mix to upsample data sources of very high quality; see Section 3.1.3. Finally, we compute the average of model checkpoints (Polyak (1991) averaging) during annealing to produce the final pre-trained model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q166",
      "query": "How does Llama 3 handle multilingual safety?",
      "answer": "Our experiments demonstrate that safety knowledge in English does not readily transfer to other languages, particularly given the nuance of safety policies and language-specific context. Therefore, it is essential to collect high-quality safety data for each language. We also found that the distribution of safety data per language significantly impacts performance from a safety standpoint, with some languages benefiting from transfer learning while others require more language-specific data. To achieve a balance between FRR and VR, we iteratively add adversarial and borderline data while monitoring the impact on both metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q167",
      "query": "What is the purpose of the speech adapter in Llama 3's architecture?",
      "answer": "The speech adapter contains about 100M parameters. It is composed of a convolution layer, a rotary Transformer layer, and a linear layer. The convolution layer has a kernel size of 3 and a stride of 2, which is designed to reduce the speech frame length to 80ms. This allows the model to provide more coarse-grained features to the language model. The Transformer layer has a latent dimension of 3072 and a feed-forward network with a dimension of 4096 which further processes the information from speech with context after the convolutional downsampling. Finally, the linear layer maps the output dimension to match that of the language-model embedding layer.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q170",
      "query": "How does Llama 3 handle the challenge of model heterogeneity during training?",
      "answer": "The model computation is heterogeneous because more computation is performed on some tokens than on others. In particular, image tokens are processed by the image encoder and the cross-attention layers, whereas text tokens are only processed by the language backbone. This heterogeneity leads to bottlenecks in the scheduling of pipeline parallelism. We address this problem by ensuring each pipeline stage contains five layers: namely, four self-attention layers in the language backbone and a cross-attention layer. (Recall that we introduce a cross-attention layer after every fourth self-attention layer.) In addition, we replicate the image encoder on all pipeline stages. Because we train on paired image-text data, this enables us to perform load balancing between the image and text parts of the computation.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q171",
      "query": "What is the purpose of the video adapter in Llama 3's architecture?",
      "answer": "Our model takes as input up to 64 frames (uniformly sampled from a full video), each of which are processed by the image encoder. We model temporal structure in videos through two components: (i) encoded video frames are aggregated by a temporal aggregator which merges 32 consecutive frames into one, (ii) additional video cross attention layers are added before every fourth image cross attention layer. The temporal aggregator is implemented as a perceiver resampler (Jaegle et al., 2021; Alayrac et al., 2022). We pre-train using 16 frames per video (aggregated to 1 frame), but increase the number of input frames to 64 during supervised finetuning.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q172",
      "query": "How does Llama 3 handle the challenge of data heterogeneity during training?",
      "answer": "The data is heterogeneous because, on average, images have more tokens than the associated text: an image has 2,308 tokens, whereas the associated text contains an average of only 192 tokens. As a result, the computation of cross-attention layers requires more time and memory than the computation of self-attention layers. We address this problem by introducing sequence parallelization in the image encoder, so that each GPU processes roughly the same number of tokens. Because the average text size is relatively short, we also use a substantially larger micro-batch size (8 instead of 1).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q173",
      "query": "What is the purpose of the text normalization module in Llama 3's speech generation process?",
      "answer": "As a determinant of the semantic correctness of generated speech, the text normalization (TN) module carries out context-aware transformation from written-form text into the respective spoken form which is eventually verbalized by the downstream components. For example, the written-form text 123 is read as a cardinal number (one hundred twenty three) or spelled digit-by-digit (one two three) depending on the semantic context. The TN system consists of a streaming LSTM-based sequence-tagging model that predicts the sequence of handcrafted TN rules used to transform the input text (Kang et al., 2024). The neural model also takes in Llama 3 embeddings via cross attention to leverage the contextual information encoded therein, enabling minimal text token lookahead and streaming input/output.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q174",
      "query": "How does Llama 3 handle the challenge of numerical instabilities during training?",
      "answer": "After the image encoder is added to the model, we find that performing gradient accumulation in bf16 led to numerical instabilities. The most likely explanation for this is that image tokens are introduced into the language backbone via all cross-attention layers. This implies that numerical deviations in the representation of an image token have an outsized impact on the overall computation because the errors are compounded. We address this by performing gradient accumulation in FP32.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q175",
      "query": "What is the purpose of the prosody model in Llama 3's speech generation process?",
      "answer": "To enhance the naturalness and expressiveness of synthesized speech, we integrate a decoder-only Transformer-based Prosody model (PM) (Radford et al., 2021) that takes the Llama 3 embeddings as an additional input. This integration leverages the linguistic capabilities of Llama 3, utilizing both its textual output and intermediate embeddings at the token rate (Devlin et al., 2018; Dong et al., 2019; Raffel et al., 2020; Guo et al., 2023) to enhance the prediction of prosody features, thus reducing the lookahead required by the model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q177",
      "query": "What is the purpose of the Code Shield in Llama 3's system-level safety components?",
      "answer": "Code Shield is an example of a class of system-level protections based on providing inference-time filtering. In particular, it focuses on detecting the generation of insecure code before it might enter a downstream usecase such as a production system. It does so by leveraging a static analysis library, the Insecure Code Detector (ICD), to identify insecure code. ICD uses a suite of static analysis tools to perform the analysis across 7 programming languages. These kinds of guardrails are generally useful for developers, who can deploy multi-layered protections in various applications.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q178",
      "query": "How does Llama 3 handle the challenge of tool usage safety?",
      "answer": "The diversity of possible tools and the implementation of the tool usage call and integration into the model make tool usage a challenging capability to fully mitigate (Wallace et al., 2024). We focus on the search usecase. Violation and false refusal rates are shown in Figure 20. We tested against the Comp. 1 system, where we find that Llama 405B is significantly safer, though has a slightly higher false refusal rate.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q182",
      "query": "How can long-context capabilities be added to a language model?",
      "answer": "In the final stages of pre-training, we train on long sequences to support context windows of up to 128K tokens. We do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length. We increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. We assess successful adaptation by measuring whether (1) model performance on short-context evaluations has recovered completely and (2) the model perfectly solves \"needle in a haystack\" tasks up to that length.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q183",
      "query": "What is the trade-off between violation rate and false refusal rate in AI safety?",
      "answer": "While a low violation rate is desirable, it is critical to consider false refusal as a counter-metric, as a model that always refuses is maximally safe, but not helpful in the slightest. Similarly, a model that always answers every prompt, regardless of how problematic the request, would be overly harmful and toxic. In Figure 21, leveraging our internal benchmarks, we explore how different models and systems in industry navigate this trade off and how Llama 3 compares. We find that our models achieve very competitive violation rate metrics while keeping false refusal rate low as well, indicating a solid balance between helpfulness and safety.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q185",
      "query": "Why is multilingual safety challenging in language models?",
      "answer": "Our experiments demonstrate that safety knowledge in English does not readily transfer to other languages, particularly given the nuance of safety policies and language-specific context. Therefore, it is essential to collect high-quality safety data for each language. We also found that the distribution of safety data per language significantly impacts performance from a safety standpoint, with some languages benefiting from transfer learning while others require more language-specific data. To achieve a balance between FRR and VR, we iteratively add adversarial and borderline data while monitoring the impact on both metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q187",
      "query": "How can language confusion in multilingual speech recognition be mitigated?",
      "answer": "Multilingual ASR and AST modeling often results in language confusion/interference, which leads to degraded performance. A popular way to mitigate this is to incorporate language identification (LID) information, both on the source and target side. This can lead to improved performance in the predetermined set of directions, but it does come with potential loss of generality. For instance, if a translation system expects LID on both source and target side, then the model will not likely to show good zero-shot performance in directions that were not seen in training. So our challenge is to design a system that allows LID information to some extent, but keeps the model general enoug h such that we can have the model do speech translation in unseen directions. To address this, we design system prompts which only contain LID for the text to be emitted (target side). There is no LID information for the speech input (source side) in these prompts, which also potentially allows it to work with code-switched speech.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q188",
      "query": "What is the role of an image adapter in multimodal AI models?",
      "answer": "We introduce cross-attention layers between the visual token representations produced by the image encoder and the token representations produced by the language model (Alayrac et al., 2022). The cross-attention layers are applied after every fourth self-attention layer in the core language model. Like the language model itself, the cross-attention layers use generalized query attention (GQA) for increased efficiency. The cross-attention layers introduce substantial numbers of additional trainable parameters into the model: for Llama 3 405B, the cross-attention layers have \u2248100B parameters.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q190",
      "query": "What is the purpose of a video adapter in multimodal AI models?",
      "answer": "Our model takes as input up to 64 frames (uniformly sampled from a full video), each of which are processed by the image encoder. We model temporal structure in videos through two components: (i) encoded video frames are aggregated by a temporal aggregator which merges 32 consecutive frames into one, (ii) additional video cross attention layers are added before every fourth image cross attention layer. The temporal aggregator is implemented as a perceiver resampler (Jaegle et al., 2021; Alayrac et al., 2022). We pre-train using 16 frames per video (aggregated to 1 frame), but increase the number of input frames to 64 during supervised finetuning.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q191",
      "query": "How can data heterogeneity be addressed in multimodal AI training?",
      "answer": "The data is heterogeneous because, on average, images have more tokens than the associated text: an image has 2,308 tokens, whereas the associated text contains an average of only 192 tokens. As a result, the computation of cross-attention layers requires more time and memory than the computation of self-attention layers. We address this problem by introducing sequence parallelization in the image encoder, so that each GPU processes roughly the same number of tokens. Because the average text size is relatively short, we also use a substantially larger micro-batch size (8 instead of 1).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q192",
      "query": "What is the role of text normalization in AI-based speech generation?",
      "answer": "As a determinant of the semantic correctness of generated speech, the text normalization (TN) module carries out context-aware transformation from written-form text into the respective spoken form which is eventually verbalized by the downstream components. For example, the written-form text 123 is read as a cardinal number (one hundred twenty three) or spelled digit-by-digit (one two three) depending on the semantic context. The TN system consists of a streaming LSTM-based sequence-tagging model that predicts the sequence of handcrafted TN rules used to transform the input text (Kang et al., 2024). The neural model also takes in Llama 3 embeddings via cross attention to leverage the contextual information encoded therein, enabling minimal text token lookahead and streaming input/output.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q194",
      "query": "How can long-context safety be ensured in large language models?",
      "answer": "Long-context models are vulnerable to many-shot jailbreaking attacks without targeted mitigation (Anil et al., 2024). To address this, we finetune our models on SFT datasets that include examples of safe behavior in the presence of demonstrations of unsafe behavior in context. We develop a scalable mitigation strategy that significantly reduces VR, effectively neutralizing the impact of longer context attacks even for 256-shot attacks. This approach shows little to no impact on FRR and most helpfulness metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q195",
      "query": "What is the purpose of Code Shield in AI safety systems?",
      "answer": "Code Shield is an example of a class of system-level protections based on providing inference-time filtering. In particular, it focuses on detecting the generation of insecure code before it might enter a downstream usecase such as a production system. It does so by leveraging a static analysis library, the Insecure Code Detector (ICD), to identify insecure code. ICD uses a suite of static analysis tools to perform the analysis across 7 programming languages. These kinds of guardrails are generally useful for developers, who can deploy multi-layered protections in various applications.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q196",
      "query": "Why is tool usage safety challenging in large language models?",
      "answer": "The diversity of possible tools and the implementation of the tool usage call and integration into the model make tool usage a challenging capability to fully mitigate (Wallace et al., 2024). We focus on the search usecase. Violation and false refusal rates are shown in Figure 20. We tested against the Comp. 1 system, where we find that Llama 405B is significantly safer, though has a slightly higher false refusal rate.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q197",
      "query": "How can pipeline parallelism be improved in large-scale AI training?",
      "answer": "To address these issues, we modify our pipeline schedule as shown in Figure 6, which allows setting N flexibly\u2014in this case N = 5, which can run a arbitrary number of micro-batches in each batch. This allows us to run: (1) fewer micro-batches than the number of stages when we have batch size limit at large scale; or (2) more micro-batches to hide point-to-point communication, finding a sweet spot between DFS and breadth first schedule (BFS) for the best communication and memory efficiency. To balance the pipeline, we reduce one Transformer layer each from the first and the last stages, respectively. This means that the first model chunk on the first stage has only the embedding, and the last model chunk on the last stage has only output projection and loss calculation.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q199",
      "query": "What are the three key factors that strongly influence language model performance?",
      "answer": "Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the amount of compute C used for training.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q200",
      "query": "How does language model performance scale with model size, dataset size, and compute?",
      "answer": "Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance must flatten out eventually before reaching zero loss.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q203",
      "query": "What is the relationship between model size and sample efficiency in language models?",
      "answer": "Large models are more sample-efficient than small models, reaching the same level of performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q204",
      "query": "How should compute be allocated for optimal language model training?",
      "answer": "When working within a fixed compute budget C but without any other restrictions on the model size N or available data D, we attain optimal performance by training very large models and stopping significantly short of convergence (see Figure 3). Maximally compute-efficient training would therefore be far more sample efficient than one might expect based on training small models to convergence, with data requirements growing very slowly as D \u223c C0.27 with training compute.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q206",
      "query": "How does the performance of language models transfer to different text distributions?",
      "answer": "When we evaluate models on text with a different distribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss \u2013 in other words, transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q207",
      "query": "What is the relationship between model architecture and performance in Transformers?",
      "answer": "Performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section 3)",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q208",
      "query": "How does the critical batch size change as language model performance improves?",
      "answer": "The critical batch size B_crit follows a power law in the loss as performance increase, and does not depend directly on the model size. We find that the critical batch size approximately doubles for every 13% decrease in loss.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q209",
      "query": "What is the optimal allocation of compute between model size and amount of data processed during training?",
      "answer": "Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we should predominantly increase the model size N, while simultaneously scaling up the batch size via B \u221d B_crit with negligible increase in the number of serial steps.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q210",
      "query": "How does the performance of LSTMs compare to Transformers for language modeling?",
      "answer": "We see from these figures that the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match the Transformer performance for later tokens.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q211",
      "query": "What is the relationship between overfitting and dataset size in language models?",
      "answer": "Overfitting should be related to the variance or the signal-to-noise ratio of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function, since we expect to be able to expand the loss about the D \u2192 \u221e limit.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q214",
      "query": "How does the choice of learning rate schedule affect the final performance of language models?",
      "answer": "We conclude that the choice of learning rate schedule is mostly irrelevant, as long as the total summed learning rate is sufficiently large, and the schedule includes a warmup period and a final decay to near-vanishing learning rate. Variations among schedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different training runs.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q217",
      "query": "What is the relationship between model size and the early stopping step in language model training?",
      "answer": "We characterize the step on which early stopping occurs, as a function of the extent of overfitting. The red line indicates a lower bound for early stopping that is derived in Section 5.3.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q219",
      "query": "What is TinyStories and how does it differ from typical language model training datasets?",
      "answer": "We introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q220",
      "query": "How do small language models (SLMs) trained on TinyStories compare to larger models in terms of coherent text generation?",
      "answer": "We show that TinyStories can be used to train and evaluate SLMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q221",
      "query": "What is the GPT-4 based framework for evaluating language models' generated content?",
      "answer": "We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often require the model's output to be very structured, and moreover it provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and instruction-following.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q223",
      "query": "How does the research demonstrate that small language models can exhibit reasoning capabilities?",
      "answer": "To complete this sentence, the language model needs to invoke reasoning: it needs to apply the principle of disjunction elimination: if Lily wants either a cat or a dog, and she cannot get a dog, then she must choose a cat. It also needs to choose a words that expresses Lily's intention or action that is coherent with the tone and style of the text.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q224",
      "query": "What challenges do small language models (SLMs) typically face in text generation tasks?",
      "answer": "For example, models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate any consistent text beyond a few words even after extensive training on large corpora such as the Pile, Common Crawl or the CC-100. These models often produce incoherent, repetitive, or nonsensical sentences, and fail to maintain a clear topic or a logical structure across paragraphs.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q228",
      "query": "How does the GPT-Eval framework assess the quality of generated stories?",
      "answer": "We provide the model with a story's beginning, taken from a manually-prepared dataset consisting of around 50 prompts, generate a completion using the model, and provide the story's beginning together with the model's completion to GPT-4, asking it to grade the completion assignment in terms of grammar, creativity, and its consistency with the beginning of the story.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q231",
      "query": "How is the Rouge score used to quantitatively measure the similarity between generated stories and the training dataset?",
      "answer": "We measure the diversity of the stories quantitatively using word and n-gram overlap. We inspect the overlap of words and n-grams between different stories generated by the models, and compare them with the overlap in the dataset. We find that the models' generations have a very low overlap with the dataset, indicating that they are not repeating the same words or phrases. We use the standard Rouge score, for the source text T1, T2 with k-gram Gk(T1), Gk(T2) respectively, the rougek precision score is defined as:\nRk,p(T1, T2) = 1/|Gk(T1)| \u03a3t\u2208Gk(T1) 1t\u2208Gk(T2).\nThe Rougek precision score measures how many k-grams in T1 is included in that of T2. The final Rougek score (fmeasure) is given as:\nRk(T1, T2) = 2Rk(T1, T2) \u00d7 Rk(T2, T1) / (Rk(T1, T2) + Rk(T2, T1)).",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q232",
      "query": "What evidence suggests that models trained on TinyStories have higher interpretability compared to larger models?",
      "answer": "Our findings suggest that the attention heads exhibit diverse and meaningful functions, such as attending to the previous word, the subject of the sentence, the end of the sentence, or the main topic of the story. We also observe that some attention heads specialize in generating certain types of words, such as nouns, verbs, or punctuation. These results suggest that the attention heads learn to perform different linguistic tasks and capture different aspects of the stories.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q233",
      "query": "How do the researchers analyze the roles of different neurons in the MLP of models trained on TinyStories?",
      "answer": "We use the method similar to [18] to identify the most influential tokens in the MLP for each neuron. We find that some neurons are activated on words that have a specific role in the sentence (such as the subject or the action), or in the story (such as the introduction of the protagonist). These findings suggest that the neurons in the MLP learn to encode different semantic and stylistic information and influence the generation process.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q234",
      "query": "What patterns are observed in the attention heads of models trained on TinyStories?",
      "answer": "There seems to be a clear separation between heads with attention pattern based mainly on the distance between tokens, and heads whose attention pattern has a stronger dependence on the semantic meaning:\nDistance based attention. Out of the 16 attention heads, we observe multiple positional-based attention heads, such that each token attends to tokens with a prescribed relative distance. Different heads are associated with different distances.\nSemantic based attention. We also observe that there is (1). one head that the word \"the\" and \"a\" all attend to the word \"banana\", interestingly, the \"the\" at \"the park\" also attends to \"banana\", but the model still manage to generate \"park\", which is the consistent completion. (2). Another attention head gives a pattern where the tokens \"the\" and \"a\" all attend to \"park\". (3). There is third head that most of the words attend to the name of \"Tom\" and \"Lucy\".",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q236",
      "query": "How does the number of attention heads affect the performance of models trained on TinyStories?",
      "answer": "Our results, shown in Figure 24, suggest that in the regime where the number of heads is small, increasing it improves the performance of the model across all metrics.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q237",
      "query": "What are the main contributions of the TinyStories research?",
      "answer": "\u2022 Our main contribution is that we show TinyStories can be used to train and evaluate SLMs that are much smaller than the state-of-the-art models (below 10 million parameters with an embedding dimension of 256), or have much simpler architectures (with only one transformer block), yet still produce a diverse set of fluent and consistent stories that are comparable or superior to those generated by larger and more complex models. Moreover, despite of the small size of the models, we still observe an emergence of reasoning capabilities, knowledge of general facts and ability to follow certain instructions.\n\u2022 We introduce a new paradigm for evaluating language models using GPT-4, which overcomes many of the limitations of standard benchmarks.\n\u2022 We show that although the training of generative models on TinyStories can typically be done in less than a day on a single GPU, they still exhibit many behaviors similar to the ones observed in LLMs, such as scaling laws, trade-offs between width and depth, etc. Even with limited computational resources, we are able to conduct extensive experiments to study the effects of different hyperparameters, architectures and training methods on the performance and quality of the models.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q238",
      "query": "What are the limitations in assessing the creativity of language models trained on simplified datasets?",
      "answer": "We provided evidence to the fact that the models trained on TinyStories are able to produce genuinely new stories, rather than just copying chunks of text the dataset. It remains a challenge, however, to assess the true extent of the \"creativity\" of our models, and to which the models reflect a certain \"understanding\" (on a very low level of course) of the stories that they produce as opposed to just template matching to create a plausible continuation. We hope that this dataset can be used in future works to obtain insights about the degree of creativity of language models.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q241",
      "query": "What are the main constraints for the 'Cramming' setup in this study?",
      "answer": "\u2022 A transformer-based language model of arbitrary size is trained with masked-language modeling, completely from scratch.\n\u2022 Existing pretrained models cannot be included in any part of the pipeline.\n\u2022 Any raw text (excluding downstream data) can be included for training. This means that one can achieve speedups by making judicious choices about how and when to sample data, provided the sampling mechanism does not require a pre-trained model.\n\u2022 The downloading and pre-processing of raw data is exempted from the total compute budget. Pre-processing may include CPU-based tokenizer construction, tokenization, and filtering, but cannot include representation learning (e.g. pre-training a word embedding is not allowed, unless it is counted towards the final runtime).\n\u2022 Training proceeds on a single GPU for 24 hours.\n\u2022 Downstream performance is evaluated on GLUE (Wang et al., 2018). Downstream finetun-ing on GLUE is limited to brief training with only the training data of the downstream task (we consider 5 epochs or less) and needs to work with hyperparameters set globally for all GLUE tasks. Downstream finetuning is excluded from the total compute budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q242",
      "query": "What GPUs were used in the cramming experiment?",
      "answer": "In our implementation, we analyze both a setup with a classical rtx2080ti GPU (released September 2018) and separate setups with a more modern rtxa4000 or rtxa6000 GPU (re-leased October 2020). We pair each unit with 4 CPU cores and 32GB of RAM.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q243",
      "query": "Why is it challenging to scale down language model training?",
      "answer": "An unsurprising consequence of these laws is that scaling down is hard; while smaller model architectures enable speeding up gradient computations, overall rates of model improvement over time remain nearly constant.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q244",
      "query": "What is the significance of scaling laws in language model training?",
      "answer": "Scaling Laws The difficulty in finding tangible improvements is echoed in the scaling laws of Kaplan et al. (2020). Over a wide range of transformer model shapes, Kaplan et al. (2020) find only model size (as number of parameters in non-embedding layers) strongly predicts performance. Further, for a fixed compute budget, an optimal model size can be derived, but performance is only mildly connected to model size - larger models processes less data per unit of compute, but improve faster by almost the same margin.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q245",
      "query": "What data sources were used for training in the cramming experiment?",
      "answer": "We start our investigation with a close analogue to the original raw text sources of Devlin et al. (2019), using a recent dump of the English Wikipedia (20220301.en) and En-glish bookcorpus, noting the commentary of Tan (2019); Bandy & Vincent (2021).",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q247",
      "query": "What modifications were made to the attention block in the cramming experiment?",
      "answer": "Attention Block: We disable all QKV biases (Dayma et al., 2021). This exploits the scaling law by removing a layer of computation, making the forward and backward pass somewhat faster, while keeping the model size nearly constant. We find that we could decrease gradient costs by reducing the number of attention heads (Merity, 2019; Araabi & Monz, 2020; Liu et al., 2021b; Javaheripi et al., 2022), as this parallelizes better on the GPU and provides a slight performance boost. Yet, reducing the amount of heads also decreases finetuning performance, so we ultimately keep all 12 heads.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q249",
      "query": "How does the cramming experiment handle positional embeddings?",
      "answer": "We implement scaled sinusoidal positional embeddings as described in Hua et al. (2022), finding incremental benefits over learned or unscaled sinusoidal embeddings. We see no improvements from decoupling the input and output embeddings (Chung et al., 2020). The suggestion from Lan et al. (2019) to factorize the input embedding provides no gains in our setting. We include a layer normalization at the end of the embedding block.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q252",
      "query": "What is the optimal batch size found in the cramming experiment?",
      "answer": "We find that the optimal batch size in this setting is around 1536 for minimal pretraining loss, but 4032 for maximal downstream performance for the 2080ti, i.e. we accumulate gradients and only perform an update every 16 and 42 forward/backward passes, respectively. For the larger A4000 and A6000 cards, this corresponds to a micro-batch size of 128/256 and final batch size of 4096, which we again accumulate.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q254",
      "query": "What data filtering techniques were found to be effective in the cramming experiment?",
      "answer": "We then test filtering for un-compressible data. We use the tokenizer itself to remove all training sequences from C4 set that cannot be compressed well; we simply set a threshold t, e.g. t = 0.3, and drop all entries from the dataset where the number of tokens in the entry is larger than t times the number of raw characters. This removes, for example, sequences consisting of hard-to-compress HTML or markdown code. Surprisingly, this results in a measurable improvement on C4, summarized in Table 2.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q255",
      "query": "How does the cramming model perform on the GLUE benchmark compared to BERT-base?",
      "answer": "Overall, performance is surprisingly decent, especially for the larger datasets of MNLI, QQP, QNLI and SST-2, where downstream finetuning can smooth out the remaining differences between the full BERT model and the crammed variants. Further, we find substantial gains over both a naive BERT training with limited budget, and over the recipe described in (Izsak et al., 2021).",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q256",
      "query": "What is the main limitation of the cramming model's performance on GLUE tasks?",
      "answer": "Overall, the crammed model mostly works, even for smaller datasets. The average is brought down however by a significant drop on CoLA (corpus of linguistic acceptability) (Warstadt et al., 2019).",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q257",
      "query": "How does the cramming recipe perform when given more compute budget?",
      "answer": "We also verify what happens if the cramming recipe discussed so far is used with more budget. To this end, we train models for 48 hours on 8 A6000 GPUs, which ends up to be 208 total exaFLOP, c.f. Table 1. We directly apply the setting described so far, simply scaling the learning rate schedules to cover the new budget of 48 hours. In Table 6, we find that the discussed recipe does immediately generalize to larger compute budgets. This is surprising, not the least, as now, the dataset (which was sorted in Section 4.4 is now too small and repeated multiple times. The newly trained models have strong performances, especially on MNLI and SST-2, where they significantly outperform the original BERT checkpoint and fall into a similar range as the roBERTA-base checkpoint of Liu et al. (2019), which was trained with much more compute.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q258",
      "query": "What are the main conclusions drawn from the cramming experiment?",
      "answer": "We discuss how much performance a transformer-based language model can achieve when crammed into a setting with very limited compute, finding that several strands of modification lead to decent downstream performance on GLUE. Overall though, cramming language models appears hard, as we empirically find many implications of Kaplan et al. (2020) to still hold in this regime, and for examples improvements through larger models are evened out by their slower speed. We hope that this work can provide a baseline for explorations of the question of cramming we formalize in Section 2 and cast an additional light on a number of improvements and tricks proposed for transformer architectures in recent years.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q260",
      "query": "How does model size affect the efficiency of language model training?",
      "answer": "Models with more parameters learn more efficiently, as their MLM loss decreases faster on a per-gradient basis. However, smaller architectures make up for their slower learning efficiency by higher throughput, and thus process more tokens over the limited budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q261",
      "query": "What are some techniques to improve the performance of transformer-based models without increasing model size?",
      "answer": "We can exploit scaling laws by quickly searching for architectural choices that speed up computation while keeping model size roughly constant. A number of obvious optimizations fall into this category, and we describe them below, in addition to several other tweaks that provide marginal but worthwhile/free gains.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q263",
      "query": "What are the trade-offs between model size and training efficiency for language models?",
      "answer": "Per-token efficiency of training depends strongly on model size, but not transformer type. Furthermore, smaller models learn less efficiently, and this largely mitigates any throughput gains. Fortunately, the fact that training efficiency is nearly constant across models of the same size means that we can boost performance by finding architecture modifications that speed up gradient computation while keeping the parameter count nearly constant.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q267",
      "query": "What are some strategies for efficient tokenization in language model training?",
      "answer": "We force all text into lower-case, strip accents and non-ascii characters and create an English tokenizer from scratch based only on this data. We choose WordPiece with a vocabulary size of 2^15 = 32768 (Wu et al., 2016). We found no significant change in performance with BPE (Sennrich et al., 2016) or SentencePiece with Unigrams (Kudo, 2018; Kudo & Richardson, 2019). Smaller vocabulary sizes (2^12, 2^13, 2^14) resulted in worse performance, while larger vocabulary sizes (2^16) we not reliably better.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q268",
      "query": "How can language models be efficiently trained to perform well on downstream tasks with limited resources?",
      "answer": "We find substantial gains over both a naive BERT training with limited budget, and over the recipe described in (Izsak et al., 2021). Overall, performance is surprisingly decent, especially for the larger datasets of MNLI, QQP, QNLI and SST-2, where downstream finetuning can smooth out the remaining differences between the full BERT model and the crammed variants.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q269",
      "query": "What is the main achievement of AlphaZero in chess and shogi?",
      "answer": "Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q270",
      "query": "How does AlphaZero's approach differ from traditional chess programs?",
      "answer": "Instead of a handcrafted evaluation function and move ordering heuristics, AlphaZero utilises a deep neural network (p, v) = f\u03b8(s) with parameters \u03b8. This neural network takes the board position s as an input and outputs a vector of move probabilities p with components pa = P r(a|s) for each action a, and a scalar value v estimating the expected outcome z from position s, v \u2248 E[z|s]. AlphaZero learns these move probabilities and value estimates entirely from self-play; these are then used to guide its search.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q274",
      "query": "How long did it take for AlphaZero to outperform Stockfish in chess?",
      "answer": "In chess, AlphaZero outperformed Stockfish after just 4 hours (300k steps)",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q275",
      "query": "What was the performance of AlphaZero against Stockfish in a 100-game match?",
      "answer": "AlphaZero convincingly defeated all opponents, losing zero games to Stockfish and eight games to Elmo (see Supplementary Material for several example games), as well as defeating the previous version of AlphaGo Zero",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q278",
      "query": "What was unique about AlphaZero's approach to learning chess openings?",
      "answer": "Each of these openings is independently discovered and played frequently by AlphaZero during self-play training. When starting from each human opening, AlphaZero convincingly defeated Stockfish, suggesting that it has indeed mastered a wide spectrum of chess play.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q280",
      "query": "How does AlphaZero represent moves in chess as output from its neural network?",
      "answer": "We represent the policy \u03c0(a|s) by a 8 \u00d7 8 \u00d7 73 stack of planes encoding a probability distribution over 4,672 possible moves. Each of the 8 \u00d7 8 positions identifies the square from which to \"pick up\" a piece. The first 56 planes encode possible 'queen moves' for any piece: a number of squares [1..7] in which the piece will be moved, along one of eight relative compass directions {N, N E, E, SE, S, SW, W, N W }. The next 8 planes encode possible knight moves for that piece. The final 9 planes encode possible underpromotions for pawn moves or captures in two possible diagonals, to knight, bishop or rook respectively. Other pawn moves or captures from the seventh rank are promoted to a queen.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q281",
      "query": "What hardware was used to train AlphaZero?",
      "answer": "Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 first-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q282",
      "query": "How many simulations did AlphaZero use during training?",
      "answer": "During training, each MCTS used 800 simulations.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q283",
      "query": "What was the learning rate schedule used for training AlphaZero?",
      "answer": "The learning rate was set to 0.2 for each game, and was dropped three times (to 0.02, 0.002 and 0.0002 respectively) during the course of training.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q284",
      "query": "How was Dirichlet noise applied in AlphaZero's training?",
      "answer": "Dirichlet noise Dir(\u03b1) was added to the prior probabilities in the root node; this was scaled in inverse proportion to the approximate number of legal moves in a typical position, to a value of \u03b1 = {0.3, 0.15, 0.03} for chess, shogi and Go respectively.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q285",
      "query": "What versions of Stockfish and Elmo were used for comparison with AlphaZero?",
      "answer": "To evaluate performance in chess, we used Stockfish version 8 (official Linux release) as a baseline program, using 64 CPU threads and a hash size of 1GB. To evaluate performance in shogi, we used Elmo version WCSC27 in combination with YaneuraOu 2017 Early KPPT 4.73 64AVX2 with 64 CPU threads and a hash size of 1GB with the usi option of EnteringKingRule set to NoEnteringKing.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q286",
      "query": "How were Elo ratings calculated for AlphaZero and its opponents?",
      "answer": "We estimate the probability that player a will defeat player b by a logistic function p(a defeats b) = 1/(1+exp (celo(e(b)\u2212e(a))), and estimate the ratings e(\u00b7) by Bayesian logistic regression, computed by the BayesElo program (10) using the standard constant celo = 1/400. Elo ratings were computed from the results of a 1 second per move tournament between iterations of AlphaZero during training, and also a baseline player: either Stockfish, Elmo or AlphaGo Lee respectively.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q287",
      "query": "What were the time control settings for the matches between AlphaZero and its opponents?",
      "answer": "Settings were chosen to correspond with computer chess tournament conditions: each player was allowed 1 minute per move, resignation was enabled for all players (-900 centipawns for 10 consecutive moves for Stockfish and Elmo, 5% winrate for AlphaZero). Pondering was disabled for all players.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q288",
      "query": "How does AlphaZero's approach differ from traditional game-playing programs in terms of domain knowledge?",
      "answer": "AlphaZero is a generic reinforcement learning algorithm \u2013 originally devised for the game of Go \u2013 that achieved superior results within a few hours, searching a thousand times fewer positions, given no domain knowledge except the rules of the game.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q289",
      "query": "What are the advantages of using Monte Carlo tree search in game-playing AI?",
      "answer": "Each search consists of a series of simulated games of self-play that traverse a tree from root sroot to leaf. Each simulation proceeds by selecting in each state s a move a with low visit count, high move probability and high value (averaged over the leaf states of simulations that selected a from s) according to the current neural network f\u03b8. The search returns a vector \u03c0 representing a probability distribution over moves, either proportionally or greedily with respect to the visit counts at the root state.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q291",
      "query": "What are the challenges in developing AI for asymmetric board games?",
      "answer": "The rules of chess and shogi are asymmetric, and in general symmetries cannot be assumed. AlphaZero does not augment the training data and does not transform the board position during MCTS.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q292",
      "query": "How can neural networks be used to evaluate board positions in strategy games?",
      "answer": "This neural network takes the board position s as an input and outputs a vector of move probabilities p with components pa = P r(a|s) for each action a, and a scalar value v estimating the expected outcome z from position s, v \u2248 E[z|s].",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q294",
      "query": "How can machine learning be used to discover and evaluate chess openings?",
      "answer": "Each of these openings is independently discovered and played frequently by AlphaZero during self-play training. When starting from each human opening, AlphaZero convincingly defeated Stockfish, suggesting that it has indeed mastered a wide spectrum of chess play.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q295",
      "query": "What are effective ways to represent board states as input for neural networks in game-playing AI?",
      "answer": "The input to the neural network is an N \u00d7 N \u00d7 (M T + L) image stack that represents state using a concatenation of T sets of M planes of size N \u00d7 N . Each set of planes represents the board position at a time-step t \u2212 T + 1, ..., t, and is set to zero for time-steps less than 1. The board is oriented to the perspective of the current player. The M feature planes are composed of binary feature planes indicating the presence of the player's pieces, with one plane for each piece type, and a second set of planes indicating the presence of the opponent's pieces.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q296",
      "query": "How can move selection in chess be represented as an output from a neural network?",
      "answer": "We represent the policy \u03c0(a|s) by a 8 \u00d7 8 \u00d7 73 stack of planes encoding a probability distribution over 4,672 possible moves. Each of the 8 \u00d7 8 positions identifies the square from which to \"pick up\" a piece. The first 56 planes encode possible 'queen moves' for any piece: a number of squares [1..7] in which the piece will be moved, along one of eight relative compass directions {N, N E, E, SE, S, SW, W, N W }. The next 8 planes encode possible knight moves for that piece. The final 9 planes encode possible underpromotions for pawn moves or captures in two possible diagonals, to knight, bishop or rook respectively. Other pawn moves or captures from the seventh rank are promoted to a queen.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q297",
      "query": "What role does exploration noise play in training game-playing AI systems?",
      "answer": "Dirichlet noise Dir(\u03b1) was added to the prior probabilities in the root node; this was scaled in inverse proportion to the approximate number of legal moves in a typical position, to a value of \u03b1 = {0.3, 0.15, 0.03} for chess, shogi and Go respectively.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q298",
      "query": "How can self-play be used to train AI systems for complex board games?",
      "answer": "The parameters \u03b8 of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters \u03b8. Games are played by selecting moves for both players by MCTS, at \u223c \u03c0t. At the end of the game, the terminal position sT is scored according to the rules of the game to compute the game outcome z: \u22121 for a loss, 0 for a draw, and +1 for a win. The neural network parameters \u03b8 are updated so as to minimise the error between the predicted outcome vt and the game outcome z, and to maximise the similarity of the policy vector pt to the search probabilities \u03c0t.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q299",
      "query": "What is the motivation behind using sparse autoencoders for dictionary learning in language models?",
      "answer": "We chose to study a sparse autoencoder approximation of dictionary learning (similar to Sharkey, et al. ). This was for two reasons. First, a sparse autoencoder can readily scale to very large datasets, which we believe is necessary to characterize the features present in a model trained on a large and diverse corpus. Secondly, we have a concern that iterative dictionary learning methods might be \"too strong\", in the sense of being able to recover features from the activations which the model itself cannot access. Exact compressed sensing is NP-hard, which the neural network certainly isn't doing. By contrast, a sparse autoencoder is very similar in architecture to the MLP layers in language models, and so should be similarly powerful in its ability to recover features from superposition.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q301",
      "query": "What is the 'neuron resampling' technique and why is it used in training sparse autoencoders?",
      "answer": "We find that \"resampling\" these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster, see Feature Density Histograms) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q302",
      "query": "How do the researchers evaluate the interpretability of features extracted by sparse autoencoders?",
      "answer": "In this section, we use three different methods to analyze how interpretable the typical feature is, and how that compares to neurons: human analysis, and two forms of automated interpretability. All three approaches find that features are much more interpretable than neurons.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q305",
      "query": "What is the 'activation similarity' metric and how is it used to compare features between models?",
      "answer": "One natural approach is to think of a feature as a function assigning values to datapoints; two features would be similar in this sense if they take similar values over a diverse set of data. This general approach has been explored by a number of prior papers (e.g. ). In practice, this can be approximated by representing the feature as a vector, with indices corresponding to a fixed set of data points. We call the correlations between these vectors the activation similarity between features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q307",
      "query": "What is the 'finite state automata'-like behavior observed in the extracted features?",
      "answer": "One of the most striking phenomena we've observed in our study of the features in one-layer models is the existence of \"finite state automata\"-like assemblies of features. These assemblies aren't circuits in the conventional sense \u2013 they're formed by one feature increasing the probability of tokens, which in turn cause another feature to fire on the next step, and so on.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q308",
      "query": "How do the researchers assess whether the extracted features reflect properties of the model or just the dataset?",
      "answer": "To assess the effect of dataset correlations on the interpretability of feature activations, we run dictionary learning on a version of our one-layer model with random weights. The resulting features are here, and contain many single-token features (such as \"span\", \"file\", \".\", and \"nature\") and some other features firing on seemingly arbitrary subsets of different broadly recognizable contexts (such as LaTeX or code). However, we are unable to construct interpretations for the non-single-token features that make much sense and invite the reader to examine feature visualizations from the model with randomized weights to confirm this for themselves. We conclude that the learning process for the model creates a richer structure in its activations than the distribution of tokens in the dataset alone.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q309",
      "query": "What are the key challenges in scaling sparse autoencoders to larger language models?",
      "answer": "Consider an autoencoder with a 100\u00d7 expansion factor applied to the activations of a single MLP layer of width 10,000: it would have ~20 billion parameters. Additionally, many of these features are likely quite rare, potentially requiring the autoencoder to be trained on a substantial fraction of the large model's training corpus. So it seems plausible that training the autoencoder could become very expensive, potentially even more expensive than the original model. We remain optimistic, however, and there is a silver lining \u2013 it increasingly seems like a large chunk of the mechanistic interpretability agenda will now turn on succeeding at a difficult engineering and scaling problem, which frontier AI labs have significant expertise in.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q310",
      "query": "How do the researchers validate that the extracted features are actually used by the model in meaningful ways?",
      "answer": "We show instead that the learned features have interpretable causal effects on model outputs which make sense in light of the features\u02bc activations. Note that these downstream effects are not inputs to the dictionary learning process, which only sees the activations of the MLP layer. If the resulting features also mediate important downstream behavioral effects then we can be confident that the feature is truly connected to the MLP\u02bcs functional role in the network and not just a property of the underlying data.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q311",
      "query": "What is the 'logit weight similarity' metric and how does it compare to 'activation similarity'?",
      "answer": "A second natural approach is to think of a feature in terms of its downstream effects; two features would be similar in this sense if their activation changes their models' predictions in similar ways. In our one-layer model, a simple approximation to this is the logit weights. This approximation represents each feature as a vector with indices corresponding to vocabulary tokens. We call the correlations between these vectors the logit weight similarity between features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q313",
      "query": "What is the 'attribution similarity' metric and how does it relate to activation and logit weight similarities?",
      "answer": "We want to measure something more like \"the actual effect a feature has on token probabilities.\" One way to get at this would be to compute a vector of ablation effects for every feature on every data point; pairs of features whose ablations hurt the model's predictions on the same tokens must have been predicting the same thing. Unfortunately, this would be rather expensive computationally. Instead, we scale the activation vector of a feature by the logit weights of the tokens that empirically come next in the dataset to produce an attribution vector. Correlations between those vectors provide an attribution similarity that combines both the activity of the feature with the effect it has on the loss. We find that the attribution similarity correlates quite highly with the activation similarity, meaning that features that were coactive between models were useful at predicting the same tokens.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q315",
      "query": "What is the 'importance scoring' technique used in automated interpretability, and why is it necessary?",
      "answer": "Importance scoring is the principled way to correct for the fact that we did not randomly sample all of the examples our model is asked to score. Formally, we assign to all tokens from random examples a weight of 1 and examples from each feature interval a weight of feature_density*interval_probability . feature_density is the fraction of times this feature fires over a large portion of the dataset and interval_probability converts the distribution of non-zero activations into a categorical distribution corresponding to each interval and uses the corresponding probability. We then use these weights in our Spearman correlation as before.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q316",
      "query": "How do the researchers address the issue of polysemanticity in neurons versus features extracted by sparse autoencoders?",
      "answer": "Unfortunately, the most natural computational unit of the neural network \u2013 the neuron itself \u2013 turns out not to be a natural unit for human understanding. This is because many neurons are polysemantic: they respond to mixtures of seemingly unrelated inputs. In the vision model Inception v1, a single neuron responds to faces of cats and fronts of cars . In a small language model we discuss in this paper, a single neuron responds to a mixture of academic citations, English dialogue, HTTP requests, and Korean text. Polysemanticity makes it difficult to reason about the behavior of the network in terms of the activity of individual neurons.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q318",
      "query": "How do the researchers address the challenge of determining the optimal number of features to extract using dictionary learning?",
      "answer": "If this picture is true, it would be important for a number of reasons. It suggests that determining the \"correct number of features\" for dictionary learning is less important than it might initially seem. It also suggests that dictionary learning with fewer features can provide a \"summary\" of model features, which might be very important in studying large models. Additionally, it would explain some of the stranger features we observe in the process of dictionary learning, suggesting that these are either \"collapsed\" features which would make sense if split further (see \"Bug\" 1: Single Token Features), or else highly-specific \"split\" features which do in fact make sense if analyzed closely (see \"Bug\" 2: Multiple Features for a Single Context). Finally, it suggests that our basic theory of superposition in toy models is missing an important dimension of the problem by not adequately studying highly correlated and \"action sharing\" features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q319",
      "query": "What are the advantages of using bidirectional representations in language models?",
      "answer": "Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q320",
      "query": "How does fine-tuning compare to feature-based approaches in NLP tasks?",
      "answer": "There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q322",
      "query": "How does the next sentence prediction task contribute to language understanding?",
      "answer": "Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q324",
      "query": "How do WordPiece embeddings contribute to handling various downstream tasks?",
      "answer": "We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q325",
      "query": "What factors contribute to the effectiveness of transfer learning in NLP tasks?",
      "answer": "Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architectures. Our major contribution is further generalizing these findings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q327",
      "query": "What are the key differences between feature-based and fine-tuning approaches in language representation learning?",
      "answer": "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q329",
      "query": "How does the choice of pre-training corpus affect the performance of language models?",
      "answer": "For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q332",
      "query": "What strategies are used to mitigate the mismatch between pre-training and fine-tuning in masked language models?",
      "answer": "To mitigate this, we do not always replace \"masked\" words with the actual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q333",
      "query": "How does the model size affect the performance on different NLP tasks?",
      "answer": "We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q336",
      "query": "What is the impact of bidirectional context on token-level tasks?",
      "answer": "For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right-side context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the pre-trained bidirectional models.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q337",
      "query": "How does the model handle long-range dependencies in text?",
      "answer": "BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q340",
      "query": "How do attention-based models compare to RNNs in terms of parallelization?",
      "answer": "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t\u22121 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q341",
      "query": "What are the limitations of convolutional neural networks in sequence modeling?",
      "answer": "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q342",
      "query": "How does self-attention contribute to the effectiveness of sequence transduction models?",
      "answer": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q344",
      "query": "How does scaled dot-product attention work in neural networks?",
      "answer": "We compute the dot products of the query with all keys, divide each by \u221ad k , and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention(Q, K, V ) = softmax( QK T \u221ad k )V",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q345",
      "query": "What is the purpose of multi-head attention in neural network architectures?",
      "answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q346",
      "query": "How do position-wise feed-forward networks contribute to the overall architecture of sequence transduction models?",
      "answer": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q347",
      "query": "Why is positional encoding necessary in attention-based models?",
      "answer": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add 'positional encodings' to the input embeddings at the bottoms of the encoder and decoder stacks.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q348",
      "query": "What are the advantages of using sinusoidal functions for positional encoding?",
      "answer": "We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, P E pos+k can be represented as a linear function of P E pos . We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results. We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q350",
      "query": "What is the significance of the warmup_steps parameter in the learning rate schedule?",
      "answer": "We varied the learning rate over the course of training, according to the formula: lrate = d \u22120.5 model \u00b7 min(step_num \u22120.5 , step_num \u00b7 warmup_steps \u22121.5 ) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q352",
      "query": "What are the key differences between additive attention and dot-product attention?",
      "answer": "Dot-product attention is identical to our algorithm, except for the scaling factor of 1 \u221a d k . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q353",
      "query": "How does the transformer model handle long-range dependencies in sequences?",
      "answer": "One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q355",
      "query": "How does the transformer model perform on English constituency parsing tasks?",
      "answer": "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q358",
      "query": "What is the impact of varying the number of attention heads on model performance?",
      "answer": "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q359",
      "query": "What is the relationship between model size and the number of training tokens for optimal performance in language models?",
      "answer": "We find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q360",
      "query": "How does increasing the amount of training data affect the performance of large language models?",
      "answer": "Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q361",
      "query": "What factors should be considered when allocating a computational budget for training language models?",
      "answer": "Since it is typically only feasible to train these large models once, accurately estimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021).",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q362",
      "query": "How has the trend in large language model training evolved in recent years?",
      "answer": "The trend so far in large language model training has been to increase the model size, often without increasing the number of training tokens. The largest dense transformer, MT-NLG 530B, is now over 3 larger than GPT-3's 170 billion parameters from just two years ago.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q363",
      "query": "What is the importance of dataset quality in scaling language models?",
      "answer": "Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q364",
      "query": "How does the Massive Multitask Language Understanding (MMLU) benchmark evaluate language models?",
      "answer": "The Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2020) consists of a range of exam-like questions on academic subjects.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q365",
      "query": "What are some ethical concerns associated with training large language models on vast amounts of data?",
      "answer": "Larger datasets will require extra care to ensure train-test set overlap is properly accounted for, both in the language modelling loss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical and privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and private information.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q368",
      "query": "How does the choice of optimizer affect language model performance?",
      "answer": "We use AdamW (Loshchilov and Hutter, 2019) for Chinchilla rather than Adam (Kingma and Ba, 2014) as this improves the language modelling loss and the downstream task performance after finetuning.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q371",
      "query": "What is the significance of the TruthfulQA benchmark in evaluating language models?",
      "answer": "On TruthfulQA (Lin et al., 2021), Chinchilla reaches 43.6%, 58.5%, and 66.7% accuracy with 0-shot, 5-shot, and 10-shot respectively. In comparison, Gopher achieved only 29.5% 0-shot and 43.7% 10-shot accuracy. In stark contrast with the findings of Lin et al. (2021), the large improvements (14.1% in 0-shot accuracy) achieved by Chinchilla suggest that better modelling of the pre-training data alone can lead to substantial improvements on this benchmark.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q372",
      "query": "How do language models perform on closed-book question answering tasks?",
      "answer": "On the Natural Questions dataset (Kwiatkowski et al., 2019), Chinchilla achieves new closed-book SOTA accuracies: 31.5% 5-shot and 35.5% 64-shot, compared to 21% and 28% respectively, for Gopher. On TriviaQA (Joshi et al., 2017) we show results for both the filtered (previously used in retrieval and open-book work) and unfiltered set (previously used in large language model evaluations). In both cases, Chinchilla substantially out performs Gopher.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q373",
      "query": "What are the implications of using a more compute-optimal model for downstream tasks?",
      "answer": "The energy cost of a large language model is amortized through its usage for inference an fine-tuning. The benefits of a more optimally trained smaller model, therefore, extend beyond the immediate benefits of its improved performance.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q377",
      "query": "What are some key considerations when scaling language models to larger sizes and datasets?",
      "answer": "While there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed. Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q378",
      "query": "How does the computational efficiency of language models impact their practical applications?",
      "answer": "While pre-training a large language model has a considerable compute cost, downstream fine-tuning and inference also make up substantial compute usage (Rae et al., 2021). Due to being 4 smaller than Gopher, both the memory footprint and inference cost of Chinchilla are also smaller.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q379",
      "query": "How can the pursuit of a single goal lead to the development of multiple abilities associated with intelligence?",
      "answer": "In such environments, any behaviour that maximises reward must necessarily exhibit those abilities. In this sense, the generic objective of reward maximisation contains within it many or possibly even all the goals of intelligence.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q380",
      "query": "What are the two levels of explanation for diverse forms of intelligence found in nature?",
      "answer": "Reward thus provides two levels of explanation for the bountiful expressions of intelligence found in nature. First, different forms of intelligence may arise from the maximisation of different reward signals in different environments, resulting for example in abilities as distinct as echolocation in bats, communication by whale-song, or tool use in chimpanzees. Second, the intelligence of even a single animal or human is associated with a cornucopia of abilities. According to our hypothesis, all of these abilities subserve a singular goal of maximising that animal or agent's reward within its environment.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q381",
      "query": "How might a kitchen robot develop various abilities through the pursuit of a single goal?",
      "answer": "In order for a kitchen robot to maximise cleanliness, it must presumably have abilities of perception (to differentiate clean and dirty utensils), knowledge (to understand utensils), motor control (to manipulate utensils), memory (to recall locations of utensils), language (to predict future mess from dialogue), and social intelligence (to encourage young children to make less mess). A behaviour that maximises cleanliness must therefore yield all these abilities in service of that singular goal",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q384",
      "query": "How does the reinforcement learning problem represent goals?",
      "answer": "The reinforcement learning problem represents goals by cumulative rewards. A reward is a special scalar observation R t , emitted at every time-step t by a reward signal in the environment, that provides an instantaneous measurement of progress towards a goal. An instance of the reinforcement learning problem is defined by an environment \u03b5 with a reward signal, and by a cumulative objective to maximise, such as a sum of rewards over a finite number of steps, a discounted sum, or the average reward per time-step.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q385",
      "query": "What are the limitations of the agent system in the reinforcement learning problem?",
      "answer": "The agent system \u03b1 is limited by practical constraints to a bounded set. The agent has limited capacity determined by its machinery (for example, limited memory in a computer or limited neurons in a brain). The agent and environment systems execute in real-time. While the agent spends time computing its next action (e.g. producing no-op actions while deciding whether to run away from a lion), the environment system continues to process (e.g. the lion attacks).",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q386",
      "query": "How does the definition of environment in reinforcement learning encompass various problem dimensions?",
      "answer": "Note that this definition of environment is very broad and encompasses many problem dimensions, including those in Table 1.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q390",
      "query": "How does the reward maximization approach to social intelligence differ from traditional game theory approaches?",
      "answer": "According to our hypothesis, social intelligence may instead be understood as, and implemented by, maximising cumulative reward from the point of view of one agent in an environment that contains other agents. Following this standard agent-environment protocol, one agent observes the behaviour of other agents, and may affect other agents through its actions, just as it observes and affects any other aspect of its environment. An agent that can anticipate and influence the behaviour of other agents can typically achieve greater cumulative reward. Thus, if an environment needs social intelligence (e.g. because it contains animals or humans), reward maximisation will produce social intelligence.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q399",
      "query": "What are some challenges in understanding the internal representations of large language models?",
      "answer": "Superficial Limitations. In our work, we perform dictionary learning over activations sampled from a text-only dataset similar to parts of our pretraining distribution. It did not include any \"Human:\" / \"Assistant:\" formatted data that we finetune Claude to operate on, and did not include any images. In the future, we'd like to include data more representative of the distribution Claude is finetuned to operate on. On the other hand, the fact that this method works when trained on such a different distribution (including zero-shot generalization to images) seems like a positive sign.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q400",
      "query": "How does the issue of cross-layer superposition affect the analysis of neural networks?",
      "answer": "Cross-Layer Superposition. We believe that many features in large models are in \"cross-layer superposition\". That is, gradient descent often doesn't really care exactly which layer a feature is implemented in or even if it is isolated to a specific layer, allowing for features to be \"smeared\" across layers. This is a big challenge for dictionary learning, and we don\u02bct yet know how to solve it. This work tries to partially sidestep it by focusing on the residual stream which, as the sum of the outputs of all previous layers, we expect to suffer less from cross-layer superposition.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q403",
      "query": "How do sparse autoencoders compare to neurons in terms of interpretability?",
      "answer": "To quantify this difference, we first compared the interpretability of 100 randomly chosen features versus that of 100 randomly chosen neurons. We did this with the same automated interpretability approach outlined in Towards Monosemanticity , but using Claude 3 Opus to provide explanations of features and predict their held out activations. We find that activations of a random selection of SAE features are significantly more interpretable on average than a random selection of MLP neurons.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q404",
      "query": "What is the significance of finding safety-relevant features in language models?",
      "answer": "In the long run, we hope that having access to features like these can be helpful for analyzing and ensuring the safety of models. For example, we might hope to reliably know whether a model is being deceptive or lying to us. Or we might hope to ensure that certain categories of very harmful behavior (e.g. helping to create bioweapons) can reliably be detected and stopped.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q405",
      "query": "How can feature steering be used to influence model behavior?",
      "answer": "Feature steering can be used to modify the model\u02bcs demeanor, preferences, stated goals, and biases; to induce it to make specific errors; and to circumvent model safeguards (see also Safety-Relevant Features). We find this compelling evidence that our interpretations of features line up with how they are used by the model.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q406",
      "query": "What are some examples of features that represent specific functions in code?",
      "answer": "We also discovered features that track specific function definitions and references to them in code. A particularly interesting example is an addition feature , which activates on names of functions that add numbers. For example, this feature fires on \"bar\" when it is defined to perform addition, but not when it is defined to perform multiplication. Moreover, it fires at the end of any function definition that implements addition.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q407",
      "query": "How do sparse autoencoders help in understanding the intermediate computations of language models?",
      "answer": "Another potential application of features is that they let us examine the intermediate computation that the model uses to produce an output. As a proof of concept, we observe that in prompts where intermediate computation is required, we find active features corresponding to some of the expected intermediate results.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q409",
      "query": "How do sparse autoencoders help in identifying safety-relevant features in language models?",
      "answer": "We find a diversity of highly abstract features. They both respond to and behaviorally cause abstract behaviors. Examples of features we find include features for famous people, features for countries and cities, and features tracking type signatures in code. Many features are multilingual (responding to the same concept across languages) and multimodal (responding to the same concept in both text and images), as well as encompassing both abstract and concrete instantiations of the same idea (such as code with security vulnerabilities, and abstract discussion of security vulnerabilities).",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q410",
      "query": "What are some challenges in scaling feature extraction to large language models?",
      "answer": "Getting All the Features and Compute. We do not believe we have found anywhere near \"all the features\" that exist in Sonnet, even if we restrict ourselves to the middle layer we focused on. We don't have an estimate of how many features there are or how we'd know we got all of them (if that's even the right frame!). We think it's quite likely that we're orders of magnitude short, and that if we wanted to get all the features \u2013 in all layers! \u2013 we would need to use much more compute than the total compute needed to train the underlying models. This won't be tenable: as a field, we must find significantly more efficient algorithms.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q411",
      "query": "How does the analysis of feature neighborhoods contribute to understanding language model representations?",
      "answer": "We find that this consistently surfaces features that share a related meaning or context \u2014 the interactive feature UMAP has additional neighborhoods to explore. Focusing on a small neighborhood around the Golden Gate Bridge feature , we find that there are features corresponding to particular locations in San Francisco such as Alcatraz and the Presidio. More distantly, we also see features with decreasing degrees of relatedness, such as features related to Lake Tahoe, Yosemite National Park, and Solano County (which is near San Francisco). At greater distances, we also see features related in more abstract ways, like features corresponding to tourist attractions in other regions (e.g. \"M\u00e9doc wine region, France\"; \"Isle of Skye, Scotland\"). Overall, it appears that distance in decoder space maps roughly onto relatedness in concept space, often in interesting and unexpected ways.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q412",
      "query": "What are some methods for searching for specific features in large language models?",
      "answer": "Single prompts - Our primary strategy was to use targeted prompts. In some cases, we simply supplied a single prompt that relates to the concept of interest and inspected the features that activate most strongly for specific tokens in that prompt. Prompt combinations - Often the top-activating features on a prompt are related to syntax, punctuation, specific words, or other details of the prompt unrelated to the concept of interest. In such cases, we found it useful to select for features using sets of prompts, filtering for features active for all the prompts in the set. We often included complementary \"negative\" prompts and filtered for features that were also not active for those prompts. Geometric methods - We uncovered some interesting features by exploiting the geometry of the feature vectors of the SAE \u2013 for instance, by inspecting the \"nearest neighbor\" features that have high cosine similarity with other features of interest.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q416",
      "query": "What are some potential implications of finding safety-relevant features in language models?",
      "answer": "Some of the features we find are of particular interest because they may be safety-relevant \u2013 that is, they are plausibly connected to a range of ways in which modern AI systems may cause harm. In particular, we find features related to security vulnerabilities and backdoors in code; bias (including both overt slurs, and more subtle biases); lying, deception, and power-seeking (including treacherous turns); sycophancy; and dangerous / criminal content (e.g., producing bioweapons). However, we caution not to read too much into the mere existence of such features: there's a difference (for example) between knowing about lies, being capable of lying, and actually lying in the real world. This research is also very preliminary. Further work will be needed to understand the implications of these potentially safety-relevant features.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    }
  ],
  "test": [
    {
      "id": "q331",
      "query": "How does the model handle different types of NLP tasks during fine-tuning?",
      "answer": "For each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classification or sequence tagging. At the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q222",
      "query": "What capabilities are required for a language model to complete a simple sentence about hunger?",
      "answer": "To complete this sentence in a sensible way, the language model needs to know that hunger is a state that motivates people to seek food, and that food is a category of things that can satisfy hunger. It also needs to choose a word that fits the syntactic and semantic constraints of the sentence (such as \"a snack\"), and that is plausible given the situation and the background knowledge.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q229",
      "query": "What insights about model capabilities can be drawn from the GPT-Eval scores?",
      "answer": "Figure 3 suggests that shallower models perform better in terms of grammar compared to content consistency, meaning that model depth is more important for keeping consistent with the content than for generating syntactically correct language (we provide additional evidence for this in the next section).",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q417",
      "query": "How do sparse autoencoders help in understanding the relationship between model size and feature representation?",
      "answer": "We also find evidence of feature splitting , a phenomenon in which features in smaller SAEs \"split\" into multiple features in a larger SAE, which are geometrically close and semantically related to the original feature, but represent more specific concepts. For instance, a \"San Francisco\" feature in the 1M SAE splits into two features in the 4M SAE and eleven fine-grained features in the 34M SAE. In addition to feature splitting, we also see examples in which larger SAEs contain features that represent concepts not captured by features in smaller SAEs. For instance, there is a group of earthquake features from the 4M and 34M SAEs that has no analog in this neighborhood in the 1M SAE, nor do any of the nearest 1M SAE features seem related.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q369",
      "query": "What are some challenges in evaluating the toxicity of language models?",
      "answer": "While toxicity is an umbrella term, and its evaluation in LMs comes with challenges (Welbl et al., 2021; Xu et al., 2021), automatic classifier scores can provide an indication for the levels of harmful text that a LM generates.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q240",
      "query": "How does the paper define 'Cramming' in the context of language model training?",
      "answer": "To answer these questions, we consider a challenge we call \"Cramming\" \u2013 learning a whole language model the day before the test.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q093",
      "query": "How do emergent features evolve in transformers of different sizes?",
      "answer": "Emergent outlier features are present in even very small transformers (125M parameters), and they do start out in the attention projection layers (key/query/value). Feature outliers are \"consumed\" in the attention function (softmax) and the second fully connected sublayer (contraction layer). The outlier features are likely consumed in these layers since the second feedforward network (FFN) sub-layer, and the softmax have non-linear functions that can easily squash features to zero.\n\nOnce you scale transformers a bit more (350M to 1.3B), outliers also occur in the FFN and attention output layers. At this scale, some successive attention layers and FFN layers use the same dimension to coordinate what features to remove. This has synergy. The attention layer is good at context-dependent selection and pattern matching, while the FFN layers are good at globally, context-independent pattern matching.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q154",
      "query": "What is the effect of binary quantization on retrieval performance for different embedding models?",
      "answer": "Interestingly, we can also see that all-MiniLM-L6-v2 exhibits stronger performance on binary than on int8 quantization. A possible explanation for this could be the selection of calibration data. On e5-base-v2, we observe the effect of dimension collapse, which causes the model to only use a subspace of the latent space; when performing the quantization, the whole space collapses further, leading to high performance losses.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q413",
      "query": "How do sparse autoencoders contribute to understanding the computational processes in language models?",
      "answer": "To continue this text, the model must parse the quote from John, identify his state of mind, and then translate that into a likely feeling. If we sort features by either their attribution or their ablation effect on the completion \"sad\" (with respect to a baseline completion of \"happy\"), the top two features are: \u2013 This feature fires when someone expresses a need or desire to be alone or have personal time and space, as in \"she would probably want some time to herself\". This is active from the word \"alone\" onwards. This suggests the model has gotten the gist of John's expression. \u2013 This feature detects expressions of sadness, crying, grief, and related emotional distress or sorrow, as in \"the inconsolable girl sobs\". This is active on \"John feels\". This suggests the model has inferred what someone who says they are alone might be feeling.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q078",
      "query": "Why is it important to check for development-prod skew in LLM applications?",
      "answer": "A common source of errors in traditional machine learning pipelines is train-serve skew. This happens when the data used in training differs from what the model encounters in production. Although we can use LLMs without training or finetuning, hence there's no training set, a similar issue arises with development-prod data skew. Essentially, the data we test our systems on during development should mirror what the systems will face in production. If not, we might find our production accuracy suffering.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q164",
      "query": "How does Llama 3 handle the trade-off between violation rate and false refusal rate in its safety measures?",
      "answer": "While a low violation rate is desirable, it is critical to consider false refusal as a counter-metric, as a model that always refuses is maximally safe, but not helpful in the slightest. Similarly, a model that always answers every prompt, regardless of how problematic the request, would be overly harmful and toxic. In Figure 21, leveraging our internal benchmarks, we explore how different models and systems in industry navigate this trade off and how Llama 3 compares. We find that our models achieve very competitive violation rate metrics while keeping false refusal rate low as well, indicating a solid balance between helpfulness and safety.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q193",
      "query": "What is the purpose of a prosody model in AI-based speech generation?",
      "answer": "To enhance the naturalness and expressiveness of synthesized speech, we integrate a decoder-only Transformer-based Prosody model (PM) (Radford et al., 2021) that takes the Llama 3 embeddings as an additional input. This integration leverages the linguistic capabilities of Llama 3, utilizing both its textual output and intermediate embeddings at the token rate (Devlin et al., 2018; Dong et al., 2019; Raffel et al., 2020; Guo et al., 2023) to enhance the prediction of prosody features, thus reducing the lookahead required by the model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q049",
      "query": "What is the throughput advantage of BitNet b1.58 70B over LLaMA LLM 70B?",
      "answer": "Table 3 shows that BitNet b1.58 70B can support up to 11 times the batch size of LLaMA LLM, resulting an 8.9 times higher throughput.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q398",
      "query": "How does the concept of bounded rationality relate to the reward-is-enough hypothesis?",
      "answer": "Bounded or computational rationality suggests that agents should select the program that best achieves their goals, given the real-time consequences arising from that program (for example, how long the program takes to execute), and subject to limitations on the set of programs (for example, limiting the maximum size of the program). Our contribution builds upon these viewpoints, but focuses on the question of whether a single, simple reward-based goal could provide a common basis for all abilities associated with intelligence.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q392",
      "query": "How does the reward maximization approach address the challenge of integrating different linguistic abilities?",
      "answer": "According to our hypothesis, the ability of language in its full richness, including all of these broader abilities, arises from the pursuit of reward. It is an instance of an agent's ability to produce complex sequences of actions (e.g. uttering sentences) based on complex sequences of observations (e.g. receiving sentences) in order to influence other agents in the environment (cf. discussion of social intelligence above) and accumulate greater reward.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q181",
      "query": "What is the purpose of annealing in language model training?",
      "answer": "During pre-training on the final 40M tokens, we linearly annealed the learning rate to 0, maintaining a context length of 128K tokens. During this annealing phase, we also adjusted the data mix to upsample data sources of very high quality; see Section 3.1.3. Finally, we compute the average of model checkpoints (Polyak (1991) averaging) during annealing to produce the final pre-trained model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q397",
      "query": "How does the reward-is-enough hypothesis address the challenge of sample efficiency in learning complex behaviors?",
      "answer": "We do not offer any theoretical guarantee on the sample efficiency of reinforcement learning agents. Indeed, the rate at and degree to which abilities emerge will depend upon the specific environment, learning algorithm, and inductive biases; furthermore one may construct artificial environments in which learning will fail. Instead, we conjecture that powerful reinforcement learning agents, when placed in complex environments, will in practice give rise to sophisticated expressions of intelligence.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q387",
      "query": "Why might an environment demand learned knowledge in reinforcement learning?",
      "answer": "An environment may also demand learned knowledge. This will occur when future experience is uncertain, due to unknown elements, stochasticity, or complexity of the environment. This uncertainty results in a vast array of potential knowledge that the agent may need, depending on its particular realisation of future events. In rich environments, including many of the hardest and most important problems in natural and artificial intelligence, the total space of potential knowledge is far larger than the capacity of the agent.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q330",
      "query": "What are the advantages of using self-attention mechanisms in language processing tasks?",
      "answer": "Fine-tuning is straightforward since the self-attention mechanism in the Transformer allows BERT to model many downstream tasks\u2014whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q107",
      "query": "How many completions are typically sampled for each ARC-AGI problem in a high-performing approach?",
      "answer": "Sample vast, vast numbers of completions (~5,000 per problem) from GPT-4o.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q262",
      "query": "How can data filtering techniques improve language model training?",
      "answer": "We use the tokenizer itself to remove all training sequences from C4 set that cannot be compressed well; we simply set a threshold t, e.g. t = 0.3, and drop all entries from the dataset where the number of tokens in the entry is larger than t times the number of raw characters. This removes, for example, sequences consisting of hard-to-compress HTML or markdown code. Surprisingly, this results in a measurable improvement on C4, summarized in Table 2.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q132",
      "query": "What is the 'direct path term' in transformer models and what does it represent?",
      "answer": "The direct path term, Id \u2297 W W , also occurred when we looked at the zero-layer transformer. Because it doesn't move information between positions (that's what Id denotes!), the only thing it can contribute to is the bigram statistics, and it will fill in missing gaps that other terms don't handle there.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q102",
      "query": "What was the previous state-of-the-art accuracy on ARC-AGI before 2024?",
      "answer": "The prior state of the art on a similarly difficult dataset was 34% accuracy, so this is a significant improvement.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q317",
      "query": "What is the 'feature density' metric and how is it used to evaluate the quality of extracted features?",
      "answer": "While iterating on different techniques, one important proxy for autoencoder performance is feature density. Each feature in our autoencoder only activates on a very small percentage of the total tokens in the training set. We define the feature density of each feature as the fraction of tokens on which the feature has a nonzero value. We hypothesize that language models contain a large number of features across a distribution of feature densities, and that lower-density features are harder for our autoencoder to discover because they appear less often in the training dataset. Using large training datasets was an attempt to recover such low-density features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q226",
      "query": "What method is used to increase the diversity of the TinyStories dataset?",
      "answer": "In order to address the problem of creating a diverse dataset, we collected a vocabulary consisting of about 1500 basic words, which try to mimic the vocabulary of a typical 3-4 year-old child, separated into nouns, verbs, and adjectives. In each generation, 3 words are chosen randomly (one verb, one noun, and one adjective). The model is instructed to generate a story that somehow combines these random words into the story. As we argue below, this greatly increases the diversity of the dataset, forcing the stories to span the entire vocabulary a child is familiar with, and to include a rich set of ways to combine different concepts.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q394",
      "query": "How does the reward maximization approach address the challenge of observational learning in complex environments?",
      "answer": "We speculate that these broader abilities of observation learning could be driven by the maximisation of reward, from the perspective of a single agent that simply observes other agents as integral parts of its environment, potentially leading to many of the same benefits as behavioural cloning \u2013 such as the sample-efficient acquisition of knowledge \u2013 but in a much wider and more integrated context.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q246",
      "query": "How does the paper handle tokenization for the cramming experiment?",
      "answer": "We force all text into lower-case, strip accents and non-ascii characters and create an English tokenizer from scratch based only on this data. We choose WordPiece with a vocabulary size of 2^15 = 32768 (Wu et al., 2016). We found no significant change in performance with BPE (Sennrich et al., 2016) or SentencePiece with Unigrams (Kudo, 2018; Kudo & Richardson, 2019). Smaller vocabulary sizes (2^12, 2^13, 2^14) resulted in worse performance, while larger vocabulary sizes (2^16) we not reliably better.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q155",
      "query": "How does the rescore multiplier affect the performance retention of int8 quantization?",
      "answer": "As we can see from the diagram, a higher rescore multiplier implies better retention of performance after quantization. Extrapolating from our results, we assume the relation is likely hyperbolical with performance approaching 100% as the rescore multiplier continues to rise. A rescore multiplier of 4-5 already leads to a remarkable performance retention of 99% using int8.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q018",
      "query": "How does the behavior of neural networks change when dealing with out-of-distribution inputs?",
      "answer": "It is widely recognized that Neural Networks (NNs) are susceptible to unpredictable behaviors when dealing with O.O.D inputs (Liu et al., 2021; Shen et al., 2021; Bai et al., 2021; Zhang et al., 2023).",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q186",
      "query": "What is the purpose of a speech adapter in multimodal AI models?",
      "answer": "The speech adapter contains about 100M parameters. It is composed of a convolution layer, a rotary Transformer layer, and a linear layer. The convolution layer has a kernel size of 3 and a stride of 2, which is designed to reduce the speech frame length to 80ms. This allows the model to provide more coarse-grained features to the language model. The Transformer layer has a latent dimension of 3072 and a feed-forward network with a dimension of 4096 which further processes the information from speech with context after the convolutional downsampling. Finally, the linear layer maps the output dimension to match that of the language-model embedding layer.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q253",
      "query": "How is dropout handled in the cramming experiment?",
      "answer": "In the cramming setting, training data is large compared to compute. Overfitting is not possible due to the single epoch schedule, and we disable dropout during pretraining (Brown et al., 2020) to maximize the number of parameter updates. We re-enable dropout during downstream fine-tuning with a dropout value of 0.1.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q227",
      "query": "What types of instructions are included in the TinyStories-Instruct dataset?",
      "answer": "The instructions are of four types:\n1. A list of words to be included in the story.\n2. A sentence that should appear somewhere in the story.\n3. A list of features (possible features: dialogue, bad ending, moral value, plot twist, foreshadowing, conflict).\n4. A short summary (1-2 lines) of the story.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q251",
      "query": "How is the learning rate scheduled in the cramming experiment?",
      "answer": "Following the advice of Izsak et al. (2021), we re-scale the learning rate schedule so that it is tied to our budget and the learning rate decays as the budget reduces to zero. Interestingly, we observe in Figure 2 that while globally a large number of learning rate shapes lead to similar reductions in loss, we find that we can make some gains through the choice of schedule. We find that a simple one-cycle learning rate (Smith & Topin, 2018) with a peak learning rate of 10^-3 leads to minimal pretraining loss within our budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q179",
      "query": "What are the key factors in developing high-quality foundation models?",
      "answer": "We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process:",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q105",
      "query": "How does the approach used in a high-performing ARC-AGI solution compare to AlphaCode?",
      "answer": "My approach is similar in spirit to the approach applied in AlphaCode in which a model generates millions of completions attempting to solve a programming problem and then aggregates over them to determine what to submit.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q250",
      "query": "What optimization algorithm is used in the cramming experiment?",
      "answer": "We keep Adam (Kingma & Ba, 2015) as the optimizer of choice, with weight decay of 0.01 as described in (Loshchilov & Hutter, 2017), \u03b2_1 = 0.9, \u03b2_2 = 0.98 and \u03b5 = 10^-12. To stablize training at no extra cost, we include gradient clipping at a clip value of 0.5.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q323",
      "query": "What are the advantages of using a larger model in language representation learning?",
      "answer": "We conclude that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks. It is also perhaps surprising that we are able to achieve such significant improvements on top of models which are already quite large relative to the existing literature.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q065",
      "query": "Why is it important to have small, focused prompts for LLMs?",
      "answer": "Just like how we strive (read: struggle) to keep our systems and code simple, so should we for our prompts. Instead of having a single, catch-all prompt for the meeting transcript summarizer, we can break it into steps: - Extract key decisions, action items, and owners into structured format - Check extracted details against the original transcription for consistency - Generate a concise summary from the structured details",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q074",
      "query": "What are some effective strategies for evaluating LLMs?",
      "answer": "Create unit tests (i.e., assertions) consisting of samples of inputs and outputs from production, with expectations for outputs based on at least three criteria. While three criteria might seem arbitrary, it's a practical number to start with; fewer might indicate that your task isn't sufficiently defined or is too open-ended, like a general-purpose chatbot. These unit tests, or assertions, should be triggered by any changes to the pipeline, whether it's editing a prompt, adding new context via RAG, or other modifications.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q370",
      "query": "How does the performance of language models on common sense tasks compare to human performance?",
      "answer": "We evaluate Chinchilla on various common sense benchmarks: PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), Winogrande (Sakaguchi et al., 2020), HellaSwag (Zellers et al., 2019), and BoolQ (Clark et al., 2019). We find that Chinchilla outperforms both Gopher and GPT-3 on all tasks and outperforms MT-NLG 530B on all but one task\u2014see Table 8.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q382",
      "query": "What advantages does a singular goal provide in understanding various abilities associated with intelligence?",
      "answer": "When abilities associated with intelligence arise as solutions to a singular goal of reward maximisation, this may in fact provide a deeper understanding since it explains why such an ability arises (e.g. that classification of crocodiles is important to avoid being eaten). In contrast, when each ability is understood as the solution to its own specialised goal, the why question is side-stepped in order to focus upon what that ability does (e.g. discriminating crocodiles from logs). Furthermore, a singular goal may also provide a broader understanding of each ability that may include characteristics that are otherwise hard to formalise, such as dealing with irrational agents in social intelligence (e.g. pacifying an angry aggressor), grounding language to perceptual experience (e.g. dialogue regarding the best way to peel a fruit), or understanding haptics in perception (e.g. picking a sharp object from a pocket). Finally, implementing abilities in service of a singular goal, rather than for their own specialised goals, also answers the question of how to integrate abilities, which otherwise remains an outstanding issue.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q388",
      "query": "How does the reward maximization approach to perception differ from traditional supervised learning approaches?",
      "answer": "As per our hypothesis, we suggest that perception may instead be understood as subserving the maximisation of reward. For example, the perceptual abilities listed above may arise implicitly in service of maximising healthy food, avoiding accidents, or minimising pain. Indeed, perception in some animals has been shown to be consistent with reward maximisation. Considering perception from the perspective of reward maximisation rather than supervised learning may ultimately support a greater range of perceptual behaviours, including challenging and realistic forms of perceptual abilities",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q212",
      "query": "How does the loss per token change based on its position in the context for language models?",
      "answer": "Fixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see Figure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12, LT16], or a more general feature of the model architecture and optimization.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q239",
      "query": "What is the main goal of the 'Cramming' experiment described in the paper?",
      "answer": "Our goal is to turn this trend on its head and investigate how to best scale down language model training and what trade-offs emerge when doing so: What downstream performance can be achieved by a modest researcher when training from scratch with a single GPU for a single day?",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q218",
      "query": "How does the performance of language models trained on short contexts compare to those trained on longer contexts?",
      "answer": "Even modestly sized models trained on n_ctx = 8 can dominate our largest n_ctx = 1024 models on very early tokens. This also suggests that further improvements should be possible with much larger models trained on large contexts.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q215",
      "query": "What is the relationship between model depth and generalization to other text distributions?",
      "answer": "We observe no effect of depth on generalization; generalization performance depends primarily on training distribution performance.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q126",
      "query": "What are 'induction heads' in transformer models and how do they function?",
      "answer": "Induction heads search over the context for previous examples of the present token. If they don't find it, they attend to the first token (in our case, a special token placed at the start), and do nothing. But if they do find it, they then look at the next token and copy it. This allows them to repeat previous sequences of tokens, both exactly and approximately.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q321",
      "query": "What is the purpose of the masked language model (MLM) pre-training objective?",
      "answer": "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q136",
      "query": "What are 'bottleneck activations' in transformer models?",
      "answer": "We say that an activation is a bottleneck activation if it is a lower-dimensional intermediate between two higher dimensional activations. For example, the residual stream is a bottleneck activation because it is the only way to pass information between MLP activations, which are typically four times larger than it.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q272",
      "query": "How does AlphaZero train its neural network?",
      "answer": "The parameters \u03b8 of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters \u03b8. Games are played by selecting moves for both players by MCTS, at \u223c \u03c0t. At the end of the game, the terminal position sT is scored according to the rules of the game to compute the game outcome z: \u22121 for a loss, 0 for a draw, and +1 for a win. The neural network parameters \u03b8 are updated so as to minimise the error between the predicted outcome vt and the game outcome z, and to maximise the similarity of the policy vector pt to the search probabilities \u03c0t.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q225",
      "query": "How does the TinyStories dataset address the problem of overwhelming small language models with excessive information?",
      "answer": "This raises the question of whether we can design a dataset that preserves the essential elements of natural language, such as grammar, vocabulary, facts, and reasoning, but that is much smaller and more refined in terms of its breadth and diversity. Such a dataset would allow us to isolate and examine the minimal requirements for a language model to generate coherent and fluent text, and to evaluate its performance and capabilities more precisely and fairly.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q235",
      "query": "What scaling law is observed between model size and learning budget for a fixed amount of training flops?",
      "answer": "Previous works [16, 11] have shown that there is a polynomial scaling law between model size and learning budget for LLMs, i.e., the optimal model size for a given amount of flops is proportional to the flops raised to some power \u03b1 > 1. However, these works used different ranges of model sizes (from a few million to tens of billions of parameters) and found different values of \u03b1 (around 0.7 and 0.5, respectively). A natural question is whether this scaling law is universal or depends on the dataset. Our dataset allows us to conduct a similar experiment but with much smaller models and flops. Surprisingly, we find evidence for a polynomial scaling law as well, which suggests that there might be a universal phenomenon here.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q396",
      "query": "What are the advantages of using reinforcement learning agents for maximizing reward?",
      "answer": "First, among all possible solution methods for maximising reward, surely the most natural approach is to learn to do so from experience, by interacting with the environment. Over time, that interactive experience provides a wealth of information about cause and effect, about the consequences of actions, and about how to accumulate reward. Rather than predetermining the agent's behaviour (placing faith in the designer's foreknowledge of the environment) it is natural instead to bestow the agent with a general ability to discover its own behaviours (placing faith in experience).",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q087",
      "query": "What happens to FFN layers in transformers after the emergence of outlier features?",
      "answer": "FFN layers become more \"dense\". While in computer vision, you can prune about 95% of weights without severe performance degradation, that number is 30% for transformers trained on NLP data. After emergence, this number shrinks to well below 5%. It seems that canceling out features can remove noise that is generated from the many weak features that are activated. Because these are silenced now, each set of neurons can learn much more features that are almost independent of each other due to the masking of context-dependent features.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q082",
      "query": "What is the main challenge with quantizing transformers at scale?",
      "answer": "So it turns out, that transformers have these emergent features that have very large values. They occur in particular hidden dimensions and are active in up to 75% of all sequence dimensions. They occur in all layers (well most layers, but we come to that). So if you have a transformer hidden state X of dimensionality [batch, sequence, hidden], then X[:, :, i] for some i have values that look like this:\n\n[-60.. -45, -51, -35, -20, -67]\n\nWhereas 99.9% of dimensions look like this (normally distributed with one outlier)\n\n[-0.10, -0.23,  0.08, -0.38, -0.28, -0.29, -2.11,  0.34, -0.53, -67.0]",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q121",
      "query": "In the Redwood Research blog post about achieving 50% accuracy on ARC-AGI, what was the main approach used?",
      "answer": "The main idea behind my solution is very simple: get GPT-4o to generate around 8,000 python programs which attempt to implement the transformation, select a program which is right on all the examples (usually there are 3 examples), and then submit the output this function produces when applied to the additional test input(s). I show GPT-4o the problem as images and in various ascii representations.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q033",
      "query": "How do researchers test if spatial and temporal representations in language models are linear?",
      "answer": "To test whether spatial and temporal features are represented linearly, we compare the performance of our linear ridge regression probes with that of substantially more expressive nonlinear MLP probes of the form W2 ReLU(W1 x + b1) + b2 with 256 neurons. Table 2 reports our results and shows that using nonlinear probes results in minimal improvement to R2 for any dataset or model. We take this as strong evidence that space and time are also represented linearly (or at the very least are linearly decodable), despite being continuous.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q004",
      "query": "How does SelfExtend compare to fine-tuning methods for context window extension?",
      "answer": "Notably, SelfExtend modifies only the attention mechanism during inference, eliminating the need for additional fine-tuning.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q338",
      "query": "What are the key differences between BERT and previous language representation models?",
      "answer": "Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q375",
      "query": "What are the potential risks associated with large language models in terms of bias and toxicity?",
      "answer": "Large Language Models carry potential risks such as outputting offensive language, propagating social biases, and leaking private information (Bender et al., 2021; Weidinger et al., 2021). We expect Chinchilla to carry risks similar to Gopher because Chinchilla is trained on the same data, albeit with slightly different relative weights, and because it has a similar architecture.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q304",
      "query": "How do the researchers measure the universality of features across different models?",
      "answer": "To compare features from different models, we need model-independent ways to represent a feature. One natural approach is to think of a feature as a function assigning values to datapoints; two features would be similar in this sense if they take similar values over a diverse set of data. This general approach has been explored by a number of prior papers (e.g. ). In practice, this can be approximated by representing the feature as a vector, with indices corresponding to a fixed set of data points. We call the correlations between these vectors the activation similarity between features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q075",
      "query": "How can LLM-as-Judge be effectively implemented for evaluations?",
      "answer": "Here are some suggestions to get the most out of LLM-as-Judge: - Use pairwise comparisons: Instead of asking the LLM to score a single output on a Likert scale, present it with two options and ask it to select the better one. This tends to lead to more stable results. - Control for position bias: The order of options presented can bias the LLM's decision. To mitigate this, do each pairwise comparison twice, swapping the order of pairs each time. Just be sure to attribute wins to the right option after swapping! - Allow for ties: In some cases, both options may be equally good. Thus, allow the LLM to declare a tie so it doesn't have to arbitrarily pick a winner.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q068",
      "query": "Why might RAG be preferred over finetuning for incorporating new knowledge into LLMs?",
      "answer": "Beyond improved performance, RAG has other practical advantages. First, compared to continuous pretraining or finetuning, it's easier---and cheaper!---to keep retrieval indices up-to-date. Second, if our retrieval indices have problematic documents that contain toxic or biased content, we can easily drop or modify the offending documents. Consider it an andon cord for documents that ask us to add glue to pizza.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q279",
      "query": "How does AlphaZero represent the board state as input to its neural network?",
      "answer": "The input to the neural network is an N \u00d7 N \u00d7 (M T + L) image stack that represents state using a concatenation of T sets of M planes of size N \u00d7 N . Each set of planes represents the board position at a time-step t \u2212 T + 1, ..., t, and is set to zero for time-steps less than 1. The board is oriented to the perspective of the current player. The M feature planes are composed of binary feature planes indicating the presence of the player's pieces, with one plane for each piece type, and a second set of planes indicating the presence of the opponent's pieces.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q153",
      "query": "How does the resource usage of combined binary and scalar quantization compare to normal retrieval?",
      "answer": "Through this approach, we use 5.2GB of memory and 52GB of disk space for the indices. This is considerably less than normal retrieval, requiring 200GB of memory and 200GB of disk space. Especially as you scale up even further, this will result in notable reductions in latency and costs.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q205",
      "query": "What is the ideal batch size for training large language models?",
      "answer": "The ideal batch size for training these models is roughly a power of the loss only, and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million tokens at convergence for the largest models we can train.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q032",
      "query": "What is the relationship between the number of model parameters and the quality of spatial and temporal representations?",
      "answer": "These probing experiments reveal evidence that models build spatial and temporal representations throughout the early layers before plateauing at around the model halfway point with larger models consistently outperforming smaller ones (\u00a7 3.1).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q389",
      "query": "What are some challenges in applying traditional supervised learning approaches to perception in complex environments?",
      "answer": "The distribution of data is typically context-dependent. For example, an Arctic agent may be required to classify ice and polar bears, upon which its rewards depend; while an African agent may need to classify savannah and lions. The diversity of potential data may, in rich environments, vastly exceed the agent's capacity or the quantity of pre-existing data (see Section 3.1) \u2013 requiring that perception is learned from experience.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q401",
      "query": "What are some limitations of using an L1 activation penalty in neural network analysis?",
      "answer": "Shrinkage. We use an L1 activation penalty to encourage sparsity. This approach is well known to have issues with \"shrinkage\", where non-zero activations are systematically underestimated. We believe this significantly harms sparse autoencoder performance, independent of whether we've \"learned all the features\" or how much compute we use. Recently, a number of approaches have been suggested for addressing this . Our group also unsuccessfully explored using a tanh L1 penalty, which we found improved proxy metrics, but made the resulting features less interpretable for unknown reasons.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q273",
      "query": "What are the key differences between AlphaZero and the original AlphaGo Zero algorithm?",
      "answer": "AlphaGo Zero estimates and optimises the probability of winning, assuming binary win/loss outcomes. AlphaZero instead estimates and optimises the expected outcome, taking account of draws or potentially other outcomes. The rules of Go are invariant to rotation and reflection. This fact was exploited in AlphaGo and AlphaGo Zero in two ways. First, training data was augmented by generating 8 symmetries for each position. Second, during MCTS, board positions were transformed using a randomly selected rotation or reflection before being evaluated by the neural network, so that the Monte-Carlo evaluation is averaged over different biases. The rules of chess and shogi are asymmetric, and in general symmetries cannot be assumed. AlphaZero does not augment the training data and does not transform the board position during MCTS.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q395",
      "query": "How does the reward-is-enough hypothesis propose to address the challenge of general intelligence?",
      "answer": "According to our hypothesis, general intelligence can instead be understood as, and implemented by, maximising a singular reward in a single, complex environment. For example, natural intelligence faces a contiguous stream of experience throughout its lifetime, generated from interactions with the natural world. An animal's stream of experience is sufficiently rich and varied that it may demand a flexible ability to achieve a vast variety of subgoals (such as foraging, fighting, or fleeing), in order to succeed in maximising its overall reward (such as hunger or reproduction). Similarly, if an artificial agent's stream of experience is sufficiently rich, then singular goals (such as battery-life or survival) may implicitly require the ability to achieve an equally wide variety of subgoals, and the maximisation of reward should therefore be enough to yield an artificial general intelligence.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q354",
      "query": "What regularization techniques are commonly used in training large-scale neural machine translation models?",
      "answer": "We employ three types of regularization during training: Residual Dropout We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0.1.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q300",
      "query": "How does the sparse autoencoder architecture differ from a standard autoencoder in the context of dictionary learning?",
      "answer": "We briefly overview the architecture and training of our sparse autoencoder here, and provide further details in Basic Autoencoder Training. Our sparse autoencoder is a model with a bias at the input, a linear layer with bias and ReLU for the encoder, and then another linear layer and bias for the decoder. In toy models we found that the bias terms were quite important to the autoencoder\u02bcs performance.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q408",
      "query": "What is the significance of features generalizing to image activations in language models?",
      "answer": "Generalization to Image Activations. Our SAE features were trained purely on text activations. Image activations are in some sense dramatically off-distribution for the SAE, and yet it successfully generalizes to them.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q402",
      "query": "What is the relationship between the frequency of concepts in training data and their representation in neural networks?",
      "answer": "We quantified this relationship for four different categories of concepts \u2013 elements, cities, animals and foods (fruits and vegetables) \u2013 using 100\u2013200 concepts in each category. We focused on concepts that could be unambiguously expressed by a single word (i.e. that word has few other common meanings) and with a wide distribution of frequencies in text data. We found a consistent tendency for the larger SAEs to have features for concepts that are rarer in the training data, with the rough \"threshold\" frequency required for a feature to be present being similar across categories.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q013",
      "query": "What are some challenges in evaluating long context capabilities of language models?",
      "answer": "Additionally, evaluation methodologies for assessing long context abilities remain open research questions. Standard practices have yet to emerge, complicating experimental results.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q035",
      "query": "How do researchers identify individual neurons responsible for encoding spatial and temporal information?",
      "answer": "To address this, we search for individual neurons with input or output weights that have high cosine similarity with the learned probe direction. That is, we search for neurons which read from or write to a direction similar to the one learned by the probe.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q133",
      "query": "How do transformer models handle 'pointer arithmetic' with positional embeddings?",
      "answer": "Transformers can do \"pointer arithmetic\" type operations on positional embeddings. We observed at least one case in GPT-2 where an induction head was implemented using this approach, instead of the one described above. First, an attention head attended to a similar token, returning its positional embedding. Next, a query vector for another attention head was constructed with q-composition, rotating the positional embedding forward one token. This results in an induction head.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q083",
      "query": "How does vector-wise quantization work for matrix multiplication?",
      "answer": "We can see a matrix multiplication as a sequence of independent inner products between row vectors of A and column vectors of B. We can have a separate constant for each of these vectors. Denormalization happens by multiplying these two constants together for a particular element. No other computation is needed. This is vector-wise quantization.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q092",
      "query": "What is the mechanism for removing features in transformers?",
      "answer": "How do you remove features? You have a single dimension with very large positive or negative values, and you multiply that dimension with a positive/negative number.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q339",
      "query": "What is the main advantage of using attention mechanisms in sequence transduction models?",
      "answer": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q055",
      "query": "What dataset was used to pre-train and compare BitNet b1.58 to LLaMA LLM?",
      "answer": "To ensure a fair comparison, we pre-trained the models on the RedPajama dataset [Com23] for 100 billion tokens.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q189",
      "query": "How can model heterogeneity be addressed in large-scale AI training?",
      "answer": "The model computation is heterogeneous because more computation is performed on some tokens than on others. In particular, image tokens are processed by the image encoder and the cross-attention layers, whereas text tokens are only processed by the language backbone. This heterogeneity leads to bottlenecks in the scheduling of pipeline parallelism. We address this problem by ensuring each pipeline stage contains five layers: namely, four self-attention layers in the language backbone and a cross-attention layer. (Recall that we introduce a cross-attention layer after every fourth self-attention layer.) In addition, we replicate the image encoder on all pipeline stages. Because we train on paired image-text data, this enables us to perform load balancing between the image and text parts of the computation.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q160",
      "query": "How many tokens was Llama 3 pre-trained on compared to Llama 2?",
      "answer": "We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q290",
      "query": "How can deep reinforcement learning be applied to board games?",
      "answer": "The parameters \u03b8 of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters \u03b8. Games are played by selecting moves for both players by MCTS, at \u223c \u03c0t. At the end of the game, the terminal position sT is scored according to the rules of the game to compute the game outcome z: \u22121 for a loss, 0 for a draw, and +1 for a win. The neural network parameters \u03b8 are updated so as to minimise the error between the predicted outcome vt and the game outcome z, and to maximise the similarity of the policy vector pt to the search probabilities \u03c0t.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q366",
      "query": "How does the performance of smaller, more optimally trained language models compare to larger models in terms of downstream tasks?",
      "answer": "Not only does Chinchilla outperform its much larger counterpart, Gopher, but its reduced model size reduces inference cost considerably and greatly facilitates downstream uses on smaller hardware.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q125",
      "query": "What is the purpose of the 'path expansion trick' in analyzing transformer models?",
      "answer": "Our key trick is to simply expand the product. This transforms the product (where every term corresponds to a layer), into a sum where every term corresponds to an end-to-end path.\n\nWe claim each of these end-to-end path terms is tractable to understand, can be reasoned about independently, and additively combine to create model behavior.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q184",
      "query": "How can prompt attacks be detected in language models?",
      "answer": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to subvert the intended behavior of an LLM functioning as part of an application. The model is a multi-label classifier that detects two classes of prompt attack risk - direct jailbreaks (techniques that explicitly try to override a model's safety conditioning or system prompt) and indirect prompt injections (instances where third-party data included in a model's context window includes instructions inadvertently executed as user commands by an LLM).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q216",
      "query": "How does the performance of recurrent Transformers compare to standard Transformers?",
      "answer": "Recurrent Transformers perform slightly better when comparing models with equal parameter count, but slightly worse when accounting for reuse and comparing per FLOP.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q393",
      "query": "How does the reward-based approach to generalization differ from traditional transfer learning approaches?",
      "answer": "As per our hypothesis, generalisation may instead be understood as, and implemented by, maximising cumulative reward in a continuing stream of interaction between an agent and a single complex environment \u2013 again following a standard agent-environment protocol (see Section 2.1). Environments such as the human world demand generalisation simply because the agent encounters different aspects of the environment at different times. For example, a fruit-eating animal may encounter a new tree every day; furthermore it may become injured, suffer a drought, or face an invasive species. In each case, the animal must adapt quickly to its new state, by generalising its experience from past states.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q097",
      "query": "How can researchers study emergent properties in smaller models?",
      "answer": "We train multiple smaller models, say, 125M, 350M and 1.3B parameters, and then we measure the emergent property in those models and relate it to the property that we are interested in analyzing, for example, a new architecture or a new from of interpreting models. Once we gathered this data, we can measure how the change in the emergent property changes the results of our new method. With that, we might be able to determine if our new method generalizes to models beyond 6.7B parameters.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q016",
      "query": "How does the frequency of relative positions in training data affect model performance?",
      "answer": "For example, in Llama-2 with its 4096 context window, the relative position 4095 accounts for only 1/2048 the frequency of the relative position 2048 in training. These under-trained relative positions can also degrade performance.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q148",
      "query": "What are the benefits of using binary quantization for embeddings?",
      "answer": "By applying this novel rescoring step, we are able to preserve up to ~96% of the total retrieval performance, while reducing the memory and disk space usage by 32x and improving the retrieval speed by up to 32x as well. Without the rescoring, we are able to preserve roughly ~92.5% of the total retrieval performance.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q142",
      "query": "What are the main challenges of scaling embeddings for production use cases?",
      "answer": "However, embeddings may be challenging to scale for production use cases, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in float32, i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory!",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q230",
      "query": "How do the researchers address concerns about the diversity of content generated by small models trained on TinyStories?",
      "answer": "We claim that our models are not doing exact memorization or simple template matching, as evidenced by the methods and metrics we use to evaluate the diversity of the content generated by the models. We rely on several approaches:\n\u2022 Manual inspection: We generate completions for a range of human-constructed stories. We inspect the stories generated by the models and check that they are not copies or close modifications of the stories in the dataset.\n\u2022 Completion of training stories: We take stories from the training set, truncate them in the middle and generate alternative completions with our models. We then compare the completions with the original stories. We observe that the completions are typically very different from the original stories, and often introduce new characters, events, or twists. This is shown in Figure 14.\n\u2022 Diversity of instructions: Recall that in the TinyStories-Instruct dataset, we provide a set of instructions in the form of summaries or words contained in the stories, followed by the stories themselves. We can then change the instructions, verify that the combinations do not appear in the dataset and see how the models adapt to the new instructions. We find that the models can generate diverse stories that follow the instructions, even if they are novel or challenging, such as requiring the model to fit unlikely words into the story or adding features such as a plot twist or a bad ending.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q328",
      "query": "How does the training process differ between masked language models and traditional left-to-right models?",
      "answer": "Compared to standard language model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model to converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left-to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q271",
      "query": "What search algorithm does AlphaZero use instead of alpha-beta search?",
      "answer": "Instead of an alpha-beta search with domain-specific enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root sroot to leaf.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q391",
      "query": "What advantages does reward maximization have over equilibrium-based approaches in multi-agent scenarios?",
      "answer": "Reward maximisation may in fact lead to a better solution than an equilibrium. This is because it may capitalise upon suboptimal behaviours of other agents, rather than assuming optimal or worst-case behaviour. Furthermore, reward maximisation has a unique optimal value, while the equilibrium value is non-unique in general-sum games.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q351",
      "query": "How does label smoothing affect the performance of neural machine translation models?",
      "answer": "During training, we employed label smoothing of value \u03f5 ls = 0.1. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q023",
      "query": "What are some characteristics of good artifacts for language models?",
      "answer": "Good artifacts are...\n- Substantial content (>15 lines)\n- Content that the user is likely to modify, iterate on, or take ownership of\n- Self-contained, complex content that can be understood on its own, without context from the conversation\n- Content intended for eventual use outside the conversation (e.g., reports, emails, presentations)\n- Content likely to be referenced or reused multiple times",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q314",
      "query": "How do the researchers use 'pinned feature sampling' to validate their interpretation of features?",
      "answer": "We also validate that the feature's downstream effect is in line with our interpretation as an Arabic script feature by sampling from the model with the feature activity \"pinned\" at a high value. To do this, we start with a prefix 1,2,3,4,5,6,7,8,9,10 where the model has an expected continuation (keep in mind that this is a one layer model that is very weak!). We then instead set A/1/3450 to its maximum observed value and see how that changes the samples:",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q306",
      "query": "How do the researchers address the challenge of measuring how much of the model's behavior is explained by the extracted features?",
      "answer": "One way to partly get at this question is to ask how much of the loss is explained by our features. For A/1, the run we've focused most on in this paper, 79% of the log-likelihood loss reduction provided by the MLP layer is recovered by our features. That is, the additional loss incurred by replacing the MLP activations with the autoencoder's output is just 21% of the loss that would be incurred by zero ablating the MLP. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. As an extreme example, A/5 ( n_learned_sparse=131,072 , l1_coefficient=0.004 ) recovers 94.5% of log-likelihood loss.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q277",
      "query": "How does AlphaZero's performance scale with thinking time compared to Stockfish and Elmo?",
      "answer": "AlphaZero's MCTS scaled more effectively with thinking time than either Stockfish or Elmo, calling into question the widely held belief (4, 11) that alpha-beta search is inherently superior in these domains.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q140",
      "query": "What is the 'term importance analysis' technique used in studying transformer models?",
      "answer": "But it turns out there's an algorithm which can determine the marginal effect of ablating the nth order terms (that is, the terms corresponding to paths through V-composition of n attention heads). The key trick is to run the model multiple times, replacing the present activations with activations from previous times you ran the model. This allows one to limit the depth of path, ablating all terms of order greater than that. Then by taking differences between the observed losses for each ablation, we can get the marginal effect of the nth order terms.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q367",
      "query": "What is the relationship between model size and training data size in compute-optimal language models?",
      "answer": "All three approaches suggest that as compute budget increases, model size and the amount of training data should be increased in approximately equal proportions.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q383",
      "query": "What is the most general and scalable approach to maximizing reward in artificial intelligence?",
      "answer": "Among possible methods for maximising reward, the most general and scalable approach is to learn to do so, by interacting with the environment by trial and error. We conjecture that an agent that can effectively learn to maximise reward in this manner would, when placed in a rich environment, give rise to sophisticated expressions of general intelligence.",
      "document_url": "https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2022/01/Silver_Reward-is-Enough.pdf"
    },
    {
      "id": "q357",
      "query": "How does the transformer model achieve state-of-the-art performance in machine translation tasks?",
      "answer": "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q312",
      "query": "How do the researchers use automated interpretability to evaluate feature interpretability at scale?",
      "answer": "To analyze features at a larger scale, we turned to automated interpretability . Following the approach of Bills et al. , we have a large language model, Anthropic\u02bcs Claude, generate explanations of features using examples of tokens where they activate. Next, we have the model use that explanation to predict new activations on previously unseen tokens.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q124",
      "query": "What are the three ways attention heads can compose in transformer models?",
      "answer": "When attention heads do compose, there are three options:\n\nQ-Composition: W reads in a subspace affected by a previous head.\nK-Composition: W reads in a subspace affected by a previous head.\nV-Composition: W reads in a subspace affected by a previous head.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q293",
      "query": "What are the trade-offs between search depth and position evaluations per second in game-playing AI?",
      "answer": "AlphaZero searches just 80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million for Stockfish and 35 million for Elmo. AlphaZero compensates for the lower number of evaluations by using its deep neural network to focus much more selectively on the most promising variations \u2013 arguably a more \"human-like\" approach to search, as originally proposed by Shannon",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q003",
      "query": "What is the formula for the maximum extended length of SelfExtend?",
      "answer": "Ideally, the maximum length of the extended context window is:\n(L \u2212 w\nn\n) \u2217 G\ns \n+ w\nn\n.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q128",
      "query": "What is the difference between Q/K-Composition and V-Composition in transformer attention heads?",
      "answer": "Q- and K-Composition are quite different from V-Composition. Q- and K-Composition both affect the attention pattern, allowing attention heads to express much more complex patterns. V-Composition, on the other hand, affects what information an attention head moves when it attends to a given position; the result is that V-composed heads really act more like a single unit and can be thought of as creating an additional \"virtual attention heads\".",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q168",
      "query": "How does Llama 3 handle speech recognition for multiple languages?",
      "answer": "Multilingual ASR and AST modeling often results in language confusion/interference, which leads to degraded performance. A popular way to mitigate this is to incorporate language identification (LID) information, both on the source and target side. This can lead to improved performance in the predetermined set of directions, but it does come with potential loss of generality. For instance, if a translation system expects LID on both source and target side, then the model will not likely to show good zero-shot performance in directions that were not seen in training. So our challenge is to design a system that allows LID information to some extent, but keeps the model general enough such that we can have the model do speech translation in unseen directions. To address this, we design system prompts which only contain LID for the text to be emitted (target side). There is no LID information for the speech input (source side) in these prompts, which also potentially allows it to work with code-switched speech.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q266",
      "query": "How does batch size affect language model training and performance?",
      "answer": "We find that the optimal batch size in this setting is around 1536 for minimal pretraining loss, but 4032 for maximal downstream performance for the 2080ti, i.e. we accumulate gradients and only perform an update every 16 and 42 forward/backward passes, respectively. For the larger A4000 and A6000 cards, this corresponds to a micro-batch size of 128/256 and final batch size of 4096, which we again accumulate.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q276",
      "query": "How does AlphaZero's search efficiency compare to Stockfish and Elmo?",
      "answer": "AlphaZero searches just 80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million for Stockfish and 35 million for Elmo. AlphaZero compensates for the lower number of evaluations by using its deep neural network to focus much more selectively on the most promising variations \u2013 arguably a more \"human-like\" approach to search, as originally proposed by Shannon",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q165",
      "query": "What is the purpose of the Prompt Guard in Llama 3's system-level safety components?",
      "answer": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to subvert the intended behavior of an LLM functioning as part of an application. The model is a multi-label classifier that detects two classes of prompt attack risk - direct jailbreaks (techniques that explicitly try to override a model's safety conditioning or system prompt) and indirect prompt injections (instances where third-party data included in a model's context window includes instructions inadvertently executed as user commands by an LLM).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q037",
      "query": "How do researchers test the importance of individual neurons in spatial and temporal modeling?",
      "answer": "We also perform a series of neuron ablation and intervention experiments in Appendix B to verify the importance of these neurons in spatial and temporal modeling.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q058",
      "query": "What is the new scaling law enabled by BitNet b1.58 in terms of model performance and inference cost?",
      "answer": "70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 13B FP16 LLM.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q180",
      "query": "How can numerical instabilities be addressed when training large multimodal models?",
      "answer": "After the image encoder is added to the model, we find that performing gradient accumulation in bf16 led to numerical instabilities. The most likely explanation for this is that image tokens are introduced into the language backbone via all cross-attention layers. This implies that numerical deviations in the representation of an image token have an outsized impact on the overall computation because the errors are compounded. We address this by performing gradient accumulation in FP32.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q201",
      "query": "What is the relationship between model size and data requirements to avoid performance penalties?",
      "answer": "Performance improves predictably as long as we scale up N and D in tandem, but enters a regime of diminishing returns if either N or D is held fixed while the other increases. The performance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q415",
      "query": "How do sparse autoencoders help in identifying deceptive behavior in language models?",
      "answer": "To address this question, for a random subset of the features in our 1M SAE, we measured the Pearson correlation between its activations and those of every neuron in all preceding layers. Similar to our findings in Towards Monosemanticity, we find that for the vast majority of features, there is no strongly correlated neuron \u2013 for 82% of our features, the most-correlated neuron has a correlation of 0.3 or smaller. Manually inspecting visualizations for the best-matching neuron for a random set of features, we found almost no resemblance in semantic content between the feature and the corresponding neuron. We additionally confirmed that feature activations are not strongly correlated with activations of any residual stream basis direction.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q326",
      "query": "How does the feature-based approach perform compared to fine-tuning in named entity recognition?",
      "answer": "To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without fine-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7. BERT LARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q040",
      "query": "What is the relationship between the performance of probes trained on PCA components and the model's representation of spatial and temporal information?",
      "answer": "Figure 4 illustrates the test R2 for probes trained on each model and dataset over a range of k values, as compared to the performance of the full dmodel-dimensional probe. We also report the test Spearman correlation in Figure 13 which increases much more rapidly with increasing k than the R2. Notably, the Spearman correlation only depends on the rank order of the predictions while R2 also depends on their actual value. We view this gap as further evidence that the model explicitly represents space and time as these features must account for enough variance to be in the top dozen principal components, but that the probe requires more parameters to convert from the model's coordinate system to literal spatial coordinates or timestamps.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q259",
      "query": "What are some effective strategies for training language models with limited computational resources?",
      "answer": "We find that we can find changes to the training recipe that exploit scaling laws to yield improvements by improving the effective rate of gradient computations without compromising model size. In the end, we are able to train models that achieve respectable performance \u2013 often close to and sometimes exceeding BERT on GLUE tasks \u2013 on a shoestring budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q114",
      "query": "What evidence suggests that LLMs perform in-context learning on ARC-AGI tasks?",
      "answer": "Contra Chollet, I think that current LLMs are well described as doing at least some useful learning when doing in-context learning.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q198",
      "query": "What are the challenges in scaling up AI model training to thousands of GPUs?",
      "answer": "The complexity and potential failure scenarios of 16K GPU training surpass those of much larger CPU clusters that we have operated. Moreover, the synchronous nature of training makes it less fault-tolerant\u2014a single GPU failure may require a restart of the entire job. Despite these challenges, for Llama 3, we achieved higher than 90% effective training time while supporting automated cluster maintenance, such as firmware and Linux kernel upgrades (Vigraham and Leonhardi, 2024), which resulted in at least one training interruption daily. The effective training time measures the time spent on useful training over the elapsed time.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q176",
      "query": "How does Llama 3 handle the challenge of long-context safety?",
      "answer": "Long-context models are vulnerable to many-shot jailbreaking attacks without targeted mitigation (Anil et al., 2024). To address this, we finetune our models on SFT datasets that include examples of safe behavior in the presence of demonstrations of unsafe behavior in context. We develop a scalable mitigation strategy that significantly reduces VR, effectively neutralizing the impact of longer context attacks even for 256-shot attacks. This approach shows little to no impact on FRR and most helpfulness metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q202",
      "query": "How do training curves for language models behave?",
      "answer": "Training curves follow predictable power-laws whose parameters are roughly independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q356",
      "query": "What are the potential future applications of attention-based models in sequence transduction tasks?",
      "answer": "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q006",
      "query": "How does SelfExtend perform on short context tasks compared to the original models?",
      "answer": "The results show that SelfExtend can maintain the performance of the short-context tasks, while enhance the performance on long-context tasks. Moreover, because SeldExtend does not require any fine-tuning and only takes effect during inference, SelfExtend can be readily adopted as a plug-in component for LLMs. This means SelfExtend can be automatically and inherently disabled while encountering short-text sequences. Then, with the parameters remaining unchanged, LLMs can maintain its original inference mechanism on those short-context scenarios.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q076",
      "query": "What is the 'intern test' for evaluating LLM generations?",
      "answer": "We like to use the following \"intern test\" when evaluating generations: If you took the exact input to the language model, including the context, and gave it to an average college student in the relevant major as a task, could they succeed? How long would it take?",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q265",
      "query": "What are some effective modifications to the transformer architecture for efficient training?",
      "answer": "We disable all QKV biases (Dayma et al., 2021). This exploits the scaling law by removing a layer of computation, making the forward and backward pass somewhat faster, while keeping the model size nearly constant. We find empirical gains from disabling all linear layer biases (Dayma et al., 2021). Just as for the attention layers, this leverages the scaling law by accelerating gradient computation without noticeable impacts on model size.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q042",
      "query": "What is the main advantage of BitNet b1.58 over full-precision LLMs?",
      "answer": "BitNet b1.58 matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q117",
      "query": "How could ARC-AGI be improved as an evaluation tool for AI progress?",
      "answer": "More minimally, I think that ARC-AGI would be a better evaluation of progress towards TAI if it used purely text based problems or at least had a text based subset: good vision isn't necessary for TAI and improved vision has outsized effects on ARC-AGI relative to TAI progress.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q374",
      "query": "How does the performance of language models on reading comprehension tasks compare to previous models?",
      "answer": "On the final word prediction dataset LAMBADA (Paperno et al., 2016), Chinchilla achieves 77.4% accuracy, compared to 74.5% accuracy from Gopher and 76.6% from MT-NLG 530B (see Table 7). On RACE-h and RACE-m (Lai et al., 2017), Chinchilla greatly outperforms Gopher, improving accuracy by more than 10% in both cases\u2014see Table 7.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q343",
      "query": "What are the key components of an encoder-decoder architecture in neural sequence transduction models?",
      "answer": "Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations (x 1 , ..., x n ) to a sequence of continuous representations z = (z 1 , ..., z n ). Given z, the decoder then generates an output sequence (y 1 , ..., y m ) of symbols one element at a time.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q135",
      "query": "How do transformer models implement 'copying' behavior?",
      "answer": "One of the most striking things about looking at these matrices is that most attention heads in one layer models dedicate an enormous fraction of their capacity to copying. The OV circuit sets things up so that tokens, if attended to by the head, increase the probability of that token, and to a lesser extent, similar tokens. The QK circuit then only attends back to tokens which could plausibly be the next token. Thus, tokens are copied, but only to places where bigram-ish statistics make them seem plausible.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q118",
      "query": "What are potential risks associated with scaling LLMs, as discussed in relation to ARC-AGI performance?",
      "answer": "TAI poses huge risks. Making mistaken predictions about where LLMs are heading could result in a dramatic underestimate of the dangers they could pose. If, like Mike Knoop (co-host of the ARC-AGI prize), you oppose bills like SB-1047 because you think LLMs won't scale, then it really matters that you are right about LLMs not scaling. And every time you get evidence that indicates that scaling might be dangerously powerful (and I hope this post provided some), you should update appropriately in favor of more caution.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q376",
      "query": "How does the Chinchilla model perform on gender bias tests compared to previous models?",
      "answer": "As shown in Table 10, Chinchilla correctly resolves pronouns more frequently than Gopher across all groups. Interestingly, the performance increase is considerably smaller for male pronouns (increase of 3.2%) than for female or neutral pronouns (increases of 8.3% and 9.2% respectively). We also consider gotcha examples, in which the correct pronoun resolution contradicts gender stereotypes (determined by labor statistics). Again, we see that Chinchilla resolves pronouns more accurately than Gopher.",
      "document_url": "https://arxiv.org/pdf/2203.15556"
    },
    {
      "id": "q063",
      "query": "How can you improve the performance of smaller LLMs?",
      "answer": "While it may be weaker, techniques like chain-of-thought, n-shot prompts, and in-context learning can help smaller models punch above their weight. Beyond LLM APIs, finetuning our specific tasks can also help increase performance.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q334",
      "query": "What are the computational requirements for fine-tuning language models on downstream tasks?",
      "answer": "Compared to pre-training, fine-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.",
      "document_url": "https://arxiv.org/pdf/1810.04805"
    },
    {
      "id": "q264",
      "query": "How can learning rate schedules be optimized for language model training?",
      "answer": "We re-scale the learning rate schedule so that it is tied to our budget and the learning rate decays as the budget reduces to zero. Interestingly, we observe in Figure 2 that while globally a large number of learning rate shapes lead to similar reductions in loss, we find that we can make some gains through the choice of schedule. We find that a simple one-cycle learning rate (Smith & Topin, 2018) with a peak learning rate of 10^-3 leads to minimal pretraining loss within our budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q248",
      "query": "What changes were made to the feedforward block in the cramming model?",
      "answer": "Feedforward Block: We find empirical gains from disabling all linear layer biases (Dayma et al., 2021). Just as for the attention layers, this leverages the scaling law by accelerating gradient computation without noticeable impacts on model size. As a result, we get higher throughput without compromising the rate at which the model improves. We keep the original feedforward block largely unchanged, finding no benefits from changing to another activation than GELU. We do see small improvements from re-ordering the block into a gated linear unit (Dauphin et al., 2017). In contrast to other work, e.g. (Black et al., 2022), we do not increase the number of parameters in the FFN block to compensate for the halving of the hidden dimensionality due to gating.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q303",
      "query": "What is the concept of 'feature splitting' in the context of dictionary learning?",
      "answer": "We conjecture that there is some idealized set of features that dictionary learning would return if we provided it with an unlimited dictionary size. Often, these \"true features\" are clustered into sets of similar features, which the model puts in very tight superposition. Because the number of features is restricted, dictionary learning instead returns features which cover approximately the same territory as the idealized features, at the cost of being somewhat less specific.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
    },
    {
      "id": "q146",
      "query": "What is the Hamming Distance and why is it useful for binary embeddings?",
      "answer": "We can use the Hamming Distance to retrieve these binary embeddings efficiently. This is the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings; thus, the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q414",
      "query": "What are some challenges in evaluating the effectiveness of sparse autoencoders for language model interpretability?",
      "answer": "Inability to Evaluate. In most machine learning research, one has a principled objective function which can be optimized. But in this work, it isn't really clear what the \"ground truth\" objective is. The objective we optimize \u2013 a combination of reconstruction accuracy and sparsity \u2013 is only a proxy for what we really are interested in, interpretability. For example, it isn't clear how we should trade off between the mean squared error and sparsity, nor how we'd know if we made that trade-off well. As a result, while we can very scientifically study how to optimize the loss of SAEs and infer scaling laws, it's unclear that they're really getting at the fundamental thing we care about.",
      "document_url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
    },
    {
      "id": "q169",
      "query": "What is the purpose of the image adapter in Llama 3's architecture?",
      "answer": "We introduce cross-attention layers between the visual token representations produced by the image encoder and the token representations produced by the language model (Alayrac et al., 2022). The cross-attention layers are applied after every fourth self-attention layer in the core language model. Like the language model itself, the cross-attention layers use generalized query attention (GQA) for increased efficiency. The cross-attention layers introduce substantial numbers of additional trainable parameters into the model: for Llama 3 405B, the cross-attention layers have \u2248100B parameters.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q349",
      "query": "How does the computational complexity of self-attention layers compare to recurrent layers?",
      "answer": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations.",
      "document_url": "https://arxiv.org/pdf/1706.03762"
    },
    {
      "id": "q159",
      "query": "What are the three key levers in developing high-quality foundation models according to the Llama 3 team?",
      "answer": "We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process:",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q213",
      "query": "What is the relationship between model size and the number of training steps required to reach a given performance level?",
      "answer": "The number of minimum serial steps needed to reach any fixed value of the test loss decreases precipitously with model size. Sample efficiency (show here for training far below the critical batch size) improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model to a very large one.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    }
  ]
}