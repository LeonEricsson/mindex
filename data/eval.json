{
  "dataset": "Mindex Evaluation",
  "validation": [
    {
      "id": "q001",
      "query": "What is the main goal of SelfExtend?",
      "answer": "We propose SelfExtend to extend the context window of LLMs by constructing bi-level attention information: the grouped attention and the neighbor attention. The grouped attention captures the dependencies among tokens that are far apart, while neighbor attention captures dependencies among adjacent tokens within a specified range.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q002",
      "query": "How does SelfExtend address the positional O.O.D. issue?",
      "answer": "SelfExtend addresses the issue of O.O.D. positional information by using a simple floor division operation to map unseen large relative positions to those encountered during pretraining.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q003",
      "query": "What is the formula for the maximum extended length of SelfExtend?",
      "answer": "Ideally, the maximum length of the extended context window is:\n(L − w\nn\n) ∗ G\ns \n+ w\nn\n.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q004",
      "query": "How does SelfExtend compare to fine-tuning methods for context window extension?",
      "answer": "Notably, SelfExtend modifies only the attention mechanism during inference, eliminating the need for additional fine-tuning.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q005",
      "query": "What are the two types of attention mechanisms incorporated in SelfExtend?",
      "answer": "SelfExtend incorporates two distinct types of attention mechanisms: 1) Grouped attention, specifically designed for tokens that are far apart. This approach applies a floor operation to the positions to manage long-distance relationships between tokens; 2) Standard attention, which employs the conventional attention mechanism for adjacent tokens within a specified range.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q006",
      "query": "How does SelfExtend perform on short context tasks compared to the original models?",
      "answer": "The results show that SelfExtend can maintain the performance of the short-context tasks, while enhance the performance on long-context tasks. Moreover, because SeldExtend does not require any fine-tuning and only takes effect during inference, SelfExtend can be readily adopted as a plug-in component for LLMs. This means SelfExtend can be automatically and inherently disabled while encountering short-text sequences. Then, with the parameters remaining unchanged, LLMs can maintain its original inference mechanism on those short-context scenarios.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q007",
      "query": "What are the two trade-offs observed in SelfExtend's performance?",
      "answer": "1) There is a trade-off with respect to group size in SelfExtend. Generally, both too small and too large group sizes can result in inferior performance compared to an optimal level. With a large group size, position information becomes more coarse, potentially causing performance drops. Conversely, small group sizes require SelfExtend to utilize larger position embeddings to extend the context window. These larger position embeddings are less trained compared to smaller ones.\n\n2) There is also another trade-off w.r.t. neighbor window size. With larger neighbor window sizes, there is more precise information about neighbor tokens, which is the most important. But a larger neighbor window size means SelfExtend has to use a larger group size for a long sequence, compared to using a smaller neighbor window size & smaller group size, the information about the whole sequence becomes coarse.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q008",
      "query": "How does SelfExtend perform on the passkey retrieval task with varying passkey lengths?",
      "answer": "Notably, with the exception of Yarn, many tuning-based methods are unable to accurately reproduce passkeys beyond 64 digits, and some of them even experience a marked decline in performance when the passkey length exceeds 16 digits. Remarkably, although without tuning, SelfExtend maintains its superiority.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q009",
      "query": "What is the empirical rule for selecting hyperparameters in SelfExtend?",
      "answer": "We conclude those results as an empirical rule. Denoting the pretraining context window as L, the target extension length as N , the neighbor window as W , and the group size as G, the empirical rule for selecting hyperparameters is to ensure that the following inequality holds:\n1\n2 \n× L > W + \nN − W\nG",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q010",
      "query": "What are the main limitations of SelfExtend?",
      "answer": "Limitations: SelfExtend increases computation cost with naive implementations, since it performs extra attention across all query-key pairs. However, with optimizations like blocked kernels (e.g. Flash Attention (Dao et al., 2022)), this becomes linear rather than quadratic, and the marginal cost is small enough to be ignored for long input sequences. Also, the performance degrades with large group size, preventing indefinitely long contexts. Besides, SelfExtend still processes the entire sequence to ensure information integrity, while some methods such as prompt compression (Chuang et al., 2024; Jiang et al., 2023b) can shorten the input to reduce computation. Additionally, evaluation methodologies for assessing long context abilities remain open research questions. Standard practices have yet to emerge, complicating experimental results.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q011",
      "query": "Why might fine-tuning large language models on long texts be problematic?",
      "answer": "Additionally, high-quality long text data is scarce, hindering such fine-tuning approaches. Most real-world data is short, and much long text lacks meaningful long-range dependencies. With limited appropriate data, finetuning risks degrading existing strong performance on shorter sequences from pretraining or overfitting models to the tuning set. LLMs' generalizability to broad tasks may be reduced.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q012",
      "query": "How does the importance of precise word positions change in long texts?",
      "answer": "The core idea hinges on the observation that, in long texts, exacting word positions becomes less crucial. The overall meaning and the relative order of information hold greater significance. Just like when answering questions about lengthy texts, we rely on the general location and order, not the specific word-by-word placement.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q013",
      "query": "What are some challenges in evaluating long context capabilities of language models?",
      "answer": "Additionally, evaluation methodologies for assessing long context abilities remain open research questions. Standard practices have yet to emerge, complicating experimental results.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q014",
      "query": "How does the training of language models on short texts relate to human learning?",
      "answer": "Our belief stems from the fact that when we, as human beings, are children, we are taught how to read and write using relatively short texts, such as articles spanning several pages. We rarely use extremely long texts like entire books or complete documents as learning materials. Yet, we are still able to understand long texts effectively.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q015",
      "query": "What is the relationship between perplexity and a model's ability to handle long contexts?",
      "answer": "PPL is not an effective metric for measuring the ability of LLMs to handle long contexts. In Figure 7, we introduce a seeming plausible context window extension method named 'Infinite'. When evaluated on PG19 using the same protocol, Llama-2-7b-chat with 'Infinite' achieves PPL scores that are comparable to, or even lower than, those achieved by SelfExtend, as demonstrated in Table 6. However, 'Infinite' essentially mimics the process of dividing a long sequence into short sub-sequences before processing them with LLMs, indicating that it does not genuinely address long context handling.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q016",
      "query": "How does the frequency of relative positions in training data affect model performance?",
      "answer": "For example, in Llama-2 with its 4096 context window, the relative position 4095 accounts for only 1/2048 the frequency of the relative position 2048 in training. These under-trained relative positions can also degrade performance.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q017",
      "query": "What are the fundamental capabilities tested by the passkey retrieval task?",
      "answer": "Although this task is easy and far from real-world scenarios, it tests two fundamental capabilities of LLMs: 1. The model should be able to recognize and locate the useful information across all positions of the input sequence (the most fundamental understanding capability); 2. The model should be able to use the perceived information to finish tasks (the most fundamental generation capability).",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q018",
      "query": "How does the behavior of neural networks change when dealing with out-of-distribution inputs?",
      "answer": "It is widely recognized that Neural Networks (NNs) are susceptible to unpredictable behaviors when dealing with O.O.D inputs (Liu et al., 2021; Shen et al., 2021; Bai et al., 2021; Zhang et al., 2023).",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q019",
      "query": "What is the significance of neighboring tokens in language modeling?",
      "answer": "Natural language exhibits a characteristic where meaning stays relatively consistent within short ranges like paragraphs. Therefore, using close or even identical position encodings effectively captures the necessary relative ordering of important information.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q020",
      "query": "How does the computation cost of attention mechanisms typically scale with sequence length?",
      "answer": "SelfExtend increases computation cost with naive implementations, since it performs extra attention across all query-key pairs. However, with optimizations like blocked kernels (e.g. Flash Attention (Dao et al., 2022)), this becomes linear rather than quadratic, and the marginal cost is small enough to be ignored for long input sequences.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q021",
      "query": "Why is perplexity (PPL) not an effective metric for measuring LLMs' ability to handle long contexts?",
      "answer": "The discrepancy between Perplexity (PPL) and long context ability primarily stems from how PPL is calculated by averaging over numerous tokens. As long as the majority of tokens are modeled accurately, PPL will remain low. This is closely related to the influence of neighboring tokens. Information from neighboring tokens—such as those within the local attention window of 'Infinite'—can suffice for predicting most tokens, thus leading to a low PPL. However, a few critical tokens, which are crucial for understanding long contexts and answering questions, may not be predicted accurately.",
      "document_url": "https://arxiv.org/pdf/2401.01325"
    },
    {
      "id": "q022",
      "query": "How do large language models represent space and time?",
      "answer": "We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q023",
      "query": "What are some characteristics of good artifacts for language models?",
      "answer": "Good artifacts are...\n- Substantial content (>15 lines)\n- Content that the user is likely to modify, iterate on, or take ownership of\n- Self-contained, complex content that can be understood on its own, without context from the conversation\n- Content intended for eventual use outside the conversation (e.g., reports, emails, presentations)\n- Content likely to be referenced or reused multiple times",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q024",
      "query": "What evidence suggests that language models form coherent world representations?",
      "answer": "We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q025",
      "query": "How do researchers test if language models have learned spatial and temporal representations?",
      "answer": "Using the Llama-2 (Touvron et al., 2023) and Pythia Biderman et al. (2023) family of models, we train linear regression probes (Alain & Bengio, 2016; Belinkov, 2022) on the internal activations of the names of these places and events at each layer to predict their real-world location (i.e., latitude/longitude) or time (numeric timestamp).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q026",
      "query": "What is the relationship between model size and the quality of spatial and temporal representations in language models?",
      "answer": "These probing experiments reveal evidence that models build spatial and temporal representations throughout the early layers before plateauing at around the model halfway point with larger models consistently outperforming smaller ones (§ 3.1).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q027",
      "query": "How do researchers verify that spatial and temporal representations in language models are actually used by the model?",
      "answer": "We also perform a series of neuron ablation and intervention experiments in Appendix B to verify the importance of these neurons in spatial and temporal modeling.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q028",
      "query": "What are some limitations of using English Wikipedia as a data source for studying spatial representations in language models?",
      "answer": "Several limitation are worth highlighting. First, our data only comes from English Wikipedia, and hence is skewed towards the Anglosphere. Additionally, the distribution of entity types is not uniform, e.g. we noticed the United Kingdom has many more railway stations than any other country, which could introduce unwanted correlations in the data that may affect the probes.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q029",
      "query": "How do researchers ensure that their probes are not just memorizing country or decade-level information?",
      "answer": "To better distinguish these cases, we analyze how the probes generalize when holding out specific blocks of data. In particular, we train a series of probes, where for each one, we hold out one country, state, borough, century, decade, or year for the world, USA, NYC, historical figure, entertainment, and headlines dataset respectively. We then evaluate the probes on the held out block of data.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q030",
      "query": "What evidence suggests that language models represent space and time in a unified manner across different entity types?",
      "answer": "We find that when we project the activation datasets on to the weights of the most similar neurons, these neurons are indeed highly sensitive to the true location of entities in space or time (see Figure 5). In other words, there exist individual neurons within the model that are themselves fairly predictive feature probes. Moreover, these neurons are sensitive to all of the entity types within our datasets, providing stronger evidence for the claim these representations are unified.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q031",
      "query": "How do researchers create datasets for testing spatial and temporal representations in language models?",
      "answer": "To enable our investigation, we construct six datasets of names of entities (people, places, events, etc.) with their respective location or occurrence in time, each at a different order of magnitude of scale. For each dataset, we included multiple types of entities, e.g., both populated places like cities and natural landmarks like lakes, to study how unified representations are across different object types. Furthermore, we maintain or enrich relevant metadata to enable analyzing the data with more detailed breakdowns, identify sources of train-test leakage, and support future work on factual recall within LLMs. We also attempt to deduplicate and filter out obscure or otherwise noisy data.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q032",
      "query": "What is the relationship between the number of model parameters and the quality of spatial and temporal representations?",
      "answer": "These probing experiments reveal evidence that models build spatial and temporal representations throughout the early layers before plateauing at around the model halfway point with larger models consistently outperforming smaller ones (§ 3.1).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q033",
      "query": "How do researchers test if spatial and temporal representations in language models are linear?",
      "answer": "To test whether spatial and temporal features are represented linearly, we compare the performance of our linear ridge regression probes with that of substantially more expressive nonlinear MLP probes of the form W2 ReLU(W1 x + b1) + b2 with 256 neurons. Table 2 reports our results and shows that using nonlinear probes results in minimal improvement to R2 for any dataset or model. We take this as strong evidence that space and time are also represented linearly (or at the very least are linearly decodable), despite being continuous.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q034",
      "query": "How do researchers test the robustness of spatial and temporal representations to different prompts?",
      "answer": "To study this, we create new activation datasets where we prepend different prompts to each of the entity tokens, following a few basic themes. In all cases, we include an \"empty\" prompt containing nothing other than the entity tokens (and a beginning of sequence token). We then include a prompt which asks the model to recall the relevant fact, e.g., \"What is the latitude and longitude of <place>\" or \"What was the release date of <author>'s <book>.\" For the United States and NYC datasets we also include versions of these prompts asking where in the US or NYC this location is, in an attempt to disambiguate common names of places (e.g. City Hall). As a baseline we include a prompt of 10 random tokens (sampled for each entity). To determine if we can obfuscate the subject, for some datasets we fully capitalize the names of all entities. Lastly, for the headlines dataset, we try probing on both the last token and on a period token appended to the headline.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q035",
      "query": "How do researchers identify individual neurons responsible for encoding spatial and temporal information?",
      "answer": "To address this, we search for individual neurons with input or output weights that have high cosine similarity with the learned probe direction. That is, we search for neurons which read from or write to a direction similar to the one learned by the probe.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q036",
      "query": "What is the relationship between the performance of individual neurons and the overall model's representation of spatial and temporal information?",
      "answer": "If probes trained with explicit supervision are an approximate upper bound on the extent to which a model represents these spatial and temporal features, then the performance of individual neurons is a lower bound. In particular, we generally expect features to be distributed in superposition (Elhage et al., 2022b), making individual neurons the wrong level of analysis. Nevertheless, the existence of these individual neurons, which received no supervision other than from next-token prediction, is very strong evidence that the model has learned and makes use of spatial and temporal features.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q037",
      "query": "How do researchers test the importance of individual neurons in spatial and temporal modeling?",
      "answer": "We also perform a series of neuron ablation and intervention experiments in Appendix B to verify the importance of these neurons in spatial and temporal modeling.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q038",
      "query": "What is the relationship between the size of language models and their ability to form detailed spatial models of individual cities?",
      "answer": "The dataset with the worst performance is the New York City dataset. This was expected given the relative obscurity of most of the entities compared with other datasets. However, this is also the dataset where the largest model has the best relative performance, suggesting that sufficiently large LLMs could eventually form detailed spatial models of individual cities.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q039",
      "query": "How do researchers create datasets for testing temporal representations in language models?",
      "answer": "Our three temporal datasets consist of (1) the names and occupations of historical figures who died between 1000BC and 2000AD adapted from (Annamoradnejad & Annamoradnejad, 2022); (2) the titles and creators of songs, movies, and books from 1950 to 2020 constructed from DBpedia with the Wikipedia page views filtering technique; and (3) New York Times news headlines from 2010-2020 from news desks that write about current events, adapted from (Bandy, 2021).",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q040",
      "query": "What is the relationship between the performance of probes trained on PCA components and the model's representation of spatial and temporal information?",
      "answer": "Figure 4 illustrates the test R2 for probes trained on each model and dataset over a range of k values, as compared to the performance of the full dmodel-dimensional probe. We also report the test Spearman correlation in Figure 13 which increases much more rapidly with increasing k than the R2. Notably, the Spearman correlation only depends on the rank order of the predictions while R2 also depends on their actual value. We view this gap as further evidence that the model explicitly represents space and time as these features must account for enough variance to be in the top dozen principal components, but that the probe requires more parameters to convert from the model's coordinate system to literal spatial coordinates or timestamps.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q041",
      "query": "How do researchers test if language models have a unified representation of space and time across different entity types?",
      "answer": "Similar to the above, we distinguish these hypotheses by training a series of probes where the train-test split is performed to hold out all points of a particular entity class. Table 4 reports the proximity error for the entities in the default test split compared to when heldout, averaged over all such splits as before. The results suggest that the probes largely generalize across entity types, with the main exception of the entertainment dataset.",
      "document_url": "https://arxiv.org/pdf/2310.02207"
    },
    {
      "id": "q042",
      "query": "What is the main advantage of BitNet b1.58 over full-precision LLMs?",
      "answer": "BitNet b1.58 matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q043",
      "query": "How does BitNet b1.58 constrain the weights of the model?",
      "answer": "To constrain the weights to -1, 0, or +1, we adopt an absmean quantization function. It first scales the weight matrix by its average absolute value, and then round each value to the nearest integer among {-1, 0, +1}:",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q044",
      "query": "What are the main components adopted from LLaMA in the BitNet b1.58 architecture?",
      "answer": "Specifically, it uses RMSNorm, SwiGLU, rotary embedding, and removes all biases. In this way, BitNet b1.58 can be integrated into the popular open-source software (e.g., Huggingface, vLLM, and llama.cpp) with minimal efforts.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q045",
      "query": "At what model size does BitNet b1.58 start to match full precision LLaMA LLM in terms of perplexity?",
      "answer": "It shows that BitNet b1.58 starts to match full precision LLaMA LLM at 3B model size in terms of perplexity, while being 2.71 times faster and using 3.55 times less GPU memory.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q046",
      "query": "How does the speed-up of BitNet b1.58 change as the model size increases?",
      "answer": "Figure 2 illustrates the trends of latency and memory, showing that the speed-up increases as the model size scales. In particular, BitNet b1.58 70B is 4.1 times faster than the LLaMA LLM baseline.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q047",
      "query": "What is the main difference in energy consumption between BitNet b1.58 and LLaMA LLM?",
      "answer": "The majority of BitNet b1.58 is INT8 addition calculation, while LLaMA LLM consists of both FP16 addition and FP16 multiplication.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q048",
      "query": "How much more energy efficient is BitNet b1.58 compared to LLaMA LLM for matrix multiplication?",
      "answer": "According to the energy model in [Hor14, ZZL22], BitNet b1.58 saves 71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q049",
      "query": "What is the throughput advantage of BitNet b1.58 70B over LLaMA LLM 70B?",
      "answer": "Table 3 shows that BitNet b1.58 70B can support up to 11 times the batch size of LLaMA LLM, resulting an 8.9 times higher throughput.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q050",
      "query": "How does BitNet b1.58 compare to StableLM-3B in terms of performance on various tasks?",
      "answer": "Our findings shows that BitNet b1.58 achieves a superior performance on all end tasks, indicating that 1.58-bit LLMs also have strong generalization capabilities.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q051",
      "query": "What potential advantage does BitNet b1.58 offer for Mixture-of-Experts (MoE) LLMs?",
      "answer": "Firstly, the reduced memory footprint reduces the number of devices required to deploy MoE models. Moreover, it significantly reduces the overhead of transferring activations across networks. Ultimately, there would be no overhead if the entire models could be placed on a single chip.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q052",
      "query": "How can BitNet b1.58 potentially improve handling of long sequences in LLMs?",
      "answer": "BitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the activations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources. This can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave as future work.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q053",
      "query": "What are the potential benefits of using 1.58-bit LLMs on edge and mobile devices?",
      "answer": "The reduced memory and energy consumption of 1.58-bit LLMs allows them to be deployed on these devices, enabling a wide range of applications that were previously not possible. This can greatly enhance the capabilities of edge and mobile devices and enable new and exciting applications of LLMs.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q054",
      "query": "What is the main difference between BitNet b1.58 and the original 1-bit BitNet?",
      "answer": "We have added an additional value of 0 to the original 1-bit BitNet, resulting in 1.58 bits in the binary system.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q055",
      "query": "What dataset was used to pre-train and compare BitNet b1.58 to LLaMA LLM?",
      "answer": "To ensure a fair comparison, we pre-trained the models on the RedPajama dataset [Com23] for 100 billion tokens.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q056",
      "query": "What software was used to measure the runtime GPU memory and latency of the models?",
      "answer": "The results were measured using the FasterTransformer codebase, which is well-optimized for LLM inference latency on GPU devices. The 2-bit kernel from Ladder [WMC+23] is also integrated for BitNet b1.58.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q057",
      "query": "How does BitNet b1.58 compare to LLaMA LLM in terms of memory usage for larger models?",
      "answer": "The memory consumption follows a similar trend, as the embedding remains full precision and its memory proportion is smaller for larger models.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q058",
      "query": "What is the new scaling law enabled by BitNet b1.58 in terms of model performance and inference cost?",
      "answer": "70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 13B FP16 LLM.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q059",
      "query": "How does BitNet b1.58 potentially benefit CPU devices in edge and mobile applications?",
      "answer": "Moreover, 1.58-bit LLMs are more friendly to CPU devices, which are the main processors used in edge and mobile devices. This means that BitNet b1.58 can be efficiently executed on these devices, further improving their performance and capabilities.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q060",
      "query": "What is the call to action mentioned in the paper regarding hardware for 1-bit LLMs?",
      "answer": "Going one step further, we envision and call for actions to design new hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm enabled in BitNet [WMD+23].",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q061",
      "query": "How does BitNet b1.58 handle activations compared to full-precision models?",
      "answer": "BitNet b1.58 represents a significant step towards native support for long sequences, as it reduces the activations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources.",
      "document_url": "https://arxiv.org/pdf/2402.17764"
    },
    {
      "id": "q062",
      "query": "What are some effective prompting techniques for LLMs?",
      "answer": "A few prompting techniques have consistently helped with improving performance across a variety of models and tasks: n-shot prompts + in-context learning, chain-of-thought, and providing relevant resources.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q063",
      "query": "How can you improve the performance of smaller LLMs?",
      "answer": "While it may be weaker, techniques like chain-of-thought, n-shot prompts, and in-context learning can help smaller models punch above their weight. Beyond LLM APIs, finetuning our specific tasks can also help increase performance.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q064",
      "query": "What's the benefit of using structured input and output with LLMs?",
      "answer": "Structured input and output help models better understand the input as well as return output that can reliably integrate with downstream systems. Adding serialization formatting to your inputs can help provide more clues to the model as to the relationships between tokens in the context, additional metadata to specific tokens (like types), or relate the request to similar examples in the model's training data.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q065",
      "query": "Why is it important to have small, focused prompts for LLMs?",
      "answer": "Just like how we strive (read: struggle) to keep our systems and code simple, so should we for our prompts. Instead of having a single, catch-all prompt for the meeting transcript summarizer, we can break it into steps: - Extract key decisions, action items, and owners into structured format - Check extracted details against the original transcription for consistency - Generate a concise summary from the structured details",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q066",
      "query": "What factors should be considered when evaluating the quality of RAG's retrieved documents?",
      "answer": "The quality of your RAG's output is dependent on the quality of retrieved documents, which in turn can be considered along a few factors. The first and most obvious metric is relevance. This is typically quantified via ranking metrics such as Mean Reciprocal Rank (MRR) or Normalized Discounted Cumulative Gain (NDCG). MRR evaluates how well a system places the first relevant result in a ranked list while NDCG considers the relevance of all the results and their positions. They measure how good the system is at ranking relevant documents higher and irrelevant documents lower.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q067",
      "query": "What are the advantages of keyword search in information retrieval?",
      "answer": "Keyword-based search, such as BM25, is explicitly designed for this. Finally, after years of keyword-based search, users have likely taken it for granted and may get frustrated if the document they expect to retrieve isn't being returned.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q068",
      "query": "Why might RAG be preferred over finetuning for incorporating new knowledge into LLMs?",
      "answer": "Beyond improved performance, RAG has other practical advantages. First, compared to continuous pretraining or finetuning, it's easier---and cheaper!---to keep retrieval indices up-to-date. Second, if our retrieval indices have problematic documents that contain toxic or biased content, we can easily drop or modify the offending documents. Consider it an andon cord for documents that ask us to add glue to pizza.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q069",
      "query": "How can multi-turn workflows improve LLM performance?",
      "answer": "It's common knowledge that decomposing a single big prompt into multiple smaller prompts can achieve better results. For example, AlphaCodium: By switching from a single prompt to a multi-step workflow, they increased GPT-4 accuracy (pass@5) on CodeContests from 19% to 44%. The workflow includes: - Reflecting on the problem - Reasoning on the public tests - Generating possible solutions - Ranking possible solutions - Generating synthetic tests - Iterating on the solutions on public and synthetic tests.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q070",
      "query": "What are the benefits of deterministic workflows for AI agents?",
      "answer": "A potential approach is to have agent systems produce deterministic plans which are then executed in a structured, reproducible way. First, given a high-level goal or prompt, the agent generates a plan. Then, the plan is executed deterministically. This allows each step to be more predictable and reliable. Benefits include: - Generated plans can serve as few-shot samples to prompt or finetune an agent. - Deterministic execution makes the system more reliable, and thus easier to test and debug. In addition, failures can be traced to the specific steps in the plan. - Generated plans can be represented as directed acyclic graphs (DAGs) which are easier, relative to a static prompt, to understand and adapt to new situations.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q071",
      "query": "How can you increase output diversity in LLMs beyond adjusting temperature?",
      "answer": "The simplest way is to adjust elements within the prompt. For example, if the prompt template includes a list of items, such as historical purchases, shuffling the order of these items each time they're inserted into the prompt can make a significant difference. Additionally, keeping a short list of recent outputs can help prevent redundancy. In our recommended products example, by instructing the LLM to avoid suggesting items from this recent list, or by rejecting and resampling outputs that are similar to recent suggestions, we can further diversify the responses.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q072",
      "query": "What are the benefits of caching in LLM applications?",
      "answer": "Caching saves cost and eliminates generation latency by removing the need to recompute responses for the same input. Furthermore, if a response has previously been guardrailed, we can serve these vetted responses and reduce the risk of serving harmful or inappropriate content.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q073",
      "query": "When should you consider finetuning an LLM?",
      "answer": "We may have some tasks where even the most cleverly designed prompts fall short. For example, even after significant prompt engineering, our system may still be a ways from returning reliable, high-quality output. If so, then it may be necessary to finetune a model for your specific task.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q074",
      "query": "What are some effective strategies for evaluating LLMs?",
      "answer": "Create unit tests (i.e., assertions) consisting of samples of inputs and outputs from production, with expectations for outputs based on at least three criteria. While three criteria might seem arbitrary, it's a practical number to start with; fewer might indicate that your task isn't sufficiently defined or is too open-ended, like a general-purpose chatbot. These unit tests, or assertions, should be triggered by any changes to the pipeline, whether it's editing a prompt, adding new context via RAG, or other modifications.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q075",
      "query": "How can LLM-as-Judge be effectively implemented for evaluations?",
      "answer": "Here are some suggestions to get the most out of LLM-as-Judge: - Use pairwise comparisons: Instead of asking the LLM to score a single output on a Likert scale, present it with two options and ask it to select the better one. This tends to lead to more stable results. - Control for position bias: The order of options presented can bias the LLM's decision. To mitigate this, do each pairwise comparison twice, swapping the order of pairs each time. Just be sure to attribute wins to the right option after swapping! - Allow for ties: In some cases, both options may be equally good. Thus, allow the LLM to declare a tie so it doesn't have to arbitrarily pick a winner.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q076",
      "query": "What is the 'intern test' for evaluating LLM generations?",
      "answer": "We like to use the following \"intern test\" when evaluating generations: If you took the exact input to the language model, including the context, and gave it to an average college student in the relevant major as a task, could they succeed? How long would it take?",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q077",
      "query": "How can you simplify annotation tasks for LLM evaluations?",
      "answer": "A more effective approach is to simplify the task and reduce the cognitive burden on annotators. Two tasks that work well are binary classifications and pairwise comparisons. In binary classifications, annotators are asked to make a simple yes-or-no judgment on the model's output. They might be asked whether the generated summary is factually consistent with the source document, or whether the proposed response is relevant, or if it contains toxicity. Compared to the Likert scale, binary decisions are more precise, have higher consistency among raters, and lead to higher throughput.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q078",
      "query": "Why is it important to check for development-prod skew in LLM applications?",
      "answer": "A common source of errors in traditional machine learning pipelines is train-serve skew. This happens when the data used in training differs from what the model encounters in production. Although we can use LLMs without training or finetuning, hence there's no training set, a similar issue arises with development-prod data skew. Essentially, the data we test our systems on during development should mirror what the systems will face in production. If not, we might find our production accuracy suffering.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q079",
      "query": "What are the benefits of generating structured output from LLMs?",
      "answer": "For most real-world use cases, the output of an LLM will be consumed by a downstream application via some machine-readable format. For example, Rechat, a real-estate CRM, required structured responses for the front end to render widgets. Similarly, Boba, a tool for generating product strategy ideas, needed structured output with fields for title, summary, plausibility score, and time horizon. Finally, LinkedIn shared about constraining the LLM to generate YAML, which is then used to decide which skill to use, as well as provide the parameters to invoke the skill.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q080",
      "query": "Why is it important to version and pin LLM models?",
      "answer": "Pinning model versions in production can help avoid unexpected changes in model behavior, which could lead to customer complaints about issues that may crop up when a model is swapped, such as overly verbose outputs or other unforeseen failure modes. Additionally, consider maintaining a shadow pipeline that mirrors your production setup but uses the latest model versions. This enables safe experimentation and testing with new releases. Once you've validated the stability and quality of the outputs from these newer models, you can confidently update the model versions in your production environment.",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q081",
      "query": "How can you design your UX for Human-In-The-Loop in LLM applications?",
      "answer": "By having the LLM suggest categories upfront, we reduce cognitive load on the user and they don't have to learn our taxonomy to categorize their product! At the same time, by allowing the user to review and edit the suggestion, they have the final say in how their product is classified, putting control firmly in their hands. As a bonus, the third approach creates a natural feedback loop for model improvement. Suggestions that are good are accepted (positive labels) and those that are bad are updated (negative followed by positive labels).",
      "document_url": "https://applied-llms.org/"
    },
    {
      "id": "q082",
      "query": "What is the main challenge with quantizing transformers at scale?",
      "answer": "So it turns out, that transformers have these emergent features that have very large values. They occur in particular hidden dimensions and are active in up to 75% of all sequence dimensions. They occur in all layers (well most layers, but we come to that). So if you have a transformer hidden state X of dimensionality [batch, sequence, hidden], then X[:, :, i] for some i have values that look like this:\n\n[-60.. -45, -51, -35, -20, -67]\n\nWhereas 99.9% of dimensions look like this (normally distributed with one outlier)\n\n[-0.10, -0.23,  0.08, -0.38, -0.28, -0.29, -2.11,  0.34, -0.53, -67.0]",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q083",
      "query": "How does vector-wise quantization work for matrix multiplication?",
      "answer": "We can see a matrix multiplication as a sequence of independent inner products between row vectors of A and column vectors of B. We can have a separate constant for each of these vectors. Denormalization happens by multiplying these two constants together for a particular element. No other computation is needed. This is vector-wise quantization.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q084",
      "query": "What is the key idea behind mixed precision decomposition for quantization?",
      "answer": "As such, we can separate these emergent features into a separate, high precision matrix multiplication, quantize the other 99.9% of values to Int8, can combine the output of both matrix multiplications. This avoids the information squishing to zero effect, and we can recover full transformer performance.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q085",
      "query": "At what model size does the phase shift for emergent features occur in transformers?",
      "answer": "The phase shift happens around 6.7B, where 100% of layers use the same dimension for outliers.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q086",
      "query": "How do emergent features affect attention layers in large transformers?",
      "answer": "Attention layers become very sparse. The attention is very concentrated so that just a few sequence dimensions determine the top probability and the overall probability mass. Almost all sequence dimensions have zero probability. However, this is still context-dependent, and the transformer seems to be \"unsure\" what to attend to for some sequences.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q087",
      "query": "What happens to FFN layers in transformers after the emergence of outlier features?",
      "answer": "FFN layers become more \"dense\". While in computer vision, you can prune about 95% of weights without severe performance degradation, that number is 30% for transformers trained on NLP data. After emergence, this number shrinks to well below 5%. It seems that canceling out features can remove noise that is generated from the many weak features that are activated. Because these are silenced now, each set of neurons can learn much more features that are almost independent of each other due to the masking of context-dependent features.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q088",
      "query": "How does the LLM.int8() method achieve full performance recovery in quantization?",
      "answer": "We can recover full performance by using the LLM.int8() quantization procedure. You can clearly see that there is a big dip in performance for the 8-bit baseline, which is vector-wise quantization. We need both vector-wise quantization and mixed precision decomposition, that is, the full LLM.int8() method to recover full performance. Either of these methods alone is not sufficient.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q089",
      "query": "What is the intuitive explanation for feature outliers in transformers?",
      "answer": "The most intuitive explanation of feature outliers is that transformers have two processing streams. One stream learns features that explain the inputs, and the other stream learns features that remove other features. Removing noisy, context-irrelevant features is the key to making accurate predictions. The more noisy, context-irrelevant features you remove in early layers, the less conflicting high-level features you have in later layers.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q090",
      "query": "How do emergent features in transformers relate to discretization?",
      "answer": "If you take this mechanism to an extreme, you can get discretization, which goes hand-in-hand with context-dependent memory and \"reasoning\" over elements. Discretization means, you have, say, 100 features, but you decide to remove 99% of them by setting them to zero, and you amplify the rest. The result is a single feature that is now a discrete entity. Once discretized, this entity can be stored and reused later.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q091",
      "query": "How do transformers coordinate the removal of features across layers?",
      "answer": "To coordinate these streams throughout the transformer, it is useful to dedicate certain hidden dimensions to the functionality of removing other features. That way, if the transformer needs to remove features, it knows beforehand which feature dimension to access to perform that functionality.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q092",
      "query": "What is the mechanism for removing features in transformers?",
      "answer": "How do you remove features? You have a single dimension with very large positive or negative values, and you multiply that dimension with a positive/negative number.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q093",
      "query": "How do emergent features evolve in transformers of different sizes?",
      "answer": "Emergent outlier features are present in even very small transformers (125M parameters), and they do start out in the attention projection layers (key/query/value). Feature outliers are \"consumed\" in the attention function (softmax) and the second fully connected sublayer (contraction layer). The outlier features are likely consumed in these layers since the second feedforward network (FFN) sub-layer, and the softmax have non-linear functions that can easily squash features to zero.\n\nOnce you scale transformers a bit more (350M to 1.3B), outliers also occur in the FFN and attention output layers. At this scale, some successive attention layers and FFN layers use the same dimension to coordinate what features to remove. This has synergy. The attention layer is good at context-dependent selection and pattern matching, while the FFN layers are good at globally, context-independent pattern matching.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q094",
      "query": "What happens to outlier features in transformers after the phase shift?",
      "answer": "Outliers become very large quickly. They grow from about 15 for a 6B model to about 60 for a 13B model. OPT-66B has outliers of size around 95, which indicates this growth phase is temporary.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q095",
      "query": "How do emergent features affect the stability of transformer models?",
      "answer": "Transformers become more stable. If you treat the outlier features separately, I believe you can probably run and even train transformers in less than 8-bit precision without degradation in performance.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q096",
      "query": "What is the relationship between emergent features and model perplexity?",
      "answer": "Emergence is not sudden but gradual and grows according to an exponential function related to perplexity and not model size.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q097",
      "query": "How can researchers study emergent properties in smaller models?",
      "answer": "We train multiple smaller models, say, 125M, 350M and 1.3B parameters, and then we measure the emergent property in those models and relate it to the property that we are interested in analyzing, for example, a new architecture or a new from of interpreting models. Once we gathered this data, we can measure how the change in the emergent property changes the results of our new method. With that, we might be able to determine if our new method generalizes to models beyond 6.7B parameters.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q098",
      "query": "What is the significance of the 6.7B parameter threshold in transformer models?",
      "answer": "From these findings it is clear that transformer after the phase shift at 6.7B parameters behave very different to transformers before the phase shift. As such, one should not try to generalize from <6.7B transformers to beyond 6.7B parameters.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q099",
      "query": "How can studying 'scaling laws of emergence' lead to new discoveries in transformer models?",
      "answer": "If we can correlate statistics of a property with increasing capabilities and if this property follows a function that will eventually, \"threshold\", we might have discovered a new emergent property that leads to new capabilities.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q100",
      "query": "What is the main advantage of vector-wise quantization over tensor-wise quantization?",
      "answer": "We can see a matrix multiplication as a sequence of independent inner products between row vectors of A and column vectors of B. We can have a separate constant for each of these vectors. Denormalization happens by multiplying these two constants together for a particular element. No other computation is needed. This is vector-wise quantization. More details in the paper.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q101",
      "query": "How does the coordination of outlier features change as transformer models scale up?",
      "answer": "At the 2.7B to 6B scale, things become much more coordinated. Now 60% of layers agree on which outlier dimension to use.\n\nThe phase shift happens around 6.7B, where 100% of layers use the same dimension for outliers.",
      "document_url": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
    },
    {
      "id": "q102",
      "query": "What was the previous state-of-the-art accuracy on ARC-AGI before 2024?",
      "answer": "The prior state of the art on a similarly difficult dataset was 34% accuracy, so this is a significant improvement.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q103",
      "query": "How does the performance on the ARC-AGI train set typically compare to the test set?",
      "answer": "On a held-out subset of the train set, where humans get 85% accuracy, my solution gets 72% accuracy. (The train set is significantly easier than the test set as noted here.)",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q104",
      "query": "What is a successful approach for achieving high accuracy on ARC-AGI using GPT-4?",
      "answer": "The main idea behind my solution is very simple: get GPT-4o to generate around 8,000 python programs which attempt to implement the transformation, select a program which is right on all the examples (usually there are 3 examples), and then submit the output this function produces when applied to the additional test input(s). I show GPT-4o the problem as images and in various ascii representations.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q105",
      "query": "How does the approach used in a high-performing ARC-AGI solution compare to AlphaCode?",
      "answer": "My approach is similar in spirit to the approach applied in AlphaCode in which a model generates millions of completions attempting to solve a programming problem and then aggregates over them to determine what to submit.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q106",
      "query": "What are the key steps in a method that achieved high performance on ARC-AGI using GPT-4?",
      "answer": "At a high level, the method I use is:\n\n- Provide the ARC-AGI problem to GPT-4o, with both an image representation and with various text representations for each grid in the problem. The text representations include showing which cells are occupied by different connected components of colors and showing diffs between the input and output (in cases where the grid shapes are the same).\n\n- Instruct GPT-4o to reason about what the transformation is, reason how to implement the transformation as code, and then finally actually implement the transformation in code.\n\n- Use a few-shot prompt with several carefully handwritten examples of step-by-step reasoning to actually get GPT-4o to do this reasoning somewhat effectively. The resulting prompt is usually around 30k tokens long including images.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q107",
      "query": "How many completions are typically sampled for each ARC-AGI problem in a high-performing approach?",
      "answer": "Sample vast, vast numbers of completions (~5,000 per problem) from GPT-4o.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q108",
      "query": "What technique can be used to improve the most promising completions in ARC-AGI solutions?",
      "answer": "Take the most promising 12 completions for each problem, and then try to fix each by showing GPT-4o what this program actually outputs on the examples, and then asking GPT-4o to revise the code to make it correct. We sample ~3,000 completions that attempt to fix per problem in total across these 12 starting implementations.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q109",
      "query": "How are the final submissions for each ARC-AGI problem typically selected in high-performing solutions?",
      "answer": "Then, we select 3 submissions to make based on a majority vote over programs which get the examples correct. (In the case where we don't have 3 distinct submissions from programs which get the examples right, we apply some heuristics to pick a submission, but this doesn't matter much.)",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q110",
      "query": "What is the relationship between the number of samples and the accuracy on ARC-AGI?",
      "answer": "There appears to be a relatively clean scaling law. Of course, a linear fit from log(k) to accuracy can't go on forever as it would imply you eventually go above 100% accuracy!",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q111",
      "query": "How much does accuracy typically improve with each doubling of samples in ARC-AGI solutions?",
      "answer": "The fit is in terms of log base 2. So, it indicates an additional 3% correct per doubling of k.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q112",
      "query": "What are the main limitations of GPT-4 in solving ARC-AGI problems?",
      "answer": "GPT-4o is limited by failures other than reasoning, such as:\n\n1. GPT-4o's vision is terrible on grids. When asked to describe what is in a somewhat large grid, it often fails to \"see\" the input correctly, and states wrong facts about what colors are in some location or what shapes are present.\n\n2. GPT-4o isn't that good at coding (especially not for these sort of geometric manipulation problems), and makes simple mistakes like off-by-one errors extremely often.\n\n3. GPT-4o is worse at using long contexts than other models",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q113",
      "query": "How does GPT-4's performance on ARC-AGI compare to human performance?",
      "answer": "To be clear, GPT-4o is also limited by being very dumb. A smart human with only access to the text representation could substantially outperform GPT-4o by spending a bunch of time on each problem.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q114",
      "query": "What evidence suggests that LLMs perform in-context learning on ARC-AGI tasks?",
      "answer": "Contra Chollet, I think that current LLMs are well described as doing at least some useful learning when doing in-context learning.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q115",
      "query": "What predictions have been made about future LLM performance on ARC-AGI?",
      "answer": "60% probability: If a next generation frontier LLM (e.g. GPT-5) was much better at basic visual understanding (e.g. above 85% accuracy on Vibe-Eval hard), using my exact method (with minor adaptation tweaks as needed) on that LLM would surpass typical naive MTurk performance.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q116",
      "query": "Why might ARC-AGI not be an ideal benchmark for evaluating progress towards Transformative AI?",
      "answer": "ARC-AGI probably isn't a good benchmark for evaluating progress towards TAI: substantial \"elicitation\" effort could massively improve performance on ARC-AGI in a way that might not transfer to more important and realistic tasks. I am more excited about benchmarks that directly test the ability of AIs to take the role of research scientists and engineers, for example those that METR is developing.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q117",
      "query": "How could ARC-AGI be improved as an evaluation tool for AI progress?",
      "answer": "More minimally, I think that ARC-AGI would be a better evaluation of progress towards TAI if it used purely text based problems or at least had a text based subset: good vision isn't necessary for TAI and improved vision has outsized effects on ARC-AGI relative to TAI progress.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q118",
      "query": "What are potential risks associated with scaling LLMs, as discussed in relation to ARC-AGI performance?",
      "answer": "TAI poses huge risks. Making mistaken predictions about where LLMs are heading could result in a dramatic underestimate of the dangers they could pose. If, like Mike Knoop (co-host of the ARC-AGI prize), you oppose bills like SB-1047 because you think LLMs won't scale, then it really matters that you are right about LLMs not scaling. And every time you get evidence that indicates that scaling might be dangerously powerful (and I hope this post provided some), you should update appropriately in favor of more caution.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q119",
      "query": "How does the compute usage in recent high-performing ARC-AGI solutions compare to prior work?",
      "answer": "I used over 1000x more runtime compute per problem than prior work on this benchmark. Maybe prior work on this benchmark scales well with compute and would have gotten higher accuracy with higher resources.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q120",
      "query": "What are some potential improvements for ARC-AGI solutions using GPT-4?",
      "answer": "Some tricks I don't use that might improve performance:\n\n- I currently spend the same number of samples on each problem. You could terminate early on problems which are consistently solved. I think this might reduce cost by perhaps 25-35%.\n\n- In the OpenAI API, I use n < 128 (typically 32 or 16) because n=128 typically errors from what I've seen. Currently it seems like about half of my cost is input tokens, so going to n=128 would roughly halve the cost.\n\n- It would probably help to divide the problems into substantially more categories and then build specialized prompts and tools for each category. This somewhat defeats the point of ARC-AGI though and I'm not sure what these categories would be.\n\n- Doing a second or third revision round could help substantially. (Relative to spending these samples elsewhere.)\n\n- Further extending the debugging/revision process could help substantially.\n\n- Fine-tuning of GPT-4o to better understand the representations I use (and be able to see) would surely help a bunch (though it would be expensive).",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q121",
      "query": "In the Redwood Research blog post about achieving 50% accuracy on ARC-AGI, what was the main approach used?",
      "answer": "The main idea behind my solution is very simple: get GPT-4o to generate around 8,000 python programs which attempt to implement the transformation, select a program which is right on all the examples (usually there are 3 examples), and then submit the output this function produces when applied to the additional test input(s). I show GPT-4o the problem as images and in various ascii representations.",
      "document_url": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
    },
    {
      "id": "q122",
      "query": "What is the fundamental action of attention heads in transformer models?",
      "answer": "The fundamental action of attention heads is moving information. They read information from the residual stream of one token, and write it to the residual stream of another token.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q123",
      "query": "How can attention heads in transformers be represented mathematically using tensor products?",
      "answer": "Using tensor products, we can describe the process of applying attention as:\n\nh(x) = (Id ⊗ W ) ⋅\nO\n(A ⊗ Id) ⋅\n(Id ⊗ W ) ⋅\nV\nx",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q124",
      "query": "What are the three ways attention heads can compose in transformer models?",
      "answer": "When attention heads do compose, there are three options:\n\nQ-Composition: W reads in a subspace affected by a previous head.\nK-Composition: W reads in a subspace affected by a previous head.\nV-Composition: W reads in a subspace affected by a previous head.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q125",
      "query": "What is the purpose of the 'path expansion trick' in analyzing transformer models?",
      "answer": "Our key trick is to simply expand the product. This transforms the product (where every term corresponds to a layer), into a sum where every term corresponds to an end-to-end path.\n\nWe claim each of these end-to-end path terms is tractable to understand, can be reasoned about independently, and additively combine to create model behavior.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q126",
      "query": "What are 'induction heads' in transformer models and how do they function?",
      "answer": "Induction heads search over the context for previous examples of the present token. If they don't find it, they attend to the first token (in our case, a special token placed at the start), and do nothing. But if they do find it, they then look at the next token and copy it. This allows them to repeat previous sequences of tokens, both exactly and approximately.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q127",
      "query": "How does the 'freezing attention patterns trick' help in understanding transformer models?",
      "answer": "One thought experiment which might be helpful is to imagine running the model twice. The first time you collect the attention patterns of each head. This only depends on the QK circuit. The second time, you replace the attention patterns with the \"frozen\" attention patterns you collected the first time. This gives you a function where the logits are a linear function of the tokens! We find this a very powerful way to think about transformers.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q128",
      "query": "What is the difference between Q/K-Composition and V-Composition in transformer attention heads?",
      "answer": "Q- and K-Composition are quite different from V-Composition. Q- and K-Composition both affect the attention pattern, allowing attention heads to express much more complex patterns. V-Composition, on the other hand, affects what information an attention head moves when it attends to a given position; the result is that V-composed heads really act more like a single unit and can be thought of as creating an additional \"virtual attention heads\".",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q129",
      "query": "How do 'virtual attention heads' form in transformer models?",
      "answer": "Virtual attention heads were the terms of the form (A A ) ⊗ (… W W …) in the path expansion of the logit equation, corresponding to the V-composition of two heads.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q130",
      "query": "What is the 'residual stream' in transformer models and why is it important?",
      "answer": "We generally think of the residual stream as a communication channel, since it doesn't do any processing itself and all layers communicate through it.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q131",
      "query": "How do 'skip-trigrams' function in one-layer attention-only transformers?",
      "answer": "One-layer attention-only transformers can be understood as an ensemble of a bigram model and several \"skip-trigram\" models (affecting the probabilities of sequences \"A… BC\"). Intuitively, this is because each attention head can selectively attend from the present token (\"B\") to a previous token (\"A\") and copy information to adjust the probability of possible next tokens (\"C\").",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q132",
      "query": "What is the 'direct path term' in transformer models and what does it represent?",
      "answer": "The direct path term, Id ⊗ W W , also occurred when we looked at the zero-layer transformer. Because it doesn't move information between positions (that's what Id denotes!), the only thing it can contribute to is the bigram statistics, and it will fill in missing gaps that other terms don't handle there.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q133",
      "query": "How do transformer models handle 'pointer arithmetic' with positional embeddings?",
      "answer": "Transformers can do \"pointer arithmetic\" type operations on positional embeddings. We observed at least one case in GPT-2 where an induction head was implemented using this approach, instead of the one described above. First, an attention head attended to a similar token, returning its positional embedding. Next, a query vector for another attention head was constructed with q-composition, rotating the positional embedding forward one token. This results in an induction head.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q134",
      "query": "What is the 'OV circuit' in transformer attention heads and what does it represent?",
      "answer": "W W W — We call this matrix the \"Output-Value (OV) circuit.\" It describes how a given token will affect the output logits if attended to.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q135",
      "query": "How do transformer models implement 'copying' behavior?",
      "answer": "One of the most striking things about looking at these matrices is that most attention heads in one layer models dedicate an enormous fraction of their capacity to copying. The OV circuit sets things up so that tokens, if attended to by the head, increase the probability of that token, and to a lesser extent, similar tokens. The QK circuit then only attends back to tokens which could plausibly be the next token. Thus, tokens are copied, but only to places where bigram-ish statistics make them seem plausible.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q136",
      "query": "What are 'bottleneck activations' in transformer models?",
      "answer": "We say that an activation is a bottleneck activation if it is a lower-dimensional intermediate between two higher dimensional activations. For example, the residual stream is a bottleneck activation because it is the only way to pass information between MLP activations, which are typically four times larger than it.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q137",
      "query": "How can the eigenvalue summary statistic be used to detect copying behavior in transformer attention heads?",
      "answer": "One natural approach might be to use eigenvectors and eigenvalues. Recall that v is an eigenvector of the matrix M with an eigenvalue λ if Mv = λv. Let's consider what that means for an OV circuit M = W W W if λ is a positive real number. Then we're saying that there's a linear combination of tokens which increases the linear combination of logits of those same tokens. Very roughly you could think of this as a set of tokens (perhaps all tokens representing plural words for a very broad one, or all tokens starting with a given first letter, or all tokens representing different capitalizations and inclusions of space for a single word for a narrow one) which mutually increase their own probability.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q138",
      "query": "What is the 'QK circuit' in transformer attention heads and how does it function?",
      "answer": "W W W — We call this matrix the \"query-key (QK) circuit.\" It provides the attention score for every query and key token. That is, each entry describes how much a given query token \"wants\" to attend to a given key token.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q139",
      "query": "How do transformer models handle layer normalization and what are its implications for interpretability?",
      "answer": "Layer normalization is a little more subtle. For each residual stream vector, we subtract off the average activation, normalize by variance, and then multiply by a learned set of diagonal weights and add a learned bias vector. It turns out that subtracting off the average activation is a fixed linear transformation, since it just zeros out a single dimension in the vector space. This means that everything except for normalizing by variance, layer normalization applies a fixed affine transformation. Normalizing by variance multiplies the vector by a scalar, and multiplying by a scalar commutes with all the other operations in a path. As a result, we can fold everything but normalization into adjacent parameters, and then think of the normalization scaling as a variable reweighting of the set of path terms going through that layer normalization.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q140",
      "query": "What is the 'term importance analysis' technique used in studying transformer models?",
      "answer": "But it turns out there's an algorithm which can determine the marginal effect of ablating the nth order terms (that is, the terms corresponding to paths through V-composition of n attention heads). The key trick is to run the model multiple times, replacing the present activations with activations from previous times you ran the model. This allows one to limit the depth of path, ablating all terms of order greater than that. Then by taking differences between the observed losses for each ablation, we can get the marginal effect of the nth order terms.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q141",
      "query": "How does the 'privileged basis' concept apply to different activations in transformer models?",
      "answer": "Privileged Basis vs Basis Free: A privileged basis occurs when some aspect of a model's architecture encourages neural network features to align with basis dimensions, for example because of a sparse activation function such as ReLU. In a transformer, the only vectors with privileged bases are tokens, attention patterns and MLP activations.",
      "document_url": "https://transformer-circuits.pub/2021/framework/index.html"
    },
    {
      "id": "q142",
      "query": "What are the main challenges of scaling embeddings for production use cases?",
      "answer": "However, embeddings may be challenging to scale for production use cases, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in float32, i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory!",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q143",
      "query": "How does Matryoshka Representation Learning (MRL) help with embedding efficiency?",
      "answer": "With MRL, only the first n embedding dimensions are used. This approach has already been adopted by some open models like nomic-ai/nomic-embed-text-v1.5 and mixedbread-ai/mxbai-embed-2d-large-v1 (For OpenAIs text-embedding-3-large, we see a performance retention of 93.1% at 12x compression. For nomic's model, we retain 95.8% of performance at 3x compression and 90% at 6x compression.).",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q144",
      "query": "What is embedding quantization and how does it differ from dimensionality reduction?",
      "answer": "However, there is another new approach to achieve progress on this challenge; it does not entail dimensionality reduction, but rather a reduction in the size of each of the individual values in the embedding: Quantization. Our experiments on quantization will show that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q145",
      "query": "How does binary quantization work for embeddings?",
      "answer": "To quantize float32 embeddings to binary, we simply threshold normalized embeddings at 0:\n\n[[[$$f(x) = \\left\\{ \begin{matrix}\n0 & {\text{if~}x \\leq 0} \\\n1 & {\text{if~}x > 0}\n\\end{matrix} \right.$$]{.katex-mathml}",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q146",
      "query": "What is the Hamming Distance and why is it useful for binary embeddings?",
      "answer": "We can use the Hamming Distance to retrieve these binary embeddings efficiently. This is the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings; thus, the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q147",
      "query": "How does rescoring improve the performance of binary embeddings?",
      "answer": "Yamada et al. (2021) introduced a rescore step, which they called rerank, to boost the performance. They proposed that the float32 query embedding could be compared with the binary document embeddings using dot-product. In practice, we first retrieve rescore_multiplier * top_k results with the binary query embedding and the binary document embeddings -- i.e., the list of the first k results of the double-binary retrieval -- and then rescore that list of binary document embeddings with the float32 query embedding.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q148",
      "query": "What are the benefits of using binary quantization for embeddings?",
      "answer": "By applying this novel rescoring step, we are able to preserve up to ~96% of the total retrieval performance, while reducing the memory and disk space usage by 32x and improving the retrieval speed by up to 32x as well. Without the rescoring, we are able to preserve roughly ~92.5% of the total retrieval performance.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q149",
      "query": "How does scalar quantization differ from binary quantization for embeddings?",
      "answer": "We use a scalar quantization process to convert the float32 embeddings into int8. This involves mapping the continuous range of float32 values to the discrete set of int8 values, which can represent 256 distinct levels (from -128 to 127), as shown in the image below. This is done by using a large calibration dataset of embeddings. We compute the range of these embeddings, i.e., the min and max of each embedding dimension. From there, we calculate the steps (buckets) to categorize each value.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q150",
      "query": "How does the size reduction of scalar quantization compare to binary quantization?",
      "answer": "With scalar quantization to int8, we reduce the original float32 embeddings' precision so that each value is represented with an 8-bit integer (4x smaller). Note that this differs from the binary quantization case, where each value is represented by a single bit (32x smaller).",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q151",
      "query": "What is the importance of the calibration dataset in scalar quantization?",
      "answer": "To further boost the retrieval performance, you can optionally apply the same rescoring step as for the binary embeddings. It is important to note that the calibration dataset greatly influences performance since it defines the quantization buckets.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q152",
      "query": "How can you combine binary and scalar quantization for efficient retrieval?",
      "answer": "Combining binary and scalar quantization is possible to get the best of both worlds: the extreme speed from binary embeddings and the great performance preservation of scalar embeddings with rescoring. See the demo below for a real-life implementation of this approach involving 41 million texts from Wikipedia. The pipeline for that setup is as follows:\n\n1.  The query is embedded using the mixedbread-ai/mxbai-embed-large-v1 SentenceTransformer model.\n2.  The query is quantized to binary using the quantize_embeddings function from the sentence-transformers library.\n3.  A binary index (41M binary embeddings; 5.2GB of memory/disk space) is searched using the quantized query for the top 40 documents.\n4.  The top 40 documents are loaded on the fly from an int8 index on disk (41M int8 embeddings; 0 bytes of memory, 47.5GB of disk space).\n5.  The top 40 documents are rescored using the float32 query and the int8 embeddings to get the top 10 documents.\n6.  The top 10 documents are sorted by score and displayed.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q153",
      "query": "How does the resource usage of combined binary and scalar quantization compare to normal retrieval?",
      "answer": "Through this approach, we use 5.2GB of memory and 52GB of disk space for the indices. This is considerably less than normal retrieval, requiring 200GB of memory and 200GB of disk space. Especially as you scale up even further, this will result in notable reductions in latency and costs.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q154",
      "query": "What is the effect of binary quantization on retrieval performance for different embedding models?",
      "answer": "Interestingly, we can also see that all-MiniLM-L6-v2 exhibits stronger performance on binary than on int8 quantization. A possible explanation for this could be the selection of calibration data. On e5-base-v2, we observe the effect of dimension collapse, which causes the model to only use a subspace of the latent space; when performing the quantization, the whole space collapses further, leading to high performance losses.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q155",
      "query": "How does the rescore multiplier affect the performance retention of int8 quantization?",
      "answer": "As we can see from the diagram, a higher rescore multiplier implies better retention of performance after quantization. Extrapolating from our results, we assume the relation is likely hyperbolical with performance approaching 100% as the rescore multiplier continues to rise. A rescore multiplier of 4-5 already leads to a remarkable performance retention of 99% using int8.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q156",
      "query": "What are the average speedups achieved by int8 and binary quantization in retrieval?",
      "answer": "As shown in the table, applying int8 scalar quantization results in an average speedup of 3.66x compared to full-size float32 embeddings. Additionally, binary quantization achieves a speedup of 24.76x on average. For both scalar and binary quantization, even the worst case scenario resulted in very notable speedups.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q157",
      "query": "What potential improvements are suggested for future work in embedding quantization?",
      "answer": "We are looking forward to further advancements of binary quantization. To name a few potential improvements, we suspect that there may be room for scalar quantization smaller than int8, i.e. with 128 or 64 buckets instead of 256.\n\nAdditionally, we are excited that embedding quantization is fully perpendicular to Matryoshka Representation Learning (MRL). In other words, it is possible to shrink MRL embeddings from e.g. 1024 to 128 (which usually corresponds with a 2% reduction in performance) and then apply binary or scalar quantization. We suspect this could speed up retrieval up to 32x for a 3% reduction in quality, or up to 256x for a 10% reduction in quality.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q158",
      "query": "How can embedding quantization be combined with reranker models for improved retrieval performance?",
      "answer": "Lastly, we recognize that retrieval using embedding quantization can also be combined with a separate reranker model. We imagine that a 3-step pipeline of binary search, scalar (int8) rescoring, and cross-encoder reranking allows for state-of-the-art retrieval performance at low latencies, memory usage, disk space, and costs.",
      "document_url": "https://huggingface.co/blog/embedding-quantization"
    },
    {
      "id": "q159",
      "query": "What are the three key levers in developing high-quality foundation models according to the Llama 3 team?",
      "answer": "We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process:",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q160",
      "query": "How many tokens was Llama 3 pre-trained on compared to Llama 2?",
      "answer": "We pre-train Llama 3 on a corpus of about 15T multilingual tokens, compared to 1.8T tokens for Llama 2.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q161",
      "query": "What is the purpose of using micro-batching during inference with pipeline parallelism?",
      "answer": "We use micro-batching to improve inference throughput with pipeline parallelism.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q162",
      "query": "How does Llama 3 handle long context pre-training?",
      "answer": "In the final stages of pre-training, we train on long sequences to support context windows of up to 128K tokens. We do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length. We increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. We assess successful adaptation by measuring whether (1) model performance on short-context evaluations has recovered completely and (2) the model perfectly solves \"needle in a haystack\" tasks up to that length.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q163",
      "query": "What is the purpose of the annealing phase in Llama 3's training?",
      "answer": "During pre-training on the final 40M tokens, we linearly annealed the learning rate to 0, maintaining a context length of 128K tokens. During this annealing phase, we also adjusted the data mix to upsample data sources of very high quality; see Section 3.1.3. Finally, we compute the average of model checkpoints (Polyak (1991) averaging) during annealing to produce the final pre-trained model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q164",
      "query": "How does Llama 3 handle the trade-off between violation rate and false refusal rate in its safety measures?",
      "answer": "While a low violation rate is desirable, it is critical to consider false refusal as a counter-metric, as a model that always refuses is maximally safe, but not helpful in the slightest. Similarly, a model that always answers every prompt, regardless of how problematic the request, would be overly harmful and toxic. In Figure 21, leveraging our internal benchmarks, we explore how different models and systems in industry navigate this trade off and how Llama 3 compares. We find that our models achieve very competitive violation rate metrics while keeping false refusal rate low as well, indicating a solid balance between helpfulness and safety.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q165",
      "query": "What is the purpose of the Prompt Guard in Llama 3's system-level safety components?",
      "answer": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to subvert the intended behavior of an LLM functioning as part of an application. The model is a multi-label classifier that detects two classes of prompt attack risk - direct jailbreaks (techniques that explicitly try to override a model's safety conditioning or system prompt) and indirect prompt injections (instances where third-party data included in a model's context window includes instructions inadvertently executed as user commands by an LLM).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q166",
      "query": "How does Llama 3 handle multilingual safety?",
      "answer": "Our experiments demonstrate that safety knowledge in English does not readily transfer to other languages, particularly given the nuance of safety policies and language-specific context. Therefore, it is essential to collect high-quality safety data for each language. We also found that the distribution of safety data per language significantly impacts performance from a safety standpoint, with some languages benefiting from transfer learning while others require more language-specific data. To achieve a balance between FRR and VR, we iteratively add adversarial and borderline data while monitoring the impact on both metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q167",
      "query": "What is the purpose of the speech adapter in Llama 3's architecture?",
      "answer": "The speech adapter contains about 100M parameters. It is composed of a convolution layer, a rotary Transformer layer, and a linear layer. The convolution layer has a kernel size of 3 and a stride of 2, which is designed to reduce the speech frame length to 80ms. This allows the model to provide more coarse-grained features to the language model. The Transformer layer has a latent dimension of 3072 and a feed-forward network with a dimension of 4096 which further processes the information from speech with context after the convolutional downsampling. Finally, the linear layer maps the output dimension to match that of the language-model embedding layer.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q168",
      "query": "How does Llama 3 handle speech recognition for multiple languages?",
      "answer": "Multilingual ASR and AST modeling often results in language confusion/interference, which leads to degraded performance. A popular way to mitigate this is to incorporate language identification (LID) information, both on the source and target side. This can lead to improved performance in the predetermined set of directions, but it does come with potential loss of generality. For instance, if a translation system expects LID on both source and target side, then the model will not likely to show good zero-shot performance in directions that were not seen in training. So our challenge is to design a system that allows LID information to some extent, but keeps the model general enough such that we can have the model do speech translation in unseen directions. To address this, we design system prompts which only contain LID for the text to be emitted (target side). There is no LID information for the speech input (source side) in these prompts, which also potentially allows it to work with code-switched speech.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q169",
      "query": "What is the purpose of the image adapter in Llama 3's architecture?",
      "answer": "We introduce cross-attention layers between the visual token representations produced by the image encoder and the token representations produced by the language model (Alayrac et al., 2022). The cross-attention layers are applied after every fourth self-attention layer in the core language model. Like the language model itself, the cross-attention layers use generalized query attention (GQA) for increased efficiency. The cross-attention layers introduce substantial numbers of additional trainable parameters into the model: for Llama 3 405B, the cross-attention layers have ≈100B parameters.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q170",
      "query": "How does Llama 3 handle the challenge of model heterogeneity during training?",
      "answer": "The model computation is heterogeneous because more computation is performed on some tokens than on others. In particular, image tokens are processed by the image encoder and the cross-attention layers, whereas text tokens are only processed by the language backbone. This heterogeneity leads to bottlenecks in the scheduling of pipeline parallelism. We address this problem by ensuring each pipeline stage contains five layers: namely, four self-attention layers in the language backbone and a cross-attention layer. (Recall that we introduce a cross-attention layer after every fourth self-attention layer.) In addition, we replicate the image encoder on all pipeline stages. Because we train on paired image-text data, this enables us to perform load balancing between the image and text parts of the computation.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q171",
      "query": "What is the purpose of the video adapter in Llama 3's architecture?",
      "answer": "Our model takes as input up to 64 frames (uniformly sampled from a full video), each of which are processed by the image encoder. We model temporal structure in videos through two components: (i) encoded video frames are aggregated by a temporal aggregator which merges 32 consecutive frames into one, (ii) additional video cross attention layers are added before every fourth image cross attention layer. The temporal aggregator is implemented as a perceiver resampler (Jaegle et al., 2021; Alayrac et al., 2022). We pre-train using 16 frames per video (aggregated to 1 frame), but increase the number of input frames to 64 during supervised finetuning.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q172",
      "query": "How does Llama 3 handle the challenge of data heterogeneity during training?",
      "answer": "The data is heterogeneous because, on average, images have more tokens than the associated text: an image has 2,308 tokens, whereas the associated text contains an average of only 192 tokens. As a result, the computation of cross-attention layers requires more time and memory than the computation of self-attention layers. We address this problem by introducing sequence parallelization in the image encoder, so that each GPU processes roughly the same number of tokens. Because the average text size is relatively short, we also use a substantially larger micro-batch size (8 instead of 1).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q173",
      "query": "What is the purpose of the text normalization module in Llama 3's speech generation process?",
      "answer": "As a determinant of the semantic correctness of generated speech, the text normalization (TN) module carries out context-aware transformation from written-form text into the respective spoken form which is eventually verbalized by the downstream components. For example, the written-form text 123 is read as a cardinal number (one hundred twenty three) or spelled digit-by-digit (one two three) depending on the semantic context. The TN system consists of a streaming LSTM-based sequence-tagging model that predicts the sequence of handcrafted TN rules used to transform the input text (Kang et al., 2024). The neural model also takes in Llama 3 embeddings via cross attention to leverage the contextual information encoded therein, enabling minimal text token lookahead and streaming input/output.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q174",
      "query": "How does Llama 3 handle the challenge of numerical instabilities during training?",
      "answer": "After the image encoder is added to the model, we find that performing gradient accumulation in bf16 led to numerical instabilities. The most likely explanation for this is that image tokens are introduced into the language backbone via all cross-attention layers. This implies that numerical deviations in the representation of an image token have an outsized impact on the overall computation because the errors are compounded. We address this by performing gradient accumulation in FP32.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q175",
      "query": "What is the purpose of the prosody model in Llama 3's speech generation process?",
      "answer": "To enhance the naturalness and expressiveness of synthesized speech, we integrate a decoder-only Transformer-based Prosody model (PM) (Radford et al., 2021) that takes the Llama 3 embeddings as an additional input. This integration leverages the linguistic capabilities of Llama 3, utilizing both its textual output and intermediate embeddings at the token rate (Devlin et al., 2018; Dong et al., 2019; Raffel et al., 2020; Guo et al., 2023) to enhance the prediction of prosody features, thus reducing the lookahead required by the model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q176",
      "query": "How does Llama 3 handle the challenge of long-context safety?",
      "answer": "Long-context models are vulnerable to many-shot jailbreaking attacks without targeted mitigation (Anil et al., 2024). To address this, we finetune our models on SFT datasets that include examples of safe behavior in the presence of demonstrations of unsafe behavior in context. We develop a scalable mitigation strategy that significantly reduces VR, effectively neutralizing the impact of longer context attacks even for 256-shot attacks. This approach shows little to no impact on FRR and most helpfulness metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q177",
      "query": "What is the purpose of the Code Shield in Llama 3's system-level safety components?",
      "answer": "Code Shield is an example of a class of system-level protections based on providing inference-time filtering. In particular, it focuses on detecting the generation of insecure code before it might enter a downstream usecase such as a production system. It does so by leveraging a static analysis library, the Insecure Code Detector (ICD), to identify insecure code. ICD uses a suite of static analysis tools to perform the analysis across 7 programming languages. These kinds of guardrails are generally useful for developers, who can deploy multi-layered protections in various applications.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q178",
      "query": "How does Llama 3 handle the challenge of tool usage safety?",
      "answer": "The diversity of possible tools and the implementation of the tool usage call and integration into the model make tool usage a challenging capability to fully mitigate (Wallace et al., 2024). We focus on the search usecase. Violation and false refusal rates are shown in Figure 20. We tested against the Comp. 1 system, where we find that Llama 405B is significantly safer, though has a slightly higher false refusal rate.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q179",
      "query": "What are the key factors in developing high-quality foundation models?",
      "answer": "We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process:",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q180",
      "query": "How can numerical instabilities be addressed when training large multimodal models?",
      "answer": "After the image encoder is added to the model, we find that performing gradient accumulation in bf16 led to numerical instabilities. The most likely explanation for this is that image tokens are introduced into the language backbone via all cross-attention layers. This implies that numerical deviations in the representation of an image token have an outsized impact on the overall computation because the errors are compounded. We address this by performing gradient accumulation in FP32.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q181",
      "query": "What is the purpose of annealing in language model training?",
      "answer": "During pre-training on the final 40M tokens, we linearly annealed the learning rate to 0, maintaining a context length of 128K tokens. During this annealing phase, we also adjusted the data mix to upsample data sources of very high quality; see Section 3.1.3. Finally, we compute the average of model checkpoints (Polyak (1991) averaging) during annealing to produce the final pre-trained model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q182",
      "query": "How can long-context capabilities be added to a language model?",
      "answer": "In the final stages of pre-training, we train on long sequences to support context windows of up to 128K tokens. We do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length. We increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. We assess successful adaptation by measuring whether (1) model performance on short-context evaluations has recovered completely and (2) the model perfectly solves \"needle in a haystack\" tasks up to that length.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q183",
      "query": "What is the trade-off between violation rate and false refusal rate in AI safety?",
      "answer": "While a low violation rate is desirable, it is critical to consider false refusal as a counter-metric, as a model that always refuses is maximally safe, but not helpful in the slightest. Similarly, a model that always answers every prompt, regardless of how problematic the request, would be overly harmful and toxic. In Figure 21, leveraging our internal benchmarks, we explore how different models and systems in industry navigate this trade off and how Llama 3 compares. We find that our models achieve very competitive violation rate metrics while keeping false refusal rate low as well, indicating a solid balance between helpfulness and safety.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q184",
      "query": "How can prompt attacks be detected in language models?",
      "answer": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to subvert the intended behavior of an LLM functioning as part of an application. The model is a multi-label classifier that detects two classes of prompt attack risk - direct jailbreaks (techniques that explicitly try to override a model's safety conditioning or system prompt) and indirect prompt injections (instances where third-party data included in a model's context window includes instructions inadvertently executed as user commands by an LLM).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q185",
      "query": "Why is multilingual safety challenging in language models?",
      "answer": "Our experiments demonstrate that safety knowledge in English does not readily transfer to other languages, particularly given the nuance of safety policies and language-specific context. Therefore, it is essential to collect high-quality safety data for each language. We also found that the distribution of safety data per language significantly impacts performance from a safety standpoint, with some languages benefiting from transfer learning while others require more language-specific data. To achieve a balance between FRR and VR, we iteratively add adversarial and borderline data while monitoring the impact on both metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q186",
      "query": "What is the purpose of a speech adapter in multimodal AI models?",
      "answer": "The speech adapter contains about 100M parameters. It is composed of a convolution layer, a rotary Transformer layer, and a linear layer. The convolution layer has a kernel size of 3 and a stride of 2, which is designed to reduce the speech frame length to 80ms. This allows the model to provide more coarse-grained features to the language model. The Transformer layer has a latent dimension of 3072 and a feed-forward network with a dimension of 4096 which further processes the information from speech with context after the convolutional downsampling. Finally, the linear layer maps the output dimension to match that of the language-model embedding layer.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q187",
      "query": "How can language confusion in multilingual speech recognition be mitigated?",
      "answer": "Multilingual ASR and AST modeling often results in language confusion/interference, which leads to degraded performance. A popular way to mitigate this is to incorporate language identification (LID) information, both on the source and target side. This can lead to improved performance in the predetermined set of directions, but it does come with potential loss of generality. For instance, if a translation system expects LID on both source and target side, then the model will not likely to show good zero-shot performance in directions that were not seen in training. So our challenge is to design a system that allows LID information to some extent, but keeps the model general enoug h such that we can have the model do speech translation in unseen directions. To address this, we design system prompts which only contain LID for the text to be emitted (target side). There is no LID information for the speech input (source side) in these prompts, which also potentially allows it to work with code-switched speech.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q188",
      "query": "What is the role of an image adapter in multimodal AI models?",
      "answer": "We introduce cross-attention layers between the visual token representations produced by the image encoder and the token representations produced by the language model (Alayrac et al., 2022). The cross-attention layers are applied after every fourth self-attention layer in the core language model. Like the language model itself, the cross-attention layers use generalized query attention (GQA) for increased efficiency. The cross-attention layers introduce substantial numbers of additional trainable parameters into the model: for Llama 3 405B, the cross-attention layers have ≈100B parameters.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q189",
      "query": "How can model heterogeneity be addressed in large-scale AI training?",
      "answer": "The model computation is heterogeneous because more computation is performed on some tokens than on others. In particular, image tokens are processed by the image encoder and the cross-attention layers, whereas text tokens are only processed by the language backbone. This heterogeneity leads to bottlenecks in the scheduling of pipeline parallelism. We address this problem by ensuring each pipeline stage contains five layers: namely, four self-attention layers in the language backbone and a cross-attention layer. (Recall that we introduce a cross-attention layer after every fourth self-attention layer.) In addition, we replicate the image encoder on all pipeline stages. Because we train on paired image-text data, this enables us to perform load balancing between the image and text parts of the computation.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q190",
      "query": "What is the purpose of a video adapter in multimodal AI models?",
      "answer": "Our model takes as input up to 64 frames (uniformly sampled from a full video), each of which are processed by the image encoder. We model temporal structure in videos through two components: (i) encoded video frames are aggregated by a temporal aggregator which merges 32 consecutive frames into one, (ii) additional video cross attention layers are added before every fourth image cross attention layer. The temporal aggregator is implemented as a perceiver resampler (Jaegle et al., 2021; Alayrac et al., 2022). We pre-train using 16 frames per video (aggregated to 1 frame), but increase the number of input frames to 64 during supervised finetuning.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q191",
      "query": "How can data heterogeneity be addressed in multimodal AI training?",
      "answer": "The data is heterogeneous because, on average, images have more tokens than the associated text: an image has 2,308 tokens, whereas the associated text contains an average of only 192 tokens. As a result, the computation of cross-attention layers requires more time and memory than the computation of self-attention layers. We address this problem by introducing sequence parallelization in the image encoder, so that each GPU processes roughly the same number of tokens. Because the average text size is relatively short, we also use a substantially larger micro-batch size (8 instead of 1).",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q192",
      "query": "What is the role of text normalization in AI-based speech generation?",
      "answer": "As a determinant of the semantic correctness of generated speech, the text normalization (TN) module carries out context-aware transformation from written-form text into the respective spoken form which is eventually verbalized by the downstream components. For example, the written-form text 123 is read as a cardinal number (one hundred twenty three) or spelled digit-by-digit (one two three) depending on the semantic context. The TN system consists of a streaming LSTM-based sequence-tagging model that predicts the sequence of handcrafted TN rules used to transform the input text (Kang et al., 2024). The neural model also takes in Llama 3 embeddings via cross attention to leverage the contextual information encoded therein, enabling minimal text token lookahead and streaming input/output.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q193",
      "query": "What is the purpose of a prosody model in AI-based speech generation?",
      "answer": "To enhance the naturalness and expressiveness of synthesized speech, we integrate a decoder-only Transformer-based Prosody model (PM) (Radford et al., 2021) that takes the Llama 3 embeddings as an additional input. This integration leverages the linguistic capabilities of Llama 3, utilizing both its textual output and intermediate embeddings at the token rate (Devlin et al., 2018; Dong et al., 2019; Raffel et al., 2020; Guo et al., 2023) to enhance the prediction of prosody features, thus reducing the lookahead required by the model.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q194",
      "query": "How can long-context safety be ensured in large language models?",
      "answer": "Long-context models are vulnerable to many-shot jailbreaking attacks without targeted mitigation (Anil et al., 2024). To address this, we finetune our models on SFT datasets that include examples of safe behavior in the presence of demonstrations of unsafe behavior in context. We develop a scalable mitigation strategy that significantly reduces VR, effectively neutralizing the impact of longer context attacks even for 256-shot attacks. This approach shows little to no impact on FRR and most helpfulness metrics.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q195",
      "query": "What is the purpose of Code Shield in AI safety systems?",
      "answer": "Code Shield is an example of a class of system-level protections based on providing inference-time filtering. In particular, it focuses on detecting the generation of insecure code before it might enter a downstream usecase such as a production system. It does so by leveraging a static analysis library, the Insecure Code Detector (ICD), to identify insecure code. ICD uses a suite of static analysis tools to perform the analysis across 7 programming languages. These kinds of guardrails are generally useful for developers, who can deploy multi-layered protections in various applications.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q196",
      "query": "Why is tool usage safety challenging in large language models?",
      "answer": "The diversity of possible tools and the implementation of the tool usage call and integration into the model make tool usage a challenging capability to fully mitigate (Wallace et al., 2024). We focus on the search usecase. Violation and false refusal rates are shown in Figure 20. We tested against the Comp. 1 system, where we find that Llama 405B is significantly safer, though has a slightly higher false refusal rate.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q197",
      "query": "How can pipeline parallelism be improved in large-scale AI training?",
      "answer": "To address these issues, we modify our pipeline schedule as shown in Figure 6, which allows setting N flexibly—in this case N = 5, which can run a arbitrary number of micro-batches in each batch. This allows us to run: (1) fewer micro-batches than the number of stages when we have batch size limit at large scale; or (2) more micro-batches to hide point-to-point communication, finding a sweet spot between DFS and breadth first schedule (BFS) for the best communication and memory efficiency. To balance the pipeline, we reduce one Transformer layer each from the first and the last stages, respectively. This means that the first model chunk on the first stage has only the embedding, and the last model chunk on the last stage has only output projection and loss calculation.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q198",
      "query": "What are the challenges in scaling up AI model training to thousands of GPUs?",
      "answer": "The complexity and potential failure scenarios of 16K GPU training surpass those of much larger CPU clusters that we have operated. Moreover, the synchronous nature of training makes it less fault-tolerant—a single GPU failure may require a restart of the entire job. Despite these challenges, for Llama 3, we achieved higher than 90% effective training time while supporting automated cluster maintenance, such as firmware and Linux kernel upgrades (Vigraham and Leonhardi, 2024), which resulted in at least one training interruption daily. The effective training time measures the time spent on useful training over the elapsed time.",
      "document_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/"
    },
    {
      "id": "q199",
      "query": "What are the three key factors that strongly influence language model performance?",
      "answer": "Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embeddings), the size of the dataset D, and the amount of compute C used for training.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q200",
      "query": "How does language model performance scale with model size, dataset size, and compute?",
      "answer": "Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance must flatten out eventually before reaching zero loss.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q201",
      "query": "What is the relationship between model size and data requirements to avoid performance penalties?",
      "answer": "Performance improves predictably as long as we scale up N and D in tandem, but enters a regime of diminishing returns if either N or D is held fixed while the other increases. The performance penalty depends predictably on the ratio N 0.74/D, meaning that every time we increase the model size 8x, we only need to increase the data by roughly 5x to avoid a penalty.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q202",
      "query": "How do training curves for language models behave?",
      "answer": "Training curves follow predictable power-laws whose parameters are roughly independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q203",
      "query": "What is the relationship between model size and sample efficiency in language models?",
      "answer": "Large models are more sample-efficient than small models, reaching the same level of performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q204",
      "query": "How should compute be allocated for optimal language model training?",
      "answer": "When working within a fixed compute budget C but without any other restrictions on the model size N or available data D, we attain optimal performance by training very large models and stopping significantly short of convergence (see Figure 3). Maximally compute-efficient training would therefore be far more sample efficient than one might expect based on training small models to convergence, with data requirements growing very slowly as D ∼ C0.27 with training compute.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q205",
      "query": "What is the ideal batch size for training large language models?",
      "answer": "The ideal batch size for training these models is roughly a power of the loss only, and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million tokens at convergence for the largest models we can train.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q206",
      "query": "How does the performance of language models transfer to different text distributions?",
      "answer": "When we evaluate models on text with a different distribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q207",
      "query": "What is the relationship between model architecture and performance in Transformers?",
      "answer": "Performance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section 3)",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q208",
      "query": "How does the critical batch size change as language model performance improves?",
      "answer": "The critical batch size B_crit follows a power law in the loss as performance increase, and does not depend directly on the model size. We find that the critical batch size approximately doubles for every 13% decrease in loss.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q209",
      "query": "What is the optimal allocation of compute between model size and amount of data processed during training?",
      "answer": "Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝ B_crit with negligible increase in the number of serial steps.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q210",
      "query": "How does the performance of LSTMs compare to Transformers for language modeling?",
      "answer": "We see from these figures that the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match the Transformer performance for later tokens.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q211",
      "query": "What is the relationship between overfitting and dataset size in language models?",
      "answer": "Overfitting should be related to the variance or the signal-to-noise ratio of the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function, since we expect to be able to expand the loss about the D → ∞ limit.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q212",
      "query": "How does the loss per token change based on its position in the context for language models?",
      "answer": "Fixing model size, it appears that the loss scales as a power-law as a function of position T in the context, see Figure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12, LT16], or a more general feature of the model architecture and optimization.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q213",
      "query": "What is the relationship between model size and the number of training steps required to reach a given performance level?",
      "answer": "The number of minimum serial steps needed to reach any fixed value of the test loss decreases precipitously with model size. Sample efficiency (show here for training far below the critical batch size) improves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model to a very large one.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q214",
      "query": "How does the choice of learning rate schedule affect the final performance of language models?",
      "answer": "We conclude that the choice of learning rate schedule is mostly irrelevant, as long as the total summed learning rate is sufficiently large, and the schedule includes a warmup period and a final decay to near-vanishing learning rate. Variations among schedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different training runs.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q215",
      "query": "What is the relationship between model depth and generalization to other text distributions?",
      "answer": "We observe no effect of depth on generalization; generalization performance depends primarily on training distribution performance.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q216",
      "query": "How does the performance of recurrent Transformers compare to standard Transformers?",
      "answer": "Recurrent Transformers perform slightly better when comparing models with equal parameter count, but slightly worse when accounting for reuse and comparing per FLOP.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q217",
      "query": "What is the relationship between model size and the early stopping step in language model training?",
      "answer": "We characterize the step on which early stopping occurs, as a function of the extent of overfitting. The red line indicates a lower bound for early stopping that is derived in Section 5.3.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q218",
      "query": "How does the performance of language models trained on short contexts compare to those trained on longer contexts?",
      "answer": "Even modestly sized models trained on n_ctx = 8 can dominate our largest n_ctx = 1024 models on very early tokens. This also suggests that further improvements should be possible with much larger models trained on large contexts.",
      "document_url": "https://arxiv.org/pdf/2001.08361"
    },
    {
      "id": "q219",
      "query": "What is TinyStories and how does it differ from typical language model training datasets?",
      "answer": "We introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q220",
      "query": "How do small language models (SLMs) trained on TinyStories compare to larger models in terms of coherent text generation?",
      "answer": "We show that TinyStories can be used to train and evaluate SLMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q221",
      "query": "What is the GPT-4 based framework for evaluating language models' generated content?",
      "answer": "We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often require the model's output to be very structured, and moreover it provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and instruction-following.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q222",
      "query": "What capabilities are required for a language model to complete a simple sentence about hunger?",
      "answer": "To complete this sentence in a sensible way, the language model needs to know that hunger is a state that motivates people to seek food, and that food is a category of things that can satisfy hunger. It also needs to choose a word that fits the syntactic and semantic constraints of the sentence (such as \"a snack\"), and that is plausible given the situation and the background knowledge.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q223",
      "query": "How does the research demonstrate that small language models can exhibit reasoning capabilities?",
      "answer": "To complete this sentence, the language model needs to invoke reasoning: it needs to apply the principle of disjunction elimination: if Lily wants either a cat or a dog, and she cannot get a dog, then she must choose a cat. It also needs to choose a words that expresses Lily's intention or action that is coherent with the tone and style of the text.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q224",
      "query": "What challenges do small language models (SLMs) typically face in text generation tasks?",
      "answer": "For example, models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate any consistent text beyond a few words even after extensive training on large corpora such as the Pile, Common Crawl or the CC-100. These models often produce incoherent, repetitive, or nonsensical sentences, and fail to maintain a clear topic or a logical structure across paragraphs.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q225",
      "query": "How does the TinyStories dataset address the problem of overwhelming small language models with excessive information?",
      "answer": "This raises the question of whether we can design a dataset that preserves the essential elements of natural language, such as grammar, vocabulary, facts, and reasoning, but that is much smaller and more refined in terms of its breadth and diversity. Such a dataset would allow us to isolate and examine the minimal requirements for a language model to generate coherent and fluent text, and to evaluate its performance and capabilities more precisely and fairly.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q226",
      "query": "What method is used to increase the diversity of the TinyStories dataset?",
      "answer": "In order to address the problem of creating a diverse dataset, we collected a vocabulary consisting of about 1500 basic words, which try to mimic the vocabulary of a typical 3-4 year-old child, separated into nouns, verbs, and adjectives. In each generation, 3 words are chosen randomly (one verb, one noun, and one adjective). The model is instructed to generate a story that somehow combines these random words into the story. As we argue below, this greatly increases the diversity of the dataset, forcing the stories to span the entire vocabulary a child is familiar with, and to include a rich set of ways to combine different concepts.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q227",
      "query": "What types of instructions are included in the TinyStories-Instruct dataset?",
      "answer": "The instructions are of four types:\n1. A list of words to be included in the story.\n2. A sentence that should appear somewhere in the story.\n3. A list of features (possible features: dialogue, bad ending, moral value, plot twist, foreshadowing, conflict).\n4. A short summary (1-2 lines) of the story.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q228",
      "query": "How does the GPT-Eval framework assess the quality of generated stories?",
      "answer": "We provide the model with a story's beginning, taken from a manually-prepared dataset consisting of around 50 prompts, generate a completion using the model, and provide the story's beginning together with the model's completion to GPT-4, asking it to grade the completion assignment in terms of grammar, creativity, and its consistency with the beginning of the story.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q229",
      "query": "What insights about model capabilities can be drawn from the GPT-Eval scores?",
      "answer": "Figure 3 suggests that shallower models perform better in terms of grammar compared to content consistency, meaning that model depth is more important for keeping consistent with the content than for generating syntactically correct language (we provide additional evidence for this in the next section).",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q230",
      "query": "How do the researchers address concerns about the diversity of content generated by small models trained on TinyStories?",
      "answer": "We claim that our models are not doing exact memorization or simple template matching, as evidenced by the methods and metrics we use to evaluate the diversity of the content generated by the models. We rely on several approaches:\n• Manual inspection: We generate completions for a range of human-constructed stories. We inspect the stories generated by the models and check that they are not copies or close modifications of the stories in the dataset.\n• Completion of training stories: We take stories from the training set, truncate them in the middle and generate alternative completions with our models. We then compare the completions with the original stories. We observe that the completions are typically very different from the original stories, and often introduce new characters, events, or twists. This is shown in Figure 14.\n• Diversity of instructions: Recall that in the TinyStories-Instruct dataset, we provide a set of instructions in the form of summaries or words contained in the stories, followed by the stories themselves. We can then change the instructions, verify that the combinations do not appear in the dataset and see how the models adapt to the new instructions. We find that the models can generate diverse stories that follow the instructions, even if they are novel or challenging, such as requiring the model to fit unlikely words into the story or adding features such as a plot twist or a bad ending.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q231",
      "query": "How is the Rouge score used to quantitatively measure the similarity between generated stories and the training dataset?",
      "answer": "We measure the diversity of the stories quantitatively using word and n-gram overlap. We inspect the overlap of words and n-grams between different stories generated by the models, and compare them with the overlap in the dataset. We find that the models' generations have a very low overlap with the dataset, indicating that they are not repeating the same words or phrases. We use the standard Rouge score, for the source text T1, T2 with k-gram Gk(T1), Gk(T2) respectively, the rougek precision score is defined as:\nRk,p(T1, T2) = 1/|Gk(T1)| Σt∈Gk(T1) 1t∈Gk(T2).\nThe Rougek precision score measures how many k-grams in T1 is included in that of T2. The final Rougek score (fmeasure) is given as:\nRk(T1, T2) = 2Rk(T1, T2) × Rk(T2, T1) / (Rk(T1, T2) + Rk(T2, T1)).",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q232",
      "query": "What evidence suggests that models trained on TinyStories have higher interpretability compared to larger models?",
      "answer": "Our findings suggest that the attention heads exhibit diverse and meaningful functions, such as attending to the previous word, the subject of the sentence, the end of the sentence, or the main topic of the story. We also observe that some attention heads specialize in generating certain types of words, such as nouns, verbs, or punctuation. These results suggest that the attention heads learn to perform different linguistic tasks and capture different aspects of the stories.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q233",
      "query": "How do the researchers analyze the roles of different neurons in the MLP of models trained on TinyStories?",
      "answer": "We use the method similar to [18] to identify the most influential tokens in the MLP for each neuron. We find that some neurons are activated on words that have a specific role in the sentence (such as the subject or the action), or in the story (such as the introduction of the protagonist). These findings suggest that the neurons in the MLP learn to encode different semantic and stylistic information and influence the generation process.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q234",
      "query": "What patterns are observed in the attention heads of models trained on TinyStories?",
      "answer": "There seems to be a clear separation between heads with attention pattern based mainly on the distance between tokens, and heads whose attention pattern has a stronger dependence on the semantic meaning:\nDistance based attention. Out of the 16 attention heads, we observe multiple positional-based attention heads, such that each token attends to tokens with a prescribed relative distance. Different heads are associated with different distances.\nSemantic based attention. We also observe that there is (1). one head that the word \"the\" and \"a\" all attend to the word \"banana\", interestingly, the \"the\" at \"the park\" also attends to \"banana\", but the model still manage to generate \"park\", which is the consistent completion. (2). Another attention head gives a pattern where the tokens \"the\" and \"a\" all attend to \"park\". (3). There is third head that most of the words attend to the name of \"Tom\" and \"Lucy\".",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q235",
      "query": "What scaling law is observed between model size and learning budget for a fixed amount of training flops?",
      "answer": "Previous works [16, 11] have shown that there is a polynomial scaling law between model size and learning budget for LLMs, i.e., the optimal model size for a given amount of flops is proportional to the flops raised to some power α > 1. However, these works used different ranges of model sizes (from a few million to tens of billions of parameters) and found different values of α (around 0.7 and 0.5, respectively). A natural question is whether this scaling law is universal or depends on the dataset. Our dataset allows us to conduct a similar experiment but with much smaller models and flops. Surprisingly, we find evidence for a polynomial scaling law as well, which suggests that there might be a universal phenomenon here.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q236",
      "query": "How does the number of attention heads affect the performance of models trained on TinyStories?",
      "answer": "Our results, shown in Figure 24, suggest that in the regime where the number of heads is small, increasing it improves the performance of the model across all metrics.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q237",
      "query": "What are the main contributions of the TinyStories research?",
      "answer": "• Our main contribution is that we show TinyStories can be used to train and evaluate SLMs that are much smaller than the state-of-the-art models (below 10 million parameters with an embedding dimension of 256), or have much simpler architectures (with only one transformer block), yet still produce a diverse set of fluent and consistent stories that are comparable or superior to those generated by larger and more complex models. Moreover, despite of the small size of the models, we still observe an emergence of reasoning capabilities, knowledge of general facts and ability to follow certain instructions.\n• We introduce a new paradigm for evaluating language models using GPT-4, which overcomes many of the limitations of standard benchmarks.\n• We show that although the training of generative models on TinyStories can typically be done in less than a day on a single GPU, they still exhibit many behaviors similar to the ones observed in LLMs, such as scaling laws, trade-offs between width and depth, etc. Even with limited computational resources, we are able to conduct extensive experiments to study the effects of different hyperparameters, architectures and training methods on the performance and quality of the models.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q238",
      "query": "What are the limitations in assessing the creativity of language models trained on simplified datasets?",
      "answer": "We provided evidence to the fact that the models trained on TinyStories are able to produce genuinely new stories, rather than just copying chunks of text the dataset. It remains a challenge, however, to assess the true extent of the \"creativity\" of our models, and to which the models reflect a certain \"understanding\" (on a very low level of course) of the stories that they produce as opposed to just template matching to create a plausible continuation. We hope that this dataset can be used in future works to obtain insights about the degree of creativity of language models.",
      "document_url": "https://arxiv.org/pdf/2305.07759"
    },
    {
      "id": "q239",
      "query": "What is the main goal of the 'Cramming' experiment described in the paper?",
      "answer": "Our goal is to turn this trend on its head and investigate how to best scale down language model training and what trade-offs emerge when doing so: What downstream performance can be achieved by a modest researcher when training from scratch with a single GPU for a single day?",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q240",
      "query": "How does the paper define 'Cramming' in the context of language model training?",
      "answer": "To answer these questions, we consider a challenge we call \"Cramming\" – learning a whole language model the day before the test.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q241",
      "query": "What are the main constraints for the 'Cramming' setup in this study?",
      "answer": "• A transformer-based language model of arbitrary size is trained with masked-language modeling, completely from scratch.\n• Existing pretrained models cannot be included in any part of the pipeline.\n• Any raw text (excluding downstream data) can be included for training. This means that one can achieve speedups by making judicious choices about how and when to sample data, provided the sampling mechanism does not require a pre-trained model.\n• The downloading and pre-processing of raw data is exempted from the total compute budget. Pre-processing may include CPU-based tokenizer construction, tokenization, and filtering, but cannot include representation learning (e.g. pre-training a word embedding is not allowed, unless it is counted towards the final runtime).\n• Training proceeds on a single GPU for 24 hours.\n• Downstream performance is evaluated on GLUE (Wang et al., 2018). Downstream finetun-ing on GLUE is limited to brief training with only the training data of the downstream task (we consider 5 epochs or less) and needs to work with hyperparameters set globally for all GLUE tasks. Downstream finetuning is excluded from the total compute budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q242",
      "query": "What GPUs were used in the cramming experiment?",
      "answer": "In our implementation, we analyze both a setup with a classical rtx2080ti GPU (released September 2018) and separate setups with a more modern rtxa4000 or rtxa6000 GPU (re-leased October 2020). We pair each unit with 4 CPU cores and 32GB of RAM.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q243",
      "query": "Why is it challenging to scale down language model training?",
      "answer": "An unsurprising consequence of these laws is that scaling down is hard; while smaller model architectures enable speeding up gradient computations, overall rates of model improvement over time remain nearly constant.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q244",
      "query": "What is the significance of scaling laws in language model training?",
      "answer": "Scaling Laws The difficulty in finding tangible improvements is echoed in the scaling laws of Kaplan et al. (2020). Over a wide range of transformer model shapes, Kaplan et al. (2020) find only model size (as number of parameters in non-embedding layers) strongly predicts performance. Further, for a fixed compute budget, an optimal model size can be derived, but performance is only mildly connected to model size - larger models processes less data per unit of compute, but improve faster by almost the same margin.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q245",
      "query": "What data sources were used for training in the cramming experiment?",
      "answer": "We start our investigation with a close analogue to the original raw text sources of Devlin et al. (2019), using a recent dump of the English Wikipedia (20220301.en) and En-glish bookcorpus, noting the commentary of Tan (2019); Bandy & Vincent (2021).",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q246",
      "query": "How does the paper handle tokenization for the cramming experiment?",
      "answer": "We force all text into lower-case, strip accents and non-ascii characters and create an English tokenizer from scratch based only on this data. We choose WordPiece with a vocabulary size of 2^15 = 32768 (Wu et al., 2016). We found no significant change in performance with BPE (Sennrich et al., 2016) or SentencePiece with Unigrams (Kudo, 2018; Kudo & Richardson, 2019). Smaller vocabulary sizes (2^12, 2^13, 2^14) resulted in worse performance, while larger vocabulary sizes (2^16) we not reliably better.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q247",
      "query": "What modifications were made to the attention block in the cramming experiment?",
      "answer": "Attention Block: We disable all QKV biases (Dayma et al., 2021). This exploits the scaling law by removing a layer of computation, making the forward and backward pass somewhat faster, while keeping the model size nearly constant. We find that we could decrease gradient costs by reducing the number of attention heads (Merity, 2019; Araabi & Monz, 2020; Liu et al., 2021b; Javaheripi et al., 2022), as this parallelizes better on the GPU and provides a slight performance boost. Yet, reducing the amount of heads also decreases finetuning performance, so we ultimately keep all 12 heads.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q248",
      "query": "What changes were made to the feedforward block in the cramming model?",
      "answer": "Feedforward Block: We find empirical gains from disabling all linear layer biases (Dayma et al., 2021). Just as for the attention layers, this leverages the scaling law by accelerating gradient computation without noticeable impacts on model size. As a result, we get higher throughput without compromising the rate at which the model improves. We keep the original feedforward block largely unchanged, finding no benefits from changing to another activation than GELU. We do see small improvements from re-ordering the block into a gated linear unit (Dauphin et al., 2017). In contrast to other work, e.g. (Black et al., 2022), we do not increase the number of parameters in the FFN block to compensate for the halving of the hidden dimensionality due to gating.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q249",
      "query": "How does the cramming experiment handle positional embeddings?",
      "answer": "We implement scaled sinusoidal positional embeddings as described in Hua et al. (2022), finding incremental benefits over learned or unscaled sinusoidal embeddings. We see no improvements from decoupling the input and output embeddings (Chung et al., 2020). The suggestion from Lan et al. (2019) to factorize the input embedding provides no gains in our setting. We include a layer normalization at the end of the embedding block.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q250",
      "query": "What optimization algorithm is used in the cramming experiment?",
      "answer": "We keep Adam (Kingma & Ba, 2015) as the optimizer of choice, with weight decay of 0.01 as described in (Loshchilov & Hutter, 2017), β_1 = 0.9, β_2 = 0.98 and ε = 10^-12. To stablize training at no extra cost, we include gradient clipping at a clip value of 0.5.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q251",
      "query": "How is the learning rate scheduled in the cramming experiment?",
      "answer": "Following the advice of Izsak et al. (2021), we re-scale the learning rate schedule so that it is tied to our budget and the learning rate decays as the budget reduces to zero. Interestingly, we observe in Figure 2 that while globally a large number of learning rate shapes lead to similar reductions in loss, we find that we can make some gains through the choice of schedule. We find that a simple one-cycle learning rate (Smith & Topin, 2018) with a peak learning rate of 10^-3 leads to minimal pretraining loss within our budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q252",
      "query": "What is the optimal batch size found in the cramming experiment?",
      "answer": "We find that the optimal batch size in this setting is around 1536 for minimal pretraining loss, but 4032 for maximal downstream performance for the 2080ti, i.e. we accumulate gradients and only perform an update every 16 and 42 forward/backward passes, respectively. For the larger A4000 and A6000 cards, this corresponds to a micro-batch size of 128/256 and final batch size of 4096, which we again accumulate.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q253",
      "query": "How is dropout handled in the cramming experiment?",
      "answer": "In the cramming setting, training data is large compared to compute. Overfitting is not possible due to the single epoch schedule, and we disable dropout during pretraining (Brown et al., 2020) to maximize the number of parameter updates. We re-enable dropout during downstream fine-tuning with a dropout value of 0.1.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q254",
      "query": "What data filtering techniques were found to be effective in the cramming experiment?",
      "answer": "We then test filtering for un-compressible data. We use the tokenizer itself to remove all training sequences from C4 set that cannot be compressed well; we simply set a threshold t, e.g. t = 0.3, and drop all entries from the dataset where the number of tokens in the entry is larger than t times the number of raw characters. This removes, for example, sequences consisting of hard-to-compress HTML or markdown code. Surprisingly, this results in a measurable improvement on C4, summarized in Table 2.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q255",
      "query": "How does the cramming model perform on the GLUE benchmark compared to BERT-base?",
      "answer": "Overall, performance is surprisingly decent, especially for the larger datasets of MNLI, QQP, QNLI and SST-2, where downstream finetuning can smooth out the remaining differences between the full BERT model and the crammed variants. Further, we find substantial gains over both a naive BERT training with limited budget, and over the recipe described in (Izsak et al., 2021).",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q256",
      "query": "What is the main limitation of the cramming model's performance on GLUE tasks?",
      "answer": "Overall, the crammed model mostly works, even for smaller datasets. The average is brought down however by a significant drop on CoLA (corpus of linguistic acceptability) (Warstadt et al., 2019).",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q257",
      "query": "How does the cramming recipe perform when given more compute budget?",
      "answer": "We also verify what happens if the cramming recipe discussed so far is used with more budget. To this end, we train models for 48 hours on 8 A6000 GPUs, which ends up to be 208 total exaFLOP, c.f. Table 1. We directly apply the setting described so far, simply scaling the learning rate schedules to cover the new budget of 48 hours. In Table 6, we find that the discussed recipe does immediately generalize to larger compute budgets. This is surprising, not the least, as now, the dataset (which was sorted in Section 4.4 is now too small and repeated multiple times. The newly trained models have strong performances, especially on MNLI and SST-2, where they significantly outperform the original BERT checkpoint and fall into a similar range as the roBERTA-base checkpoint of Liu et al. (2019), which was trained with much more compute.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q258",
      "query": "What are the main conclusions drawn from the cramming experiment?",
      "answer": "We discuss how much performance a transformer-based language model can achieve when crammed into a setting with very limited compute, finding that several strands of modification lead to decent downstream performance on GLUE. Overall though, cramming language models appears hard, as we empirically find many implications of Kaplan et al. (2020) to still hold in this regime, and for examples improvements through larger models are evened out by their slower speed. We hope that this work can provide a baseline for explorations of the question of cramming we formalize in Section 2 and cast an additional light on a number of improvements and tricks proposed for transformer architectures in recent years.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q259",
      "query": "What are some effective strategies for training language models with limited computational resources?",
      "answer": "We find that we can find changes to the training recipe that exploit scaling laws to yield improvements by improving the effective rate of gradient computations without compromising model size. In the end, we are able to train models that achieve respectable performance – often close to and sometimes exceeding BERT on GLUE tasks – on a shoestring budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q260",
      "query": "How does model size affect the efficiency of language model training?",
      "answer": "Models with more parameters learn more efficiently, as their MLM loss decreases faster on a per-gradient basis. However, smaller architectures make up for their slower learning efficiency by higher throughput, and thus process more tokens over the limited budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q261",
      "query": "What are some techniques to improve the performance of transformer-based models without increasing model size?",
      "answer": "We can exploit scaling laws by quickly searching for architectural choices that speed up computation while keeping model size roughly constant. A number of obvious optimizations fall into this category, and we describe them below, in addition to several other tweaks that provide marginal but worthwhile/free gains.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q262",
      "query": "How can data filtering techniques improve language model training?",
      "answer": "We use the tokenizer itself to remove all training sequences from C4 set that cannot be compressed well; we simply set a threshold t, e.g. t = 0.3, and drop all entries from the dataset where the number of tokens in the entry is larger than t times the number of raw characters. This removes, for example, sequences consisting of hard-to-compress HTML or markdown code. Surprisingly, this results in a measurable improvement on C4, summarized in Table 2.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q263",
      "query": "What are the trade-offs between model size and training efficiency for language models?",
      "answer": "Per-token efficiency of training depends strongly on model size, but not transformer type. Furthermore, smaller models learn less efficiently, and this largely mitigates any throughput gains. Fortunately, the fact that training efficiency is nearly constant across models of the same size means that we can boost performance by finding architecture modifications that speed up gradient computation while keeping the parameter count nearly constant.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q264",
      "query": "How can learning rate schedules be optimized for language model training?",
      "answer": "We re-scale the learning rate schedule so that it is tied to our budget and the learning rate decays as the budget reduces to zero. Interestingly, we observe in Figure 2 that while globally a large number of learning rate shapes lead to similar reductions in loss, we find that we can make some gains through the choice of schedule. We find that a simple one-cycle learning rate (Smith & Topin, 2018) with a peak learning rate of 10^-3 leads to minimal pretraining loss within our budget.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q265",
      "query": "What are some effective modifications to the transformer architecture for efficient training?",
      "answer": "We disable all QKV biases (Dayma et al., 2021). This exploits the scaling law by removing a layer of computation, making the forward and backward pass somewhat faster, while keeping the model size nearly constant. We find empirical gains from disabling all linear layer biases (Dayma et al., 2021). Just as for the attention layers, this leverages the scaling law by accelerating gradient computation without noticeable impacts on model size.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q266",
      "query": "How does batch size affect language model training and performance?",
      "answer": "We find that the optimal batch size in this setting is around 1536 for minimal pretraining loss, but 4032 for maximal downstream performance for the 2080ti, i.e. we accumulate gradients and only perform an update every 16 and 42 forward/backward passes, respectively. For the larger A4000 and A6000 cards, this corresponds to a micro-batch size of 128/256 and final batch size of 4096, which we again accumulate.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q267",
      "query": "What are some strategies for efficient tokenization in language model training?",
      "answer": "We force all text into lower-case, strip accents and non-ascii characters and create an English tokenizer from scratch based only on this data. We choose WordPiece with a vocabulary size of 2^15 = 32768 (Wu et al., 2016). We found no significant change in performance with BPE (Sennrich et al., 2016) or SentencePiece with Unigrams (Kudo, 2018; Kudo & Richardson, 2019). Smaller vocabulary sizes (2^12, 2^13, 2^14) resulted in worse performance, while larger vocabulary sizes (2^16) we not reliably better.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q268",
      "query": "How can language models be efficiently trained to perform well on downstream tasks with limited resources?",
      "answer": "We find substantial gains over both a naive BERT training with limited budget, and over the recipe described in (Izsak et al., 2021). Overall, performance is surprisingly decent, especially for the larger datasets of MNLI, QQP, QNLI and SST-2, where downstream finetuning can smooth out the remaining differences between the full BERT model and the crammed variants.",
      "document_url": "https://arxiv.org/pdf/2212.14034"
    },
    {
      "id": "q269",
      "query": "What is the main achievement of AlphaZero in chess and shogi?",
      "answer": "Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q270",
      "query": "How does AlphaZero's approach differ from traditional chess programs?",
      "answer": "Instead of a handcrafted evaluation function and move ordering heuristics, AlphaZero utilises a deep neural network (p, v) = fθ(s) with parameters θ. This neural network takes the board position s as an input and outputs a vector of move probabilities p with components pa = P r(a|s) for each action a, and a scalar value v estimating the expected outcome z from position s, v ≈ E[z|s]. AlphaZero learns these move probabilities and value estimates entirely from self-play; these are then used to guide its search.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q271",
      "query": "What search algorithm does AlphaZero use instead of alpha-beta search?",
      "answer": "Instead of an alpha-beta search with domain-specific enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root sroot to leaf.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q272",
      "query": "How does AlphaZero train its neural network?",
      "answer": "The parameters θ of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters θ. Games are played by selecting moves for both players by MCTS, at ∼ πt. At the end of the game, the terminal position sT is scored according to the rules of the game to compute the game outcome z: −1 for a loss, 0 for a draw, and +1 for a win. The neural network parameters θ are updated so as to minimise the error between the predicted outcome vt and the game outcome z, and to maximise the similarity of the policy vector pt to the search probabilities πt.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q273",
      "query": "What are the key differences between AlphaZero and the original AlphaGo Zero algorithm?",
      "answer": "AlphaGo Zero estimates and optimises the probability of winning, assuming binary win/loss outcomes. AlphaZero instead estimates and optimises the expected outcome, taking account of draws or potentially other outcomes. The rules of Go are invariant to rotation and reflection. This fact was exploited in AlphaGo and AlphaGo Zero in two ways. First, training data was augmented by generating 8 symmetries for each position. Second, during MCTS, board positions were transformed using a randomly selected rotation or reflection before being evaluated by the neural network, so that the Monte-Carlo evaluation is averaged over different biases. The rules of chess and shogi are asymmetric, and in general symmetries cannot be assumed. AlphaZero does not augment the training data and does not transform the board position during MCTS.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q274",
      "query": "How long did it take for AlphaZero to outperform Stockfish in chess?",
      "answer": "In chess, AlphaZero outperformed Stockfish after just 4 hours (300k steps)",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q275",
      "query": "What was the performance of AlphaZero against Stockfish in a 100-game match?",
      "answer": "AlphaZero convincingly defeated all opponents, losing zero games to Stockfish and eight games to Elmo (see Supplementary Material for several example games), as well as defeating the previous version of AlphaGo Zero",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q276",
      "query": "How does AlphaZero's search efficiency compare to Stockfish and Elmo?",
      "answer": "AlphaZero searches just 80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million for Stockfish and 35 million for Elmo. AlphaZero compensates for the lower number of evaluations by using its deep neural network to focus much more selectively on the most promising variations – arguably a more \"human-like\" approach to search, as originally proposed by Shannon",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q277",
      "query": "How does AlphaZero's performance scale with thinking time compared to Stockfish and Elmo?",
      "answer": "AlphaZero's MCTS scaled more effectively with thinking time than either Stockfish or Elmo, calling into question the widely held belief (4, 11) that alpha-beta search is inherently superior in these domains.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q278",
      "query": "What was unique about AlphaZero's approach to learning chess openings?",
      "answer": "Each of these openings is independently discovered and played frequently by AlphaZero during self-play training. When starting from each human opening, AlphaZero convincingly defeated Stockfish, suggesting that it has indeed mastered a wide spectrum of chess play.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q279",
      "query": "How does AlphaZero represent the board state as input to its neural network?",
      "answer": "The input to the neural network is an N × N × (M T + L) image stack that represents state using a concatenation of T sets of M planes of size N × N . Each set of planes represents the board position at a time-step t − T + 1, ..., t, and is set to zero for time-steps less than 1. The board is oriented to the perspective of the current player. The M feature planes are composed of binary feature planes indicating the presence of the player's pieces, with one plane for each piece type, and a second set of planes indicating the presence of the opponent's pieces.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q280",
      "query": "How does AlphaZero represent moves in chess as output from its neural network?",
      "answer": "We represent the policy π(a|s) by a 8 × 8 × 73 stack of planes encoding a probability distribution over 4,672 possible moves. Each of the 8 × 8 positions identifies the square from which to \"pick up\" a piece. The first 56 planes encode possible 'queen moves' for any piece: a number of squares [1..7] in which the piece will be moved, along one of eight relative compass directions {N, N E, E, SE, S, SW, W, N W }. The next 8 planes encode possible knight moves for that piece. The final 9 planes encode possible underpromotions for pawn moves or captures in two possible diagonals, to knight, bishop or rook respectively. Other pawn moves or captures from the seventh rank are promoted to a queen.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q281",
      "query": "What hardware was used to train AlphaZero?",
      "answer": "Training proceeded for 700,000 steps (mini-batches of size 4,096) starting from randomly initialised parameters, using 5,000 first-generation TPUs (15) to generate self-play games and 64 second-generation TPUs to train the neural networks.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q282",
      "query": "How many simulations did AlphaZero use during training?",
      "answer": "During training, each MCTS used 800 simulations.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q283",
      "query": "What was the learning rate schedule used for training AlphaZero?",
      "answer": "The learning rate was set to 0.2 for each game, and was dropped three times (to 0.02, 0.002 and 0.0002 respectively) during the course of training.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q284",
      "query": "How was Dirichlet noise applied in AlphaZero's training?",
      "answer": "Dirichlet noise Dir(α) was added to the prior probabilities in the root node; this was scaled in inverse proportion to the approximate number of legal moves in a typical position, to a value of α = {0.3, 0.15, 0.03} for chess, shogi and Go respectively.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q285",
      "query": "What versions of Stockfish and Elmo were used for comparison with AlphaZero?",
      "answer": "To evaluate performance in chess, we used Stockfish version 8 (official Linux release) as a baseline program, using 64 CPU threads and a hash size of 1GB. To evaluate performance in shogi, we used Elmo version WCSC27 in combination with YaneuraOu 2017 Early KPPT 4.73 64AVX2 with 64 CPU threads and a hash size of 1GB with the usi option of EnteringKingRule set to NoEnteringKing.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q286",
      "query": "How were Elo ratings calculated for AlphaZero and its opponents?",
      "answer": "We estimate the probability that player a will defeat player b by a logistic function p(a defeats b) = 1/(1+exp (celo(e(b)−e(a))), and estimate the ratings e(·) by Bayesian logistic regression, computed by the BayesElo program (10) using the standard constant celo = 1/400. Elo ratings were computed from the results of a 1 second per move tournament between iterations of AlphaZero during training, and also a baseline player: either Stockfish, Elmo or AlphaGo Lee respectively.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q287",
      "query": "What were the time control settings for the matches between AlphaZero and its opponents?",
      "answer": "Settings were chosen to correspond with computer chess tournament conditions: each player was allowed 1 minute per move, resignation was enabled for all players (-900 centipawns for 10 consecutive moves for Stockfish and Elmo, 5% winrate for AlphaZero). Pondering was disabled for all players.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q288",
      "query": "How does AlphaZero's approach differ from traditional game-playing programs in terms of domain knowledge?",
      "answer": "AlphaZero is a generic reinforcement learning algorithm – originally devised for the game of Go – that achieved superior results within a few hours, searching a thousand times fewer positions, given no domain knowledge except the rules of the game.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q289",
      "query": "What are the advantages of using Monte Carlo tree search in game-playing AI?",
      "answer": "Each search consists of a series of simulated games of self-play that traverse a tree from root sroot to leaf. Each simulation proceeds by selecting in each state s a move a with low visit count, high move probability and high value (averaged over the leaf states of simulations that selected a from s) according to the current neural network fθ. The search returns a vector π representing a probability distribution over moves, either proportionally or greedily with respect to the visit counts at the root state.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q290",
      "query": "How can deep reinforcement learning be applied to board games?",
      "answer": "The parameters θ of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters θ. Games are played by selecting moves for both players by MCTS, at ∼ πt. At the end of the game, the terminal position sT is scored according to the rules of the game to compute the game outcome z: −1 for a loss, 0 for a draw, and +1 for a win. The neural network parameters θ are updated so as to minimise the error between the predicted outcome vt and the game outcome z, and to maximise the similarity of the policy vector pt to the search probabilities πt.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q291",
      "query": "What are the challenges in developing AI for asymmetric board games?",
      "answer": "The rules of chess and shogi are asymmetric, and in general symmetries cannot be assumed. AlphaZero does not augment the training data and does not transform the board position during MCTS.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q292",
      "query": "How can neural networks be used to evaluate board positions in strategy games?",
      "answer": "This neural network takes the board position s as an input and outputs a vector of move probabilities p with components pa = P r(a|s) for each action a, and a scalar value v estimating the expected outcome z from position s, v ≈ E[z|s].",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q293",
      "query": "What are the trade-offs between search depth and position evaluations per second in game-playing AI?",
      "answer": "AlphaZero searches just 80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million for Stockfish and 35 million for Elmo. AlphaZero compensates for the lower number of evaluations by using its deep neural network to focus much more selectively on the most promising variations – arguably a more \"human-like\" approach to search, as originally proposed by Shannon",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q294",
      "query": "How can machine learning be used to discover and evaluate chess openings?",
      "answer": "Each of these openings is independently discovered and played frequently by AlphaZero during self-play training. When starting from each human opening, AlphaZero convincingly defeated Stockfish, suggesting that it has indeed mastered a wide spectrum of chess play.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q295",
      "query": "What are effective ways to represent board states as input for neural networks in game-playing AI?",
      "answer": "The input to the neural network is an N × N × (M T + L) image stack that represents state using a concatenation of T sets of M planes of size N × N . Each set of planes represents the board position at a time-step t − T + 1, ..., t, and is set to zero for time-steps less than 1. The board is oriented to the perspective of the current player. The M feature planes are composed of binary feature planes indicating the presence of the player's pieces, with one plane for each piece type, and a second set of planes indicating the presence of the opponent's pieces.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q296",
      "query": "How can move selection in chess be represented as an output from a neural network?",
      "answer": "We represent the policy π(a|s) by a 8 × 8 × 73 stack of planes encoding a probability distribution over 4,672 possible moves. Each of the 8 × 8 positions identifies the square from which to \"pick up\" a piece. The first 56 planes encode possible 'queen moves' for any piece: a number of squares [1..7] in which the piece will be moved, along one of eight relative compass directions {N, N E, E, SE, S, SW, W, N W }. The next 8 planes encode possible knight moves for that piece. The final 9 planes encode possible underpromotions for pawn moves or captures in two possible diagonals, to knight, bishop or rook respectively. Other pawn moves or captures from the seventh rank are promoted to a queen.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q297",
      "query": "What role does exploration noise play in training game-playing AI systems?",
      "answer": "Dirichlet noise Dir(α) was added to the prior probabilities in the root node; this was scaled in inverse proportion to the approximate number of legal moves in a typical position, to a value of α = {0.3, 0.15, 0.03} for chess, shogi and Go respectively.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q298",
      "query": "How can self-play be used to train AI systems for complex board games?",
      "answer": "The parameters θ of the deep neural network in AlphaZero are trained by self-play reinforcement learning, starting from randomly initialised parameters θ. Games are played by selecting moves for both players by MCTS, at ∼ πt. At the end of the game, the terminal position sT is scored according to the rules of the game to compute the game outcome z: −1 for a loss, 0 for a draw, and +1 for a win. The neural network parameters θ are updated so as to minimise the error between the predicted outcome vt and the game outcome z, and to maximise the similarity of the policy vector pt to the search probabilities πt.",
      "document_url": "https://arxiv.org/pdf/1712.01815"
    },
    {
      "id": "q299",
      "query": "What is the motivation behind using sparse autoencoders for dictionary learning in language models?",
      "answer": "We chose to study a sparse autoencoder approximation of dictionary learning (similar to Sharkey, et al. ). This was for two reasons. First, a sparse autoencoder can readily scale to very large datasets, which we believe is necessary to characterize the features present in a model trained on a large and diverse corpus. Secondly, we have a concern that iterative dictionary learning methods might be \"too strong\", in the sense of being able to recover features from the activations which the model itself cannot access. Exact compressed sensing is NP-hard, which the neural network certainly isn't doing. By contrast, a sparse autoencoder is very similar in architecture to the MLP layers in language models, and so should be similarly powerful in its ability to recover features from superposition.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q300",
      "query": "How does the sparse autoencoder architecture differ from a standard autoencoder in the context of dictionary learning?",
      "answer": "We briefly overview the architecture and training of our sparse autoencoder here, and provide further details in Basic Autoencoder Training. Our sparse autoencoder is a model with a bias at the input, a linear layer with bias and ReLU for the encoder, and then another linear layer and bias for the decoder. In toy models we found that the bias terms were quite important to the autoencoderʼs performance.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q301",
      "query": "What is the 'neuron resampling' technique and why is it used in training sparse autoencoders?",
      "answer": "We find that \"resampling\" these dead neurons during training improves the number of likely-interpretable features (i.e., those in the high density cluster, see Feature Density Histograms) and reduces total loss. This resampling may be compatible with the Lottery Ticket Hypothesis and increase the number of chances the network has to find promising feature directions.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q302",
      "query": "How do the researchers evaluate the interpretability of features extracted by sparse autoencoders?",
      "answer": "In this section, we use three different methods to analyze how interpretable the typical feature is, and how that compares to neurons: human analysis, and two forms of automated interpretability. All three approaches find that features are much more interpretable than neurons.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q303",
      "query": "What is the concept of 'feature splitting' in the context of dictionary learning?",
      "answer": "We conjecture that there is some idealized set of features that dictionary learning would return if we provided it with an unlimited dictionary size. Often, these \"true features\" are clustered into sets of similar features, which the model puts in very tight superposition. Because the number of features is restricted, dictionary learning instead returns features which cover approximately the same territory as the idealized features, at the cost of being somewhat less specific.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q304",
      "query": "How do the researchers measure the universality of features across different models?",
      "answer": "To compare features from different models, we need model-independent ways to represent a feature. One natural approach is to think of a feature as a function assigning values to datapoints; two features would be similar in this sense if they take similar values over a diverse set of data. This general approach has been explored by a number of prior papers (e.g. ). In practice, this can be approximated by representing the feature as a vector, with indices corresponding to a fixed set of data points. We call the correlations between these vectors the activation similarity between features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q305",
      "query": "What is the 'activation similarity' metric and how is it used to compare features between models?",
      "answer": "One natural approach is to think of a feature as a function assigning values to datapoints; two features would be similar in this sense if they take similar values over a diverse set of data. This general approach has been explored by a number of prior papers (e.g. ). In practice, this can be approximated by representing the feature as a vector, with indices corresponding to a fixed set of data points. We call the correlations between these vectors the activation similarity between features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q306",
      "query": "How do the researchers address the challenge of measuring how much of the model's behavior is explained by the extracted features?",
      "answer": "One way to partly get at this question is to ask how much of the loss is explained by our features. For A/1, the run we've focused most on in this paper, 79% of the log-likelihood loss reduction provided by the MLP layer is recovered by our features. That is, the additional loss incurred by replacing the MLP activations with the autoencoder's output is just 21% of the loss that would be incurred by zero ablating the MLP. This loss penalty can be reduced by using more features, or using a lower L1 coefficient. As an extreme example, A/5 ( n_learned_sparse=131,072 , l1_coefficient=0.004 ) recovers 94.5% of log-likelihood loss.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q307",
      "query": "What is the 'finite state automata'-like behavior observed in the extracted features?",
      "answer": "One of the most striking phenomena we've observed in our study of the features in one-layer models is the existence of \"finite state automata\"-like assemblies of features. These assemblies aren't circuits in the conventional sense – they're formed by one feature increasing the probability of tokens, which in turn cause another feature to fire on the next step, and so on.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q308",
      "query": "How do the researchers assess whether the extracted features reflect properties of the model or just the dataset?",
      "answer": "To assess the effect of dataset correlations on the interpretability of feature activations, we run dictionary learning on a version of our one-layer model with random weights. The resulting features are here, and contain many single-token features (such as \"span\", \"file\", \".\", and \"nature\") and some other features firing on seemingly arbitrary subsets of different broadly recognizable contexts (such as LaTeX or code). However, we are unable to construct interpretations for the non-single-token features that make much sense and invite the reader to examine feature visualizations from the model with randomized weights to confirm this for themselves. We conclude that the learning process for the model creates a richer structure in its activations than the distribution of tokens in the dataset alone.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q309",
      "query": "What are the key challenges in scaling sparse autoencoders to larger language models?",
      "answer": "Consider an autoencoder with a 100× expansion factor applied to the activations of a single MLP layer of width 10,000: it would have ~20 billion parameters. Additionally, many of these features are likely quite rare, potentially requiring the autoencoder to be trained on a substantial fraction of the large model's training corpus. So it seems plausible that training the autoencoder could become very expensive, potentially even more expensive than the original model. We remain optimistic, however, and there is a silver lining – it increasingly seems like a large chunk of the mechanistic interpretability agenda will now turn on succeeding at a difficult engineering and scaling problem, which frontier AI labs have significant expertise in.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q310",
      "query": "How do the researchers validate that the extracted features are actually used by the model in meaningful ways?",
      "answer": "We show instead that the learned features have interpretable causal effects on model outputs which make sense in light of the featuresʼ activations. Note that these downstream effects are not inputs to the dictionary learning process, which only sees the activations of the MLP layer. If the resulting features also mediate important downstream behavioral effects then we can be confident that the feature is truly connected to the MLPʼs functional role in the network and not just a property of the underlying data.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q311",
      "query": "What is the 'logit weight similarity' metric and how does it compare to 'activation similarity'?",
      "answer": "A second natural approach is to think of a feature in terms of its downstream effects; two features would be similar in this sense if their activation changes their models' predictions in similar ways. In our one-layer model, a simple approximation to this is the logit weights. This approximation represents each feature as a vector with indices corresponding to vocabulary tokens. We call the correlations between these vectors the logit weight similarity between features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q312",
      "query": "How do the researchers use automated interpretability to evaluate feature interpretability at scale?",
      "answer": "To analyze features at a larger scale, we turned to automated interpretability . Following the approach of Bills et al. , we have a large language model, Anthropicʼs Claude, generate explanations of features using examples of tokens where they activate. Next, we have the model use that explanation to predict new activations on previously unseen tokens.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q313",
      "query": "What is the 'attribution similarity' metric and how does it relate to activation and logit weight similarities?",
      "answer": "We want to measure something more like \"the actual effect a feature has on token probabilities.\" One way to get at this would be to compute a vector of ablation effects for every feature on every data point; pairs of features whose ablations hurt the model's predictions on the same tokens must have been predicting the same thing. Unfortunately, this would be rather expensive computationally. Instead, we scale the activation vector of a feature by the logit weights of the tokens that empirically come next in the dataset to produce an attribution vector. Correlations between those vectors provide an attribution similarity that combines both the activity of the feature with the effect it has on the loss. We find that the attribution similarity correlates quite highly with the activation similarity, meaning that features that were coactive between models were useful at predicting the same tokens.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q314",
      "query": "How do the researchers use 'pinned feature sampling' to validate their interpretation of features?",
      "answer": "We also validate that the feature's downstream effect is in line with our interpretation as an Arabic script feature by sampling from the model with the feature activity \"pinned\" at a high value. To do this, we start with a prefix 1,2,3,4,5,6,7,8,9,10 where the model has an expected continuation (keep in mind that this is a one layer model that is very weak!). We then instead set A/1/3450 to its maximum observed value and see how that changes the samples:",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q315",
      "query": "What is the 'importance scoring' technique used in automated interpretability, and why is it necessary?",
      "answer": "Importance scoring is the principled way to correct for the fact that we did not randomly sample all of the examples our model is asked to score. Formally, we assign to all tokens from random examples a weight of 1 and examples from each feature interval a weight of feature_density*interval_probability . feature_density is the fraction of times this feature fires over a large portion of the dataset and interval_probability converts the distribution of non-zero activations into a categorical distribution corresponding to each interval and uses the corresponding probability. We then use these weights in our Spearman correlation as before.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q316",
      "query": "How do the researchers address the issue of polysemanticity in neurons versus features extracted by sparse autoencoders?",
      "answer": "Unfortunately, the most natural computational unit of the neural network – the neuron itself – turns out not to be a natural unit for human understanding. This is because many neurons are polysemantic: they respond to mixtures of seemingly unrelated inputs. In the vision model Inception v1, a single neuron responds to faces of cats and fronts of cars . In a small language model we discuss in this paper, a single neuron responds to a mixture of academic citations, English dialogue, HTTP requests, and Korean text. Polysemanticity makes it difficult to reason about the behavior of the network in terms of the activity of individual neurons.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q317",
      "query": "What is the 'feature density' metric and how is it used to evaluate the quality of extracted features?",
      "answer": "While iterating on different techniques, one important proxy for autoencoder performance is feature density. Each feature in our autoencoder only activates on a very small percentage of the total tokens in the training set. We define the feature density of each feature as the fraction of tokens on which the feature has a nonzero value. We hypothesize that language models contain a large number of features across a distribution of feature densities, and that lower-density features are harder for our autoencoder to discover because they appear less often in the training dataset. Using large training datasets was an attempt to recover such low-density features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      },
      {
      "id": "q318",
      "query": "How do the researchers address the challenge of determining the optimal number of features to extract using dictionary learning?",
      "answer": "If this picture is true, it would be important for a number of reasons. It suggests that determining the \"correct number of features\" for dictionary learning is less important than it might initially seem. It also suggests that dictionary learning with fewer features can provide a \"summary\" of model features, which might be very important in studying large models. Additionally, it would explain some of the stranger features we observe in the process of dictionary learning, suggesting that these are either \"collapsed\" features which would make sense if split further (see \"Bug\" 1: Single Token Features), or else highly-specific \"split\" features which do in fact make sense if analyzed closely (see \"Bug\" 2: Multiple Features for a Single Context). Finally, it suggests that our basic theory of superposition in toy models is missing an important dimension of the problem by not adequately studying highly correlated and \"action sharing\" features.",
      "document_url": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
      }
  ],
  "test": []
}
